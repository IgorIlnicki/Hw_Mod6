{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f64c5fb-973e-4a1e-9a9b-de2809fde199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Функція гіпотези лінійної регресії у векторному вигляді\n",
    "def hypothesis(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "     # Параметр: X : numpy array - Матриця ознак (кожен рядок - один приклад, кожний стовпчик - одна ознака).\n",
    "                # Параметр: theta : numpy array - Вектор параметрів моделі.\n",
    "                # Повертає: theta : numpy array - Вектор прогнозованих значень.\n",
    "# Обчислення прогнозованих значень\n",
    "#h_theta = hypothesis_linear_regression(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "788d21c4-77be-4899-8a2c-0b28bc7c5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функція втрат (MSE)\n",
    "def loss_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = np.dot(X, theta)   # Прогнозовані значення\n",
    "    loss = np.sum((predictions - y) ** 2) / (2 * m)  # Обчислення MSE\n",
    "    return loss\n",
    "# Обчислення функції втрат для конкретного прикладу\n",
    "#loss = loss_function(X, y, theta)\n",
    "#print(\"Значення функції втрат (MSE):\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a674df7-ce18-4e7a-be4e-a84d754b6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Один крок градієнтного спуску\n",
    "# Крок навчання\n",
    "learning_rate = 0.01\n",
    "# Градієнтний спуск\n",
    "def gradient_descent(X, y, theta, learning_rate):\n",
    "    m = len(y)\n",
    "    for i in range(iterations):\n",
    "        predictions = hypothesis(X, theta)   # Прогнозовані значення\n",
    "        gradient = np.dot(X.T, (predictions - y)) / m   # Обчислення градієнта\n",
    "        theta -= learning_rate * gradient    # Оновлення параметрів\n",
    "    return theta\n",
    "      # learning_rate (float) - Крок навчання (розмір кроку для оновлення параметрів)\n",
    "      # Повертає: theta : numpy array - Нові значення параметрів після кроку градієнтного спуску\n",
    "# Виконання одного кроку градієнтного спуску\n",
    "#theta = gradient_descent_step(X, y, theta, learning_rate)\n",
    "#print(\"Нові значення параметрів після кроку градієнтного спуску:\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90c609f0-d010-4a15-9f13-7780648772b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 13106676989947.729\n",
      "Iteration 2: Loss = 13106436307111.898\n",
      "Iteration 3: Loss = 13106195693984.004\n",
      "Iteration 4: Loss = 13105955150543.584\n",
      "Iteration 5: Loss = 13105714676770.182\n",
      "Iteration 6: Loss = 13105474272643.35\n",
      "Iteration 7: Loss = 13105233938142.65\n",
      "Iteration 8: Loss = 13104993673247.64\n",
      "Iteration 9: Loss = 13104753477937.895\n",
      "Iteration 10: Loss = 13104513352192.984\n",
      "Iteration 11: Loss = 13104273295992.498\n",
      "Iteration 12: Loss = 13104033309316.016\n",
      "Iteration 13: Loss = 13103793392143.137\n",
      "Iteration 14: Loss = 13103553544453.46\n",
      "Iteration 15: Loss = 13103313766226.594\n",
      "Iteration 16: Loss = 13103074057442.145\n",
      "Iteration 17: Loss = 13102834418079.734\n",
      "Iteration 18: Loss = 13102594848118.986\n",
      "Iteration 19: Loss = 13102355347539.531\n",
      "Iteration 20: Loss = 13102115916321.004\n",
      "Iteration 21: Loss = 13101876554443.047\n",
      "Iteration 22: Loss = 13101637261885.312\n",
      "Iteration 23: Loss = 13101398038627.445\n",
      "Iteration 24: Loss = 13101158884649.115\n",
      "Iteration 25: Loss = 13100919799929.984\n",
      "Iteration 26: Loss = 13100680784449.725\n",
      "Iteration 27: Loss = 13100441838188.016\n",
      "Iteration 28: Loss = 13100202961124.541\n",
      "Iteration 29: Loss = 13099964153238.99\n",
      "Iteration 30: Loss = 13099725414511.062\n",
      "Iteration 31: Loss = 13099486744920.457\n",
      "Iteration 32: Loss = 13099248144446.887\n",
      "Iteration 33: Loss = 13099009613070.059\n",
      "Iteration 34: Loss = 13098771150769.695\n",
      "Iteration 35: Loss = 13098532757525.527\n",
      "Iteration 36: Loss = 13098294433317.287\n",
      "Iteration 37: Loss = 13098056178124.707\n",
      "Iteration 38: Loss = 13097817991927.537\n",
      "Iteration 39: Loss = 13097579874705.523\n",
      "Iteration 40: Loss = 13097341826438.424\n",
      "Iteration 41: Loss = 13097103847106.0\n",
      "Iteration 42: Loss = 13096865936688.021\n",
      "Iteration 43: Loss = 13096628095164.264\n",
      "Iteration 44: Loss = 13096390322514.506\n",
      "Iteration 45: Loss = 13096152618718.535\n",
      "Iteration 46: Loss = 13095914983756.139\n",
      "Iteration 47: Loss = 13095677417607.12\n",
      "Iteration 48: Loss = 13095439920251.281\n",
      "Iteration 49: Loss = 13095202491668.432\n",
      "Iteration 50: Loss = 13094965131838.389\n",
      "Iteration 51: Loss = 13094727840740.975\n",
      "Iteration 52: Loss = 13094490618356.016\n",
      "Iteration 53: Loss = 13094253464663.348\n",
      "Iteration 54: Loss = 13094016379642.81\n",
      "Iteration 55: Loss = 13093779363274.25\n",
      "Iteration 56: Loss = 13093542415537.516\n",
      "Iteration 57: Loss = 13093305536412.469\n",
      "Iteration 58: Loss = 13093068725878.973\n",
      "Iteration 59: Loss = 13092831983916.895\n",
      "Iteration 60: Loss = 13092595310506.113\n",
      "Iteration 61: Loss = 13092358705626.508\n",
      "Iteration 62: Loss = 13092122169257.967\n",
      "Iteration 63: Loss = 13091885701380.387\n",
      "Iteration 64: Loss = 13091649301973.66\n",
      "Iteration 65: Loss = 13091412971017.7\n",
      "Iteration 66: Loss = 13091176708492.41\n",
      "Iteration 67: Loss = 13090940514377.713\n",
      "Iteration 68: Loss = 13090704388653.533\n",
      "Iteration 69: Loss = 13090468331299.797\n",
      "Iteration 70: Loss = 13090232342296.44\n",
      "Iteration 71: Loss = 13089996421623.402\n",
      "Iteration 72: Loss = 13089760569260.635\n",
      "Iteration 73: Loss = 13089524785188.088\n",
      "Iteration 74: Loss = 13089289069385.717\n",
      "Iteration 75: Loss = 13089053421833.494\n",
      "Iteration 76: Loss = 13088817842511.38\n",
      "Iteration 77: Loss = 13088582331399.365\n",
      "Iteration 78: Loss = 13088346888477.416\n",
      "Iteration 79: Loss = 13088111513725.537\n",
      "Iteration 80: Loss = 13087876207123.713\n",
      "Iteration 81: Loss = 13087640968651.945\n",
      "Iteration 82: Loss = 13087405798290.242\n",
      "Iteration 83: Loss = 13087170696018.613\n",
      "Iteration 84: Loss = 13086935661817.078\n",
      "Iteration 85: Loss = 13086700695665.662\n",
      "Iteration 86: Loss = 13086465797544.393\n",
      "Iteration 87: Loss = 13086230967433.307\n",
      "Iteration 88: Loss = 13085996205312.447\n",
      "Iteration 89: Loss = 13085761511161.86\n",
      "Iteration 90: Loss = 13085526884961.598\n",
      "Iteration 91: Loss = 13085292326691.725\n",
      "Iteration 92: Loss = 13085057836332.297\n",
      "Iteration 93: Loss = 13084823413863.396\n",
      "Iteration 94: Loss = 13084589059265.094\n",
      "Iteration 95: Loss = 13084354772517.473\n",
      "Iteration 96: Loss = 13084120553600.621\n",
      "Iteration 97: Loss = 13083886402494.643\n",
      "Iteration 98: Loss = 13083652319179.623\n",
      "Iteration 99: Loss = 13083418303635.682\n",
      "Iteration 100: Loss = 13083184355842.922\n",
      "Iteration 101: Loss = 13082950475781.469\n",
      "Iteration 102: Loss = 13082716663431.441\n",
      "Iteration 103: Loss = 13082482918772.975\n",
      "Iteration 104: Loss = 13082249241786.197\n",
      "Iteration 105: Loss = 13082015632451.262\n",
      "Iteration 106: Loss = 13081782090748.305\n",
      "Iteration 107: Loss = 13081548616657.488\n",
      "Iteration 108: Loss = 13081315210158.969\n",
      "Iteration 109: Loss = 13081081871232.906\n",
      "Iteration 110: Loss = 13080848599859.479\n",
      "Iteration 111: Loss = 13080615396018.863\n",
      "Iteration 112: Loss = 13080382259691.24\n",
      "Iteration 113: Loss = 13080149190856.799\n",
      "Iteration 114: Loss = 13079916189495.732\n",
      "Iteration 115: Loss = 13079683255588.242\n",
      "Iteration 116: Loss = 13079450389114.535\n",
      "Iteration 117: Loss = 13079217590054.826\n",
      "Iteration 118: Loss = 13078984858389.328\n",
      "Iteration 119: Loss = 13078752194098.266\n",
      "Iteration 120: Loss = 13078519597161.871\n",
      "Iteration 121: Loss = 13078287067560.38\n",
      "Iteration 122: Loss = 13078054605274.031\n",
      "Iteration 123: Loss = 13077822210283.074\n",
      "Iteration 124: Loss = 13077589882567.762\n",
      "Iteration 125: Loss = 13077357622108.352\n",
      "Iteration 126: Loss = 13077125428885.11\n",
      "Iteration 127: Loss = 13076893302878.309\n",
      "Iteration 128: Loss = 13076661244068.219\n",
      "Iteration 129: Loss = 13076429252435.127\n",
      "Iteration 130: Loss = 13076197327959.318\n",
      "Iteration 131: Loss = 13075965470621.096\n",
      "Iteration 132: Loss = 13075733680400.748\n",
      "Iteration 133: Loss = 13075501957278.584\n",
      "Iteration 134: Loss = 13075270301234.916\n",
      "Iteration 135: Loss = 13075038712250.062\n",
      "Iteration 136: Loss = 13074807190304.348\n",
      "Iteration 137: Loss = 13074575735378.094\n",
      "Iteration 138: Loss = 13074344347451.643\n",
      "Iteration 139: Loss = 13074113026505.328\n",
      "Iteration 140: Loss = 13073881772519.504\n",
      "Iteration 141: Loss = 13073650585474.521\n",
      "Iteration 142: Loss = 13073419465350.73\n",
      "Iteration 143: Loss = 13073188412128.502\n",
      "Iteration 144: Loss = 13072957425788.205\n",
      "Iteration 145: Loss = 13072726506310.213\n",
      "Iteration 146: Loss = 13072495653674.906\n",
      "Iteration 147: Loss = 13072264867862.676\n",
      "Iteration 148: Loss = 13072034148853.912\n",
      "Iteration 149: Loss = 13071803496629.014\n",
      "Iteration 150: Loss = 13071572911168.38\n",
      "Iteration 151: Loss = 13071342392452.432\n",
      "Iteration 152: Loss = 13071111940461.574\n",
      "Iteration 153: Loss = 13070881555176.238\n",
      "Iteration 154: Loss = 13070651236576.846\n",
      "Iteration 155: Loss = 13070420984643.834\n",
      "Iteration 156: Loss = 13070190799357.637\n",
      "Iteration 157: Loss = 13069960680698.703\n",
      "Iteration 158: Loss = 13069730628647.482\n",
      "Iteration 159: Loss = 13069500643184.43\n",
      "Iteration 160: Loss = 13069270724290.012\n",
      "Iteration 161: Loss = 13069040871944.69\n",
      "Iteration 162: Loss = 13068811086128.947\n",
      "Iteration 163: Loss = 13068581366823.252\n",
      "Iteration 164: Loss = 13068351714008.1\n",
      "Iteration 165: Loss = 13068122127663.975\n",
      "Iteration 166: Loss = 13067892607771.375\n",
      "Iteration 167: Loss = 13067663154310.807\n",
      "Iteration 168: Loss = 13067433767262.775\n",
      "Iteration 169: Loss = 13067204446607.799\n",
      "Iteration 170: Loss = 13066975192326.389\n",
      "Iteration 171: Loss = 13066746004399.08\n",
      "Iteration 172: Loss = 13066516882806.398\n",
      "Iteration 173: Loss = 13066287827528.883\n",
      "Iteration 174: Loss = 13066058838547.078\n",
      "Iteration 175: Loss = 13065829915841.531\n",
      "Iteration 176: Loss = 13065601059392.797\n",
      "Iteration 177: Loss = 13065372269181.436\n",
      "Iteration 178: Loss = 13065143545188.012\n",
      "Iteration 179: Loss = 13064914887393.098\n",
      "Iteration 180: Loss = 13064686295777.273\n",
      "Iteration 181: Loss = 13064457770321.121\n",
      "Iteration 182: Loss = 13064229311005.227\n",
      "Iteration 183: Loss = 13064000917810.19\n",
      "Iteration 184: Loss = 13063772590716.605\n",
      "Iteration 185: Loss = 13063544329705.084\n",
      "Iteration 186: Loss = 13063316134756.234\n",
      "Iteration 187: Loss = 13063088005850.676\n",
      "Iteration 188: Loss = 13062859942969.035\n",
      "Iteration 189: Loss = 13062631946091.938\n",
      "Iteration 190: Loss = 13062404015200.016\n",
      "Iteration 191: Loss = 13062176150273.912\n",
      "Iteration 192: Loss = 13061948351294.277\n",
      "Iteration 193: Loss = 13061720618241.758\n",
      "Iteration 194: Loss = 13061492951097.014\n",
      "Iteration 195: Loss = 13061265349840.707\n",
      "Iteration 196: Loss = 13061037814453.508\n",
      "Iteration 197: Loss = 13060810344916.094\n",
      "Iteration 198: Loss = 13060582941209.14\n",
      "Iteration 199: Loss = 13060355603313.34\n",
      "Iteration 200: Loss = 13060128331209.377\n",
      "Iteration 201: Loss = 13059901124877.955\n",
      "Iteration 202: Loss = 13059673984299.777\n",
      "Iteration 203: Loss = 13059446909455.549\n",
      "Iteration 204: Loss = 13059219900325.988\n",
      "Iteration 205: Loss = 13058992956891.816\n",
      "Iteration 206: Loss = 13058766079133.758\n",
      "Iteration 207: Loss = 13058539267032.543\n",
      "Iteration 208: Loss = 13058312520568.914\n",
      "Iteration 209: Loss = 13058085839723.607\n",
      "Iteration 210: Loss = 13057859224477.38\n",
      "Iteration 211: Loss = 13057632674810.984\n",
      "Iteration 212: Loss = 13057406190705.174\n",
      "Iteration 213: Loss = 13057179772140.725\n",
      "Iteration 214: Loss = 13056953419098.404\n",
      "Iteration 215: Loss = 13056727131558.986\n",
      "Iteration 216: Loss = 13056500909503.264\n",
      "Iteration 217: Loss = 13056274752912.018\n",
      "Iteration 218: Loss = 13056048661766.043\n",
      "Iteration 219: Loss = 13055822636046.139\n",
      "Iteration 220: Loss = 13055596675733.12\n",
      "Iteration 221: Loss = 13055370780807.785\n",
      "Iteration 222: Loss = 13055144951250.96\n",
      "Iteration 223: Loss = 13054919187043.469\n",
      "Iteration 224: Loss = 13054693488166.137\n",
      "Iteration 225: Loss = 13054467854599.799\n",
      "Iteration 226: Loss = 13054242286325.29\n",
      "Iteration 227: Loss = 13054016783323.465\n",
      "Iteration 228: Loss = 13053791345575.166\n",
      "Iteration 229: Loss = 13053565973061.258\n",
      "Iteration 230: Loss = 13053340665762.598\n",
      "Iteration 231: Loss = 13053115423660.059\n",
      "Iteration 232: Loss = 13052890246734.51\n",
      "Iteration 233: Loss = 13052665134966.832\n",
      "Iteration 234: Loss = 13052440088337.912\n",
      "Iteration 235: Loss = 13052215106828.639\n",
      "Iteration 236: Loss = 13051990190419.912\n",
      "Iteration 237: Loss = 13051765339092.629\n",
      "Iteration 238: Loss = 13051540552827.703\n",
      "Iteration 239: Loss = 13051315831606.041\n",
      "Iteration 240: Loss = 13051091175408.57\n",
      "Iteration 241: Loss = 13050866584216.207\n",
      "Iteration 242: Loss = 13050642058009.889\n",
      "Iteration 243: Loss = 13050417596770.547\n",
      "Iteration 244: Loss = 13050193200479.125\n",
      "Iteration 245: Loss = 13049968869116.57\n",
      "Iteration 246: Loss = 13049744602663.834\n",
      "Iteration 247: Loss = 13049520401101.877\n",
      "Iteration 248: Loss = 13049296264411.666\n",
      "Iteration 249: Loss = 13049072192574.162\n",
      "Iteration 250: Loss = 13048848185570.348\n",
      "Iteration 251: Loss = 13048624243381.203\n",
      "Iteration 252: Loss = 13048400365987.713\n",
      "Iteration 253: Loss = 13048176553370.875\n",
      "Iteration 254: Loss = 13047952805511.678\n",
      "Iteration 255: Loss = 13047729122391.133\n",
      "Iteration 256: Loss = 13047505503990.246\n",
      "Iteration 257: Loss = 13047281950290.033\n",
      "Iteration 258: Loss = 13047058461271.512\n",
      "Iteration 259: Loss = 13046835036915.713\n",
      "Iteration 260: Loss = 13046611677203.664\n",
      "Iteration 261: Loss = 13046388382116.404\n",
      "Iteration 262: Loss = 13046165151634.977\n",
      "Iteration 263: Loss = 13045941985740.43\n",
      "Iteration 264: Loss = 13045718884413.812\n",
      "Iteration 265: Loss = 13045495847636.195\n",
      "Iteration 266: Loss = 13045272875388.63\n",
      "Iteration 267: Loss = 13045049967652.201\n",
      "Iteration 268: Loss = 13044827124407.977\n",
      "Iteration 269: Loss = 13044604345637.04\n",
      "Iteration 270: Loss = 13044381631320.477\n",
      "Iteration 271: Loss = 13044158981439.385\n",
      "Iteration 272: Loss = 13043936395974.863\n",
      "Iteration 273: Loss = 13043713874908.012\n",
      "Iteration 274: Loss = 13043491418219.941\n",
      "Iteration 275: Loss = 13043269025891.771\n",
      "Iteration 276: Loss = 13043046697904.621\n",
      "Iteration 277: Loss = 13042824434239.613\n",
      "Iteration 278: Loss = 13042602234877.887\n",
      "Iteration 279: Loss = 13042380099800.576\n",
      "Iteration 280: Loss = 13042158028988.822\n",
      "Iteration 281: Loss = 13041936022423.78\n",
      "Iteration 282: Loss = 13041714080086.6\n",
      "Iteration 283: Loss = 13041492201958.443\n",
      "Iteration 284: Loss = 13041270388020.479\n",
      "Iteration 285: Loss = 13041048638253.875\n",
      "Iteration 286: Loss = 13040826952639.803\n",
      "Iteration 287: Loss = 13040605331159.457\n",
      "Iteration 288: Loss = 13040383773794.016\n",
      "Iteration 289: Loss = 13040162280524.68\n",
      "Iteration 290: Loss = 13039940851332.643\n",
      "Iteration 291: Loss = 13039719486199.11\n",
      "Iteration 292: Loss = 13039498185105.291\n",
      "Iteration 293: Loss = 13039276948032.408\n",
      "Iteration 294: Loss = 13039055774961.68\n",
      "Iteration 295: Loss = 13038834665874.326\n",
      "Iteration 296: Loss = 13038613620751.59\n",
      "Iteration 297: Loss = 13038392639574.7\n",
      "Iteration 298: Loss = 13038171722324.906\n",
      "Iteration 299: Loss = 13037950868983.457\n",
      "Iteration 300: Loss = 13037730079531.605\n",
      "Iteration 301: Loss = 13037509353950.613\n",
      "Iteration 302: Loss = 13037288692221.742\n",
      "Iteration 303: Loss = 13037068094326.268\n",
      "Iteration 304: Loss = 13036847560245.465\n",
      "Iteration 305: Loss = 13036627089960.621\n",
      "Iteration 306: Loss = 13036406683453.014\n",
      "Iteration 307: Loss = 13036186340703.945\n",
      "Iteration 308: Loss = 13035966061694.71\n",
      "Iteration 309: Loss = 13035745846406.617\n",
      "Iteration 310: Loss = 13035525694820.969\n",
      "Iteration 311: Loss = 13035305606919.09\n",
      "Iteration 312: Loss = 13035085582682.297\n",
      "Iteration 313: Loss = 13034865622091.914\n",
      "Iteration 314: Loss = 13034645725129.28\n",
      "Iteration 315: Loss = 13034425891775.723\n",
      "Iteration 316: Loss = 13034206122012.592\n",
      "Iteration 317: Loss = 13033986415821.24\n",
      "Iteration 318: Loss = 13033766773183.016\n",
      "Iteration 319: Loss = 13033547194079.277\n",
      "Iteration 320: Loss = 13033327678491.393\n",
      "Iteration 321: Loss = 13033108226400.732\n",
      "Iteration 322: Loss = 13032888837788.672\n",
      "Iteration 323: Loss = 13032669512636.594\n",
      "Iteration 324: Loss = 13032450250925.887\n",
      "Iteration 325: Loss = 13032231052637.941\n",
      "Iteration 326: Loss = 13032011917754.156\n",
      "Iteration 327: Loss = 13031792846255.934\n",
      "Iteration 328: Loss = 13031573838124.686\n",
      "Iteration 329: Loss = 13031354893341.828\n",
      "Iteration 330: Loss = 13031136011888.775\n",
      "Iteration 331: Loss = 13030917193746.957\n",
      "Iteration 332: Loss = 13030698438897.809\n",
      "Iteration 333: Loss = 13030479747322.758\n",
      "Iteration 334: Loss = 13030261119003.254\n",
      "Iteration 335: Loss = 13030042553920.738\n",
      "Iteration 336: Loss = 13029824052056.67\n",
      "Iteration 337: Loss = 13029605613392.506\n",
      "Iteration 338: Loss = 13029387237909.71\n",
      "Iteration 339: Loss = 13029168925589.75\n",
      "Iteration 340: Loss = 13028950676414.104\n",
      "Iteration 341: Loss = 13028732490364.25\n",
      "Iteration 342: Loss = 13028514367421.678\n",
      "Iteration 343: Loss = 13028296307567.873\n",
      "Iteration 344: Loss = 13028078310784.342\n",
      "Iteration 345: Loss = 13027860377052.574\n",
      "Iteration 346: Loss = 13027642506354.088\n",
      "Iteration 347: Loss = 13027424698670.393\n",
      "Iteration 348: Loss = 13027206953983.01\n",
      "Iteration 349: Loss = 13026989272273.46\n",
      "Iteration 350: Loss = 13026771653523.277\n",
      "Iteration 351: Loss = 13026554097713.992\n",
      "Iteration 352: Loss = 13026336604827.146\n",
      "Iteration 353: Loss = 13026119174844.29\n",
      "Iteration 354: Loss = 13025901807746.973\n",
      "Iteration 355: Loss = 13025684503516.75\n",
      "Iteration 356: Loss = 13025467262135.188\n",
      "Iteration 357: Loss = 13025250083583.846\n",
      "Iteration 358: Loss = 13025032967844.309\n",
      "Iteration 359: Loss = 13024815914898.146\n",
      "Iteration 360: Loss = 13024598924726.95\n",
      "Iteration 361: Loss = 13024381997312.305\n",
      "Iteration 362: Loss = 13024165132635.805\n",
      "Iteration 363: Loss = 13023948330679.057\n",
      "Iteration 364: Loss = 13023731591423.662\n",
      "Iteration 365: Loss = 13023514914851.232\n",
      "Iteration 366: Loss = 13023298300943.387\n",
      "Iteration 367: Loss = 13023081749681.748\n",
      "Iteration 368: Loss = 13022865261047.941\n",
      "Iteration 369: Loss = 13022648835023.6\n",
      "Iteration 370: Loss = 13022432471590.367\n",
      "Iteration 371: Loss = 13022216170729.879\n",
      "Iteration 372: Loss = 13021999932423.795\n",
      "Iteration 373: Loss = 13021783756653.762\n",
      "Iteration 374: Loss = 13021567643401.441\n",
      "Iteration 375: Loss = 13021351592648.504\n",
      "Iteration 376: Loss = 13021135604376.621\n",
      "Iteration 377: Loss = 13020919678567.46\n",
      "Iteration 378: Loss = 13020703815202.713\n",
      "Iteration 379: Loss = 13020488014264.064\n",
      "Iteration 380: Loss = 13020272275733.203\n",
      "Iteration 381: Loss = 13020056599591.834\n",
      "Iteration 382: Loss = 13019840985821.656\n",
      "Iteration 383: Loss = 13019625434404.379\n",
      "Iteration 384: Loss = 13019409945321.719\n",
      "Iteration 385: Loss = 13019194518555.396\n",
      "Iteration 386: Loss = 13018979154087.137\n",
      "Iteration 387: Loss = 13018763851898.668\n",
      "Iteration 388: Loss = 13018548611971.729\n",
      "Iteration 389: Loss = 13018333434288.057\n",
      "Iteration 390: Loss = 13018118318829.406\n",
      "Iteration 391: Loss = 13017903265577.523\n",
      "Iteration 392: Loss = 13017688274514.166\n",
      "Iteration 393: Loss = 13017473345621.102\n",
      "Iteration 394: Loss = 13017258478880.098\n",
      "Iteration 395: Loss = 13017043674272.922\n",
      "Iteration 396: Loss = 13016828931781.363\n",
      "Iteration 397: Loss = 13016614251387.2\n",
      "Iteration 398: Loss = 13016399633072.225\n",
      "Iteration 399: Loss = 13016185076818.23\n",
      "Iteration 400: Loss = 13015970582607.021\n",
      "Iteration 401: Loss = 13015756150420.404\n",
      "Iteration 402: Loss = 13015541780240.184\n",
      "Iteration 403: Loss = 13015327472048.182\n",
      "Iteration 404: Loss = 13015113225826.225\n",
      "Iteration 405: Loss = 13014899041556.133\n",
      "Iteration 406: Loss = 13014684919219.742\n",
      "Iteration 407: Loss = 13014470858798.893\n",
      "Iteration 408: Loss = 13014256860275.428\n",
      "Iteration 409: Loss = 13014042923631.195\n",
      "Iteration 410: Loss = 13013829048848.049\n",
      "Iteration 411: Loss = 13013615235907.854\n",
      "Iteration 412: Loss = 13013401484792.469\n",
      "Iteration 413: Loss = 13013187795483.771\n",
      "Iteration 414: Loss = 13012974167963.629\n",
      "Iteration 415: Loss = 13012760602213.93\n",
      "Iteration 416: Loss = 13012547098216.559\n",
      "Iteration 417: Loss = 13012333655953.408\n",
      "Iteration 418: Loss = 13012120275406.373\n",
      "Iteration 419: Loss = 13011906956557.361\n",
      "Iteration 420: Loss = 13011693699388.273\n",
      "Iteration 421: Loss = 13011480503881.031\n",
      "Iteration 422: Loss = 13011267370017.549\n",
      "Iteration 423: Loss = 13011054297779.75\n",
      "Iteration 424: Loss = 13010841287149.568\n",
      "Iteration 425: Loss = 13010628338108.934\n",
      "Iteration 426: Loss = 13010415450639.793\n",
      "Iteration 427: Loss = 13010202624724.084\n",
      "Iteration 428: Loss = 13009989860343.762\n",
      "Iteration 429: Loss = 13009777157480.781\n",
      "Iteration 430: Loss = 13009564516117.105\n",
      "Iteration 431: Loss = 13009351936234.701\n",
      "Iteration 432: Loss = 13009139417815.541\n",
      "Iteration 433: Loss = 13008926960841.6\n",
      "Iteration 434: Loss = 13008714565294.861\n",
      "Iteration 435: Loss = 13008502231157.316\n",
      "Iteration 436: Loss = 13008289958410.955\n",
      "Iteration 437: Loss = 13008077747037.78\n",
      "Iteration 438: Loss = 13007865597019.791\n",
      "Iteration 439: Loss = 13007653508339.004\n",
      "Iteration 440: Loss = 13007441480977.426\n",
      "Iteration 441: Loss = 13007229514917.08\n",
      "Iteration 442: Loss = 13007017610139.992\n",
      "Iteration 443: Loss = 13006805766628.197\n",
      "Iteration 444: Loss = 13006593984363.729\n",
      "Iteration 445: Loss = 13006382263328.621\n",
      "Iteration 446: Loss = 13006170603504.932\n",
      "Iteration 447: Loss = 13005959004874.705\n",
      "Iteration 448: Loss = 13005747467420.002\n",
      "Iteration 449: Loss = 13005535991122.885\n",
      "Iteration 450: Loss = 13005324575965.416\n",
      "Iteration 451: Loss = 13005113221929.682\n",
      "Iteration 452: Loss = 13004901928997.746\n",
      "Iteration 453: Loss = 13004690697151.703\n",
      "Iteration 454: Loss = 13004479526373.635\n",
      "Iteration 455: Loss = 13004268416645.639\n",
      "Iteration 456: Loss = 13004057367949.816\n",
      "Iteration 457: Loss = 13003846380268.271\n",
      "Iteration 458: Loss = 13003635453583.111\n",
      "Iteration 459: Loss = 13003424587876.457\n",
      "Iteration 460: Loss = 13003213783130.426\n",
      "Iteration 461: Loss = 13003003039327.145\n",
      "Iteration 462: Loss = 13002792356448.742\n",
      "Iteration 463: Loss = 13002581734477.361\n",
      "Iteration 464: Loss = 13002371173395.139\n",
      "Iteration 465: Loss = 13002160673184.225\n",
      "Iteration 466: Loss = 13001950233826.771\n",
      "Iteration 467: Loss = 13001739855304.93\n",
      "Iteration 468: Loss = 13001529537600.875\n",
      "Iteration 469: Loss = 13001319280696.768\n",
      "Iteration 470: Loss = 13001109084574.783\n",
      "Iteration 471: Loss = 13000898949217.1\n",
      "Iteration 472: Loss = 13000688874605.9\n",
      "Iteration 473: Loss = 13000478860723.38\n",
      "Iteration 474: Loss = 13000268907551.73\n",
      "Iteration 475: Loss = 13000059015073.148\n",
      "Iteration 476: Loss = 12999849183269.842\n",
      "Iteration 477: Loss = 12999639412124.021\n",
      "Iteration 478: Loss = 12999429701617.902\n",
      "Iteration 479: Loss = 12999220051733.707\n",
      "Iteration 480: Loss = 12999010462453.658\n",
      "Iteration 481: Loss = 12998800933759.988\n",
      "Iteration 482: Loss = 12998591465634.941\n",
      "Iteration 483: Loss = 12998382058060.75\n",
      "Iteration 484: Loss = 12998172711019.666\n",
      "Iteration 485: Loss = 12997963424493.94\n",
      "Iteration 486: Loss = 12997754198465.832\n",
      "Iteration 487: Loss = 12997545032917.602\n",
      "Iteration 488: Loss = 12997335927831.52\n",
      "Iteration 489: Loss = 12997126883189.861\n",
      "Iteration 490: Loss = 12996917898974.902\n",
      "Iteration 491: Loss = 12996708975168.926\n",
      "Iteration 492: Loss = 12996500111754.225\n",
      "Iteration 493: Loss = 12996291308713.09\n",
      "Iteration 494: Loss = 12996082566027.824\n",
      "Iteration 495: Loss = 12995873883680.73\n",
      "Iteration 496: Loss = 12995665261654.121\n",
      "Iteration 497: Loss = 12995456699930.307\n",
      "Iteration 498: Loss = 12995248198491.613\n",
      "Iteration 499: Loss = 12995039757320.363\n",
      "Iteration 500: Loss = 12994831376398.889\n",
      "Iteration 501: Loss = 12994623055709.527\n",
      "Iteration 502: Loss = 12994414795234.615\n",
      "Iteration 503: Loss = 12994206594956.504\n",
      "Iteration 504: Loss = 12993998454857.547\n",
      "Iteration 505: Loss = 12993790374920.1\n",
      "Iteration 506: Loss = 12993582355126.523\n",
      "Iteration 507: Loss = 12993374395459.184\n",
      "Iteration 508: Loss = 12993166495900.457\n",
      "Iteration 509: Loss = 12992958656432.719\n",
      "Iteration 510: Loss = 12992750877038.355\n",
      "Iteration 511: Loss = 12992543157699.754\n",
      "Iteration 512: Loss = 12992335498399.307\n",
      "Iteration 513: Loss = 12992127899119.412\n",
      "Iteration 514: Loss = 12991920359842.479\n",
      "Iteration 515: Loss = 12991712880550.908\n",
      "Iteration 516: Loss = 12991505461227.123\n",
      "Iteration 517: Loss = 12991298101853.537\n",
      "Iteration 518: Loss = 12991090802412.576\n",
      "Iteration 519: Loss = 12990883562886.674\n",
      "Iteration 520: Loss = 12990676383258.26\n",
      "Iteration 521: Loss = 12990469263509.78\n",
      "Iteration 522: Loss = 12990262203623.678\n",
      "Iteration 523: Loss = 12990055203582.402\n",
      "Iteration 524: Loss = 12989848263368.408\n",
      "Iteration 525: Loss = 12989641382964.164\n",
      "Iteration 526: Loss = 12989434562352.129\n",
      "Iteration 527: Loss = 12989227801514.777\n",
      "Iteration 528: Loss = 12989021100434.586\n",
      "Iteration 529: Loss = 12988814459094.035\n",
      "Iteration 530: Loss = 12988607877475.611\n",
      "Iteration 531: Loss = 12988401355561.81\n",
      "Iteration 532: Loss = 12988194893335.13\n",
      "Iteration 533: Loss = 12987988490778.068\n",
      "Iteration 534: Loss = 12987782147873.137\n",
      "Iteration 535: Loss = 12987575864602.844\n",
      "Iteration 536: Loss = 12987369640949.713\n",
      "Iteration 537: Loss = 12987163476896.264\n",
      "Iteration 538: Loss = 12986957372425.027\n",
      "Iteration 539: Loss = 12986751327518.535\n",
      "Iteration 540: Loss = 12986545342159.324\n",
      "Iteration 541: Loss = 12986339416329.95\n",
      "Iteration 542: Loss = 12986133550012.943\n",
      "Iteration 543: Loss = 12985927743190.87\n",
      "Iteration 544: Loss = 12985721995846.287\n",
      "Iteration 545: Loss = 12985516307961.76\n",
      "Iteration 546: Loss = 12985310679519.857\n",
      "Iteration 547: Loss = 12985105110503.152\n",
      "Iteration 548: Loss = 12984899600894.229\n",
      "Iteration 549: Loss = 12984694150675.67\n",
      "Iteration 550: Loss = 12984488759830.066\n",
      "Iteration 551: Loss = 12984283428340.014\n",
      "Iteration 552: Loss = 12984078156188.107\n",
      "Iteration 553: Loss = 12983872943356.96\n",
      "Iteration 554: Loss = 12983667789829.184\n",
      "Iteration 555: Loss = 12983462695587.387\n",
      "Iteration 556: Loss = 12983257660614.197\n",
      "Iteration 557: Loss = 12983052684892.238\n",
      "Iteration 558: Loss = 12982847768404.143\n",
      "Iteration 559: Loss = 12982642911132.547\n",
      "Iteration 560: Loss = 12982438113060.088\n",
      "Iteration 561: Loss = 12982233374169.416\n",
      "Iteration 562: Loss = 12982028694443.19\n",
      "Iteration 563: Loss = 12981824073864.055\n",
      "Iteration 564: Loss = 12981619512414.68\n",
      "Iteration 565: Loss = 12981415010077.732\n",
      "Iteration 566: Loss = 12981210566835.883\n",
      "Iteration 567: Loss = 12981006182671.809\n",
      "Iteration 568: Loss = 12980801857568.195\n",
      "Iteration 569: Loss = 12980597591507.725\n",
      "Iteration 570: Loss = 12980393384473.096\n",
      "Iteration 571: Loss = 12980189236447.006\n",
      "Iteration 572: Loss = 12979985147412.154\n",
      "Iteration 573: Loss = 12979781117351.254\n",
      "Iteration 574: Loss = 12979577146247.016\n",
      "Iteration 575: Loss = 12979373234082.154\n",
      "Iteration 576: Loss = 12979169380839.402\n",
      "Iteration 577: Loss = 12978965586501.482\n",
      "Iteration 578: Loss = 12978761851051.13\n",
      "Iteration 579: Loss = 12978558174471.08\n",
      "Iteration 580: Loss = 12978354556744.08\n",
      "Iteration 581: Loss = 12978150997852.885\n",
      "Iteration 582: Loss = 12977947497780.236\n",
      "Iteration 583: Loss = 12977744056508.906\n",
      "Iteration 584: Loss = 12977540674021.65\n",
      "Iteration 585: Loss = 12977337350301.238\n",
      "Iteration 586: Loss = 12977134085330.45\n",
      "Iteration 587: Loss = 12976930879092.062\n",
      "Iteration 588: Loss = 12976727731568.86\n",
      "Iteration 589: Loss = 12976524642743.633\n",
      "Iteration 590: Loss = 12976321612599.176\n",
      "Iteration 591: Loss = 12976118641118.29\n",
      "Iteration 592: Loss = 12975915728283.777\n",
      "Iteration 593: Loss = 12975712874078.451\n",
      "Iteration 594: Loss = 12975510078485.127\n",
      "Iteration 595: Loss = 12975307341486.623\n",
      "Iteration 596: Loss = 12975104663065.768\n",
      "Iteration 597: Loss = 12974902043205.389\n",
      "Iteration 598: Loss = 12974699481888.318\n",
      "Iteration 599: Loss = 12974496979097.408\n",
      "Iteration 600: Loss = 12974294534815.494\n",
      "Iteration 601: Loss = 12974092149025.428\n",
      "Iteration 602: Loss = 12973889821710.07\n",
      "Iteration 603: Loss = 12973687552852.28\n",
      "Iteration 604: Loss = 12973485342434.918\n",
      "Iteration 605: Loss = 12973283190440.863\n",
      "Iteration 606: Loss = 12973081096852.99\n",
      "Iteration 607: Loss = 12972879061654.174\n",
      "Iteration 608: Loss = 12972677084827.307\n",
      "Iteration 609: Loss = 12972475166355.28\n",
      "Iteration 610: Loss = 12972273306220.984\n",
      "Iteration 611: Loss = 12972071504407.328\n",
      "Iteration 612: Loss = 12971869760897.217\n",
      "Iteration 613: Loss = 12971668075673.553\n",
      "Iteration 614: Loss = 12971466448719.266\n",
      "Iteration 615: Loss = 12971264880017.27\n",
      "Iteration 616: Loss = 12971063369550.496\n",
      "Iteration 617: Loss = 12970861917301.867\n",
      "Iteration 618: Loss = 12970660523254.33\n",
      "Iteration 619: Loss = 12970459187390.82\n",
      "Iteration 620: Loss = 12970257909694.287\n",
      "Iteration 621: Loss = 12970056690147.684\n",
      "Iteration 622: Loss = 12969855528733.963\n",
      "Iteration 623: Loss = 12969654425436.094\n",
      "Iteration 624: Loss = 12969453380237.037\n",
      "Iteration 625: Loss = 12969252393119.766\n",
      "Iteration 626: Loss = 12969051464067.256\n",
      "Iteration 627: Loss = 12968850593062.494\n",
      "Iteration 628: Loss = 12968649780088.463\n",
      "Iteration 629: Loss = 12968449025128.156\n",
      "Iteration 630: Loss = 12968248328164.572\n",
      "Iteration 631: Loss = 12968047689180.71\n",
      "Iteration 632: Loss = 12967847108159.582\n",
      "Iteration 633: Loss = 12967646585084.197\n",
      "Iteration 634: Loss = 12967446119937.57\n",
      "Iteration 635: Loss = 12967245712702.729\n",
      "Iteration 636: Loss = 12967045363362.695\n",
      "Iteration 637: Loss = 12966845071900.506\n",
      "Iteration 638: Loss = 12966644838299.197\n",
      "Iteration 639: Loss = 12966444662541.807\n",
      "Iteration 640: Loss = 12966244544611.39\n",
      "Iteration 641: Loss = 12966044484490.992\n",
      "Iteration 642: Loss = 12965844482163.678\n",
      "Iteration 643: Loss = 12965644537612.5\n",
      "Iteration 644: Loss = 12965444650820.535\n",
      "Iteration 645: Loss = 12965244821770.848\n",
      "Iteration 646: Loss = 12965045050446.521\n",
      "Iteration 647: Loss = 12964845336830.64\n",
      "Iteration 648: Loss = 12964645680906.283\n",
      "Iteration 649: Loss = 12964446082656.547\n",
      "Iteration 650: Loss = 12964246542064.531\n",
      "Iteration 651: Loss = 12964047059113.334\n",
      "Iteration 652: Loss = 12963847633786.066\n",
      "Iteration 653: Loss = 12963648266065.84\n",
      "Iteration 654: Loss = 12963448955935.77\n",
      "Iteration 655: Loss = 12963249703378.98\n",
      "Iteration 656: Loss = 12963050508378.598\n",
      "Iteration 657: Loss = 12962851370917.758\n",
      "Iteration 658: Loss = 12962652290979.592\n",
      "Iteration 659: Loss = 12962453268547.248\n",
      "Iteration 660: Loss = 12962254303603.867\n",
      "Iteration 661: Loss = 12962055396132.61\n",
      "Iteration 662: Loss = 12961856546116.623\n",
      "Iteration 663: Loss = 12961657753539.082\n",
      "Iteration 664: Loss = 12961459018383.143\n",
      "Iteration 665: Loss = 12961260340631.98\n",
      "Iteration 666: Loss = 12961061720268.773\n",
      "Iteration 667: Loss = 12960863157276.707\n",
      "Iteration 668: Loss = 12960664651638.959\n",
      "Iteration 669: Loss = 12960466203338.732\n",
      "Iteration 670: Loss = 12960267812359.217\n",
      "Iteration 671: Loss = 12960069478683.615\n",
      "Iteration 672: Loss = 12959871202295.137\n",
      "Iteration 673: Loss = 12959672983176.99\n",
      "Iteration 674: Loss = 12959474821312.396\n",
      "Iteration 675: Loss = 12959276716684.576\n",
      "Iteration 676: Loss = 12959078669276.756\n",
      "Iteration 677: Loss = 12958880679072.166\n",
      "Iteration 678: Loss = 12958682746054.045\n",
      "Iteration 679: Loss = 12958484870205.629\n",
      "Iteration 680: Loss = 12958287051510.172\n",
      "Iteration 681: Loss = 12958089289950.924\n",
      "Iteration 682: Loss = 12957891585511.139\n",
      "Iteration 683: Loss = 12957693938174.078\n",
      "Iteration 684: Loss = 12957496347923.014\n",
      "Iteration 685: Loss = 12957298814741.207\n",
      "Iteration 686: Loss = 12957101338611.943\n",
      "Iteration 687: Loss = 12956903919518.5\n",
      "Iteration 688: Loss = 12956706557444.164\n",
      "Iteration 689: Loss = 12956509252372.227\n",
      "Iteration 690: Loss = 12956312004285.982\n",
      "Iteration 691: Loss = 12956114813168.734\n",
      "Iteration 692: Loss = 12955917679003.785\n",
      "Iteration 693: Loss = 12955720601774.45\n",
      "Iteration 694: Loss = 12955523581464.041\n",
      "Iteration 695: Loss = 12955326618055.88\n",
      "Iteration 696: Loss = 12955129711533.295\n",
      "Iteration 697: Loss = 12954932861879.617\n",
      "Iteration 698: Loss = 12954736069078.174\n",
      "Iteration 699: Loss = 12954539333112.316\n",
      "Iteration 700: Loss = 12954342653965.379\n",
      "Iteration 701: Loss = 12954146031620.72\n",
      "Iteration 702: Loss = 12953949466061.693\n",
      "Iteration 703: Loss = 12953752957271.656\n",
      "Iteration 704: Loss = 12953556505233.98\n",
      "Iteration 705: Loss = 12953360109932.023\n",
      "Iteration 706: Loss = 12953163771349.174\n",
      "Iteration 707: Loss = 12952967489468.803\n",
      "Iteration 708: Loss = 12952771264274.299\n",
      "Iteration 709: Loss = 12952575095749.049\n",
      "Iteration 710: Loss = 12952378983876.451\n",
      "Iteration 711: Loss = 12952182928639.896\n",
      "Iteration 712: Loss = 12951986930022.8\n",
      "Iteration 713: Loss = 12951790988008.564\n",
      "Iteration 714: Loss = 12951595102580.607\n",
      "Iteration 715: Loss = 12951399273722.346\n",
      "Iteration 716: Loss = 12951203501417.201\n",
      "Iteration 717: Loss = 12951007785648.61\n",
      "Iteration 718: Loss = 12950812126399.992\n",
      "Iteration 719: Loss = 12950616523654.803\n",
      "Iteration 720: Loss = 12950420977396.477\n",
      "Iteration 721: Loss = 12950225487608.46\n",
      "Iteration 722: Loss = 12950030054274.213\n",
      "Iteration 723: Loss = 12949834677377.19\n",
      "Iteration 724: Loss = 12949639356900.852\n",
      "Iteration 725: Loss = 12949444092828.67\n",
      "Iteration 726: Loss = 12949248885144.113\n",
      "Iteration 727: Loss = 12949053733830.664\n",
      "Iteration 728: Loss = 12948858638871.805\n",
      "Iteration 729: Loss = 12948663600251.02\n",
      "Iteration 730: Loss = 12948468617951.803\n",
      "Iteration 731: Loss = 12948273691957.652\n",
      "Iteration 732: Loss = 12948078822252.066\n",
      "Iteration 733: Loss = 12947884008818.557\n",
      "Iteration 734: Loss = 12947689251640.633\n",
      "Iteration 735: Loss = 12947494550701.81\n",
      "Iteration 736: Loss = 12947299905985.615\n",
      "Iteration 737: Loss = 12947105317475.57\n",
      "Iteration 738: Loss = 12946910785155.205\n",
      "Iteration 739: Loss = 12946716309008.062\n",
      "Iteration 740: Loss = 12946521889017.678\n",
      "Iteration 741: Loss = 12946327525167.596\n",
      "Iteration 742: Loss = 12946133217441.373\n",
      "Iteration 743: Loss = 12945938965822.56\n",
      "Iteration 744: Loss = 12945744770294.723\n",
      "Iteration 745: Loss = 12945550630841.424\n",
      "Iteration 746: Loss = 12945356547446.227\n",
      "Iteration 747: Loss = 12945162520092.719\n",
      "Iteration 748: Loss = 12944968548764.473\n",
      "Iteration 749: Loss = 12944774633445.074\n",
      "Iteration 750: Loss = 12944580774118.113\n",
      "Iteration 751: Loss = 12944386970767.184\n",
      "Iteration 752: Loss = 12944193223375.889\n",
      "Iteration 753: Loss = 12943999531927.824\n",
      "Iteration 754: Loss = 12943805896406.605\n",
      "Iteration 755: Loss = 12943612316795.848\n",
      "Iteration 756: Loss = 12943418793079.164\n",
      "Iteration 757: Loss = 12943225325240.184\n",
      "Iteration 758: Loss = 12943031913262.53\n",
      "Iteration 759: Loss = 12942838557129.838\n",
      "Iteration 760: Loss = 12942645256825.748\n",
      "Iteration 761: Loss = 12942452012333.9\n",
      "Iteration 762: Loss = 12942258823637.945\n",
      "Iteration 763: Loss = 12942065690721.533\n",
      "Iteration 764: Loss = 12941872613568.32\n",
      "Iteration 765: Loss = 12941679592161.975\n",
      "Iteration 766: Loss = 12941486626486.156\n",
      "Iteration 767: Loss = 12941293716524.535\n",
      "Iteration 768: Loss = 12941100862260.8\n",
      "Iteration 769: Loss = 12940908063678.623\n",
      "Iteration 770: Loss = 12940715320761.693\n",
      "Iteration 771: Loss = 12940522633493.701\n",
      "Iteration 772: Loss = 12940330001858.342\n",
      "Iteration 773: Loss = 12940137425839.318\n",
      "Iteration 774: Loss = 12939944905420.334\n",
      "Iteration 775: Loss = 12939752440585.1\n",
      "Iteration 776: Loss = 12939560031317.328\n",
      "Iteration 777: Loss = 12939367677600.744\n",
      "Iteration 778: Loss = 12939175379419.076\n",
      "Iteration 779: Loss = 12938983136756.045\n",
      "Iteration 780: Loss = 12938790949595.389\n",
      "Iteration 781: Loss = 12938598817920.846\n",
      "Iteration 782: Loss = 12938406741716.166\n",
      "Iteration 783: Loss = 12938214720965.086\n",
      "Iteration 784: Loss = 12938022755651.373\n",
      "Iteration 785: Loss = 12937830845758.777\n",
      "Iteration 786: Loss = 12937638991271.062\n",
      "Iteration 787: Loss = 12937447192172.0\n",
      "Iteration 788: Loss = 12937255448445.361\n",
      "Iteration 789: Loss = 12937063760074.924\n",
      "Iteration 790: Loss = 12936872127044.469\n",
      "Iteration 791: Loss = 12936680549337.785\n",
      "Iteration 792: Loss = 12936489026938.668\n",
      "Iteration 793: Loss = 12936297559830.906\n",
      "Iteration 794: Loss = 12936106147998.312\n",
      "Iteration 795: Loss = 12935914791424.682\n",
      "Iteration 796: Loss = 12935723490093.83\n",
      "Iteration 797: Loss = 12935532243989.576\n",
      "Iteration 798: Loss = 12935341053095.736\n",
      "Iteration 799: Loss = 12935149917396.137\n",
      "Iteration 800: Loss = 12934958836874.613\n",
      "Iteration 801: Loss = 12934767811514.992\n",
      "Iteration 802: Loss = 12934576841301.121\n",
      "Iteration 803: Loss = 12934385926216.842\n",
      "Iteration 804: Loss = 12934195066246.004\n",
      "Iteration 805: Loss = 12934004261372.459\n",
      "Iteration 806: Loss = 12933813511580.07\n",
      "Iteration 807: Loss = 12933622816852.697\n",
      "Iteration 808: Loss = 12933432177174.213\n",
      "Iteration 809: Loss = 12933241592528.488\n",
      "Iteration 810: Loss = 12933051062899.398\n",
      "Iteration 811: Loss = 12932860588270.832\n",
      "Iteration 812: Loss = 12932670168626.674\n",
      "Iteration 813: Loss = 12932479803950.814\n",
      "Iteration 814: Loss = 12932289494227.154\n",
      "Iteration 815: Loss = 12932099239439.592\n",
      "Iteration 816: Loss = 12931909039572.037\n",
      "Iteration 817: Loss = 12931718894608.398\n",
      "Iteration 818: Loss = 12931528804532.594\n",
      "Iteration 819: Loss = 12931338769328.543\n",
      "Iteration 820: Loss = 12931148788980.172\n",
      "Iteration 821: Loss = 12930958863471.412\n",
      "Iteration 822: Loss = 12930768992786.197\n",
      "Iteration 823: Loss = 12930579176908.467\n",
      "Iteration 824: Loss = 12930389415822.168\n",
      "Iteration 825: Loss = 12930199709511.248\n",
      "Iteration 826: Loss = 12930010057959.662\n",
      "Iteration 827: Loss = 12929820461151.37\n",
      "Iteration 828: Loss = 12929630919070.328\n",
      "Iteration 829: Loss = 12929441431700.514\n",
      "Iteration 830: Loss = 12929251999025.895\n",
      "Iteration 831: Loss = 12929062621030.453\n",
      "Iteration 832: Loss = 12928873297698.168\n",
      "Iteration 833: Loss = 12928684029013.025\n",
      "Iteration 834: Loss = 12928494814959.02\n",
      "Iteration 835: Loss = 12928305655520.15\n",
      "Iteration 836: Loss = 12928116550680.414\n",
      "Iteration 837: Loss = 12927927500423.818\n",
      "Iteration 838: Loss = 12927738504734.375\n",
      "Iteration 839: Loss = 12927549563596.1\n",
      "Iteration 840: Loss = 12927360676993.012\n",
      "Iteration 841: Loss = 12927171844909.135\n",
      "Iteration 842: Loss = 12926983067328.5\n",
      "Iteration 843: Loss = 12926794344235.145\n",
      "Iteration 844: Loss = 12926605675613.105\n",
      "Iteration 845: Loss = 12926417061446.424\n",
      "Iteration 846: Loss = 12926228501719.152\n",
      "Iteration 847: Loss = 12926039996415.346\n",
      "Iteration 848: Loss = 12925851545519.053\n",
      "Iteration 849: Loss = 12925663149014.348\n",
      "Iteration 850: Loss = 12925474806885.291\n",
      "Iteration 851: Loss = 12925286519115.959\n",
      "Iteration 852: Loss = 12925098285690.428\n",
      "Iteration 853: Loss = 12924910106592.775\n",
      "Iteration 854: Loss = 12924721981807.09\n",
      "Iteration 855: Loss = 12924533911317.467\n",
      "Iteration 856: Loss = 12924345895107.994\n",
      "Iteration 857: Loss = 12924157933162.773\n",
      "Iteration 858: Loss = 12923970025465.918\n",
      "Iteration 859: Loss = 12923782172001.53\n",
      "Iteration 860: Loss = 12923594372753.725\n",
      "Iteration 861: Loss = 12923406627706.623\n",
      "Iteration 862: Loss = 12923218936844.348\n",
      "Iteration 863: Loss = 12923031300151.031\n",
      "Iteration 864: Loss = 12922843717610.8\n",
      "Iteration 865: Loss = 12922656189207.797\n",
      "Iteration 866: Loss = 12922468714926.166\n",
      "Iteration 867: Loss = 12922281294750.045\n",
      "Iteration 868: Loss = 12922093928663.596\n",
      "Iteration 869: Loss = 12921906616650.977\n",
      "Iteration 870: Loss = 12921719358696.338\n",
      "Iteration 871: Loss = 12921532154783.854\n",
      "Iteration 872: Loss = 12921345004897.691\n",
      "Iteration 873: Loss = 12921157909022.033\n",
      "Iteration 874: Loss = 12920970867141.049\n",
      "Iteration 875: Loss = 12920783879238.928\n",
      "Iteration 876: Loss = 12920596945299.863\n",
      "Iteration 877: Loss = 12920410065308.047\n",
      "Iteration 878: Loss = 12920223239247.674\n",
      "Iteration 879: Loss = 12920036467102.951\n",
      "Iteration 880: Loss = 12919849748858.084\n",
      "Iteration 881: Loss = 12919663084497.287\n",
      "Iteration 882: Loss = 12919476474004.777\n",
      "Iteration 883: Loss = 12919289917364.78\n",
      "Iteration 884: Loss = 12919103414561.52\n",
      "Iteration 885: Loss = 12918916965579.227\n",
      "Iteration 886: Loss = 12918730570402.135\n",
      "Iteration 887: Loss = 12918544229014.488\n",
      "Iteration 888: Loss = 12918357941400.531\n",
      "Iteration 889: Loss = 12918171707544.518\n",
      "Iteration 890: Loss = 12917985527430.697\n",
      "Iteration 891: Loss = 12917799401043.328\n",
      "Iteration 892: Loss = 12917613328366.68\n",
      "Iteration 893: Loss = 12917427309385.016\n",
      "Iteration 894: Loss = 12917241344082.613\n",
      "Iteration 895: Loss = 12917055432443.746\n",
      "Iteration 896: Loss = 12916869574452.703\n",
      "Iteration 897: Loss = 12916683770093.768\n",
      "Iteration 898: Loss = 12916498019351.229\n",
      "Iteration 899: Loss = 12916312322209.389\n",
      "Iteration 900: Loss = 12916126678652.543\n",
      "Iteration 901: Loss = 12915941088665.002\n",
      "Iteration 902: Loss = 12915755552231.072\n",
      "Iteration 903: Loss = 12915570069335.076\n",
      "Iteration 904: Loss = 12915384639961.32\n",
      "Iteration 905: Loss = 12915199264094.139\n",
      "Iteration 906: Loss = 12915013941717.861\n",
      "Iteration 907: Loss = 12914828672816.818\n",
      "Iteration 908: Loss = 12914643457375.348\n",
      "Iteration 909: Loss = 12914458295377.793\n",
      "Iteration 910: Loss = 12914273186808.5\n",
      "Iteration 911: Loss = 12914088131651.824\n",
      "Iteration 912: Loss = 12913903129892.117\n",
      "Iteration 913: Loss = 12913718181513.746\n",
      "Iteration 914: Loss = 12913533286501.076\n",
      "Iteration 915: Loss = 12913348444838.473\n",
      "Iteration 916: Loss = 12913163656510.318\n",
      "Iteration 917: Loss = 12912978921500.984\n",
      "Iteration 918: Loss = 12912794239794.863\n",
      "Iteration 919: Loss = 12912609611376.338\n",
      "Iteration 920: Loss = 12912425036229.805\n",
      "Iteration 921: Loss = 12912240514339.66\n",
      "Iteration 922: Loss = 12912056045690.312\n",
      "Iteration 923: Loss = 12911871630266.162\n",
      "Iteration 924: Loss = 12911687268051.623\n",
      "Iteration 925: Loss = 12911502959031.115\n",
      "Iteration 926: Loss = 12911318703189.06\n",
      "Iteration 927: Loss = 12911134500509.879\n",
      "Iteration 928: Loss = 12910950350978.004\n",
      "Iteration 929: Loss = 12910766254577.871\n",
      "Iteration 930: Loss = 12910582211293.92\n",
      "Iteration 931: Loss = 12910398221110.594\n",
      "Iteration 932: Loss = 12910214284012.344\n",
      "Iteration 933: Loss = 12910030399983.621\n",
      "Iteration 934: Loss = 12909846569008.88\n",
      "Iteration 935: Loss = 12909662791072.594\n",
      "Iteration 936: Loss = 12909479066159.223\n",
      "Iteration 937: Loss = 12909295394253.236\n",
      "Iteration 938: Loss = 12909111775339.115\n",
      "Iteration 939: Loss = 12908928209401.344\n",
      "Iteration 940: Loss = 12908744696424.4\n",
      "Iteration 941: Loss = 12908561236392.773\n",
      "Iteration 942: Loss = 12908377829290.967\n",
      "Iteration 943: Loss = 12908194475103.475\n",
      "Iteration 944: Loss = 12908011173814.8\n",
      "Iteration 945: Loss = 12907827925409.453\n",
      "Iteration 946: Loss = 12907644729871.95\n",
      "Iteration 947: Loss = 12907461587186.8\n",
      "Iteration 948: Loss = 12907278497338.531\n",
      "Iteration 949: Loss = 12907095460311.67\n",
      "Iteration 950: Loss = 12906912476090.748\n",
      "Iteration 951: Loss = 12906729544660.3\n",
      "Iteration 952: Loss = 12906546666004.865\n",
      "Iteration 953: Loss = 12906363840108.994\n",
      "Iteration 954: Loss = 12906181066957.227\n",
      "Iteration 955: Loss = 12905998346534.129\n",
      "Iteration 956: Loss = 12905815678824.248\n",
      "Iteration 957: Loss = 12905633063812.156\n",
      "Iteration 958: Loss = 12905450501482.416\n",
      "Iteration 959: Loss = 12905267991819.602\n",
      "Iteration 960: Loss = 12905085534808.29\n",
      "Iteration 961: Loss = 12904903130433.066\n",
      "Iteration 962: Loss = 12904720778678.508\n",
      "Iteration 963: Loss = 12904538479529.215\n",
      "Iteration 964: Loss = 12904356232969.775\n",
      "Iteration 965: Loss = 12904174038984.795\n",
      "Iteration 966: Loss = 12903991897558.87\n",
      "Iteration 967: Loss = 12903809808676.621\n",
      "Iteration 968: Loss = 12903627772322.65\n",
      "Iteration 969: Loss = 12903445788481.578\n",
      "Iteration 970: Loss = 12903263857138.031\n",
      "Iteration 971: Loss = 12903081978276.635\n",
      "Iteration 972: Loss = 12902900151882.018\n",
      "Iteration 973: Loss = 12902718377938.822\n",
      "Iteration 974: Loss = 12902536656431.682\n",
      "Iteration 975: Loss = 12902354987345.244\n",
      "Iteration 976: Loss = 12902173370664.164\n",
      "Iteration 977: Loss = 12901991806373.088\n",
      "Iteration 978: Loss = 12901810294456.68\n",
      "Iteration 979: Loss = 12901628834899.6\n",
      "Iteration 980: Loss = 12901447427686.518\n",
      "Iteration 981: Loss = 12901266072802.105\n",
      "Iteration 982: Loss = 12901084770231.04\n",
      "Iteration 983: Loss = 12900903519958.0\n",
      "Iteration 984: Loss = 12900722321967.678\n",
      "Iteration 985: Loss = 12900541176244.758\n",
      "Iteration 986: Loss = 12900360082773.936\n",
      "Iteration 987: Loss = 12900179041539.916\n",
      "Iteration 988: Loss = 12899998052527.398\n",
      "Iteration 989: Loss = 12899817115721.09\n",
      "Iteration 990: Loss = 12899636231105.709\n",
      "Iteration 991: Loss = 12899455398665.967\n",
      "Iteration 992: Loss = 12899274618386.592\n",
      "Iteration 993: Loss = 12899093890252.307\n",
      "Iteration 994: Loss = 12898913214247.846\n",
      "Iteration 995: Loss = 12898732590357.938\n",
      "Iteration 996: Loss = 12898552018567.332\n",
      "Iteration 997: Loss = 12898371498860.768\n",
      "Iteration 998: Loss = 12898191031222.99\n",
      "Iteration 999: Loss = 12898010615638.766\n",
      "Iteration 1000: Loss = 12897830252092.84\n",
      "Iteration 1001: Loss = 12897649940569.98\n",
      "Iteration 1002: Loss = 12897469681054.955\n",
      "Iteration 1003: Loss = 12897289473532.531\n",
      "Iteration 1004: Loss = 12897109317987.494\n",
      "Iteration 1005: Loss = 12896929214404.615\n",
      "Iteration 1006: Loss = 12896749162768.682\n",
      "Iteration 1007: Loss = 12896569163064.488\n",
      "Iteration 1008: Loss = 12896389215276.824\n",
      "Iteration 1009: Loss = 12896209319390.49\n",
      "Iteration 1010: Loss = 12896029475390.287\n",
      "Iteration 1011: Loss = 12895849683261.023\n",
      "Iteration 1012: Loss = 12895669942987.512\n",
      "Iteration 1013: Loss = 12895490254554.57\n",
      "Iteration 1014: Loss = 12895310617947.016\n",
      "Iteration 1015: Loss = 12895131033149.682\n",
      "Iteration 1016: Loss = 12894951500147.387\n",
      "Iteration 1017: Loss = 12894772018924.977\n",
      "Iteration 1018: Loss = 12894592589467.285\n",
      "Iteration 1019: Loss = 12894413211759.15\n",
      "Iteration 1020: Loss = 12894233885785.432\n",
      "Iteration 1021: Loss = 12894054611530.973\n",
      "Iteration 1022: Loss = 12893875388980.637\n",
      "Iteration 1023: Loss = 12893696218119.28\n",
      "Iteration 1024: Loss = 12893517098931.768\n",
      "Iteration 1025: Loss = 12893338031402.973\n",
      "Iteration 1026: Loss = 12893159015517.771\n",
      "Iteration 1027: Loss = 12892980051261.041\n",
      "Iteration 1028: Loss = 12892801138617.666\n",
      "Iteration 1029: Loss = 12892622277572.533\n",
      "Iteration 1030: Loss = 12892443468110.535\n",
      "Iteration 1031: Loss = 12892264710216.57\n",
      "Iteration 1032: Loss = 12892086003875.54\n",
      "Iteration 1033: Loss = 12891907349072.35\n",
      "Iteration 1034: Loss = 12891728745791.914\n",
      "Iteration 1035: Loss = 12891550194019.14\n",
      "Iteration 1036: Loss = 12891371693738.955\n",
      "Iteration 1037: Loss = 12891193244936.275\n",
      "Iteration 1038: Loss = 12891014847596.031\n",
      "Iteration 1039: Loss = 12890836501703.16\n",
      "Iteration 1040: Loss = 12890658207242.594\n",
      "Iteration 1041: Loss = 12890479964199.281\n",
      "Iteration 1042: Loss = 12890301772558.162\n",
      "Iteration 1043: Loss = 12890123632304.188\n",
      "Iteration 1044: Loss = 12889945543422.314\n",
      "Iteration 1045: Loss = 12889767505897.5\n",
      "Iteration 1046: Loss = 12889589519714.709\n",
      "Iteration 1047: Loss = 12889411584858.91\n",
      "Iteration 1048: Loss = 12889233701315.078\n",
      "Iteration 1049: Loss = 12889055869068.188\n",
      "Iteration 1050: Loss = 12888878088103.223\n",
      "Iteration 1051: Loss = 12888700358405.164\n",
      "Iteration 1052: Loss = 12888522679959.008\n",
      "Iteration 1053: Loss = 12888345052749.746\n",
      "Iteration 1054: Loss = 12888167476762.38\n",
      "Iteration 1055: Loss = 12887989951981.914\n",
      "Iteration 1056: Loss = 12887812478393.35\n",
      "Iteration 1057: Loss = 12887635055981.709\n",
      "Iteration 1058: Loss = 12887457684732.006\n",
      "Iteration 1059: Loss = 12887280364629.258\n",
      "Iteration 1060: Loss = 12887103095658.496\n",
      "Iteration 1061: Loss = 12886925877804.744\n",
      "Iteration 1062: Loss = 12886748711053.045\n",
      "Iteration 1063: Loss = 12886571595388.434\n",
      "Iteration 1064: Loss = 12886394530795.955\n",
      "Iteration 1065: Loss = 12886217517260.652\n",
      "Iteration 1066: Loss = 12886040554767.586\n",
      "Iteration 1067: Loss = 12885863643301.809\n",
      "Iteration 1068: Loss = 12885686782848.38\n",
      "Iteration 1069: Loss = 12885509973392.367\n",
      "Iteration 1070: Loss = 12885333214918.844\n",
      "Iteration 1071: Loss = 12885156507412.877\n",
      "Iteration 1072: Loss = 12884979850859.557\n",
      "Iteration 1073: Loss = 12884803245243.953\n",
      "Iteration 1074: Loss = 12884626690551.162\n",
      "Iteration 1075: Loss = 12884450186766.273\n",
      "Iteration 1076: Loss = 12884273733874.385\n",
      "Iteration 1077: Loss = 12884097331860.596\n",
      "Iteration 1078: Loss = 12883920980710.016\n",
      "Iteration 1079: Loss = 12883744680407.746\n",
      "Iteration 1080: Loss = 12883568430938.908\n",
      "Iteration 1081: Loss = 12883392232288.621\n",
      "Iteration 1082: Loss = 12883216084442.002\n",
      "Iteration 1083: Loss = 12883039987384.184\n",
      "Iteration 1084: Loss = 12882863941100.291\n",
      "Iteration 1085: Loss = 12882687945575.469\n",
      "Iteration 1086: Loss = 12882512000794.852\n",
      "Iteration 1087: Loss = 12882336106743.59\n",
      "Iteration 1088: Loss = 12882160263406.822\n",
      "Iteration 1089: Loss = 12881984470769.713\n",
      "Iteration 1090: Loss = 12881808728817.416\n",
      "Iteration 1091: Loss = 12881633037535.094\n",
      "Iteration 1092: Loss = 12881457396907.916\n",
      "Iteration 1093: Loss = 12881281806921.045\n",
      "Iteration 1094: Loss = 12881106267559.666\n",
      "Iteration 1095: Loss = 12880930778808.957\n",
      "Iteration 1096: Loss = 12880755340654.1\n",
      "Iteration 1097: Loss = 12880579953080.285\n",
      "Iteration 1098: Loss = 12880404616072.703\n",
      "Iteration 1099: Loss = 12880229329616.553\n",
      "Iteration 1100: Loss = 12880054093697.037\n",
      "Iteration 1101: Loss = 12879878908299.361\n",
      "Iteration 1102: Loss = 12879703773408.738\n",
      "Iteration 1103: Loss = 12879528689010.379\n",
      "Iteration 1104: Loss = 12879353655089.5\n",
      "Iteration 1105: Loss = 12879178671631.336\n",
      "Iteration 1106: Loss = 12879003738621.105\n",
      "Iteration 1107: Loss = 12878828856044.045\n",
      "Iteration 1108: Loss = 12878654023885.389\n",
      "Iteration 1109: Loss = 12878479242130.38\n",
      "Iteration 1110: Loss = 12878304510764.266\n",
      "Iteration 1111: Loss = 12878129829772.29\n",
      "Iteration 1112: Loss = 12877955199139.713\n",
      "Iteration 1113: Loss = 12877780618851.791\n",
      "Iteration 1114: Loss = 12877606088893.787\n",
      "Iteration 1115: Loss = 12877431609250.963\n",
      "Iteration 1116: Loss = 12877257179908.598\n",
      "Iteration 1117: Loss = 12877082800851.967\n",
      "Iteration 1118: Loss = 12876908472066.348\n",
      "Iteration 1119: Loss = 12876734193537.025\n",
      "Iteration 1120: Loss = 12876559965249.287\n",
      "Iteration 1121: Loss = 12876385787188.434\n",
      "Iteration 1122: Loss = 12876211659339.754\n",
      "Iteration 1123: Loss = 12876037581688.555\n",
      "Iteration 1124: Loss = 12875863554220.139\n",
      "Iteration 1125: Loss = 12875689576919.822\n",
      "Iteration 1126: Loss = 12875515649772.918\n",
      "Iteration 1127: Loss = 12875341772764.74\n",
      "Iteration 1128: Loss = 12875167945880.62\n",
      "Iteration 1129: Loss = 12874994169105.879\n",
      "Iteration 1130: Loss = 12874820442425.855\n",
      "Iteration 1131: Loss = 12874646765825.885\n",
      "Iteration 1132: Loss = 12874473139291.303\n",
      "Iteration 1133: Loss = 12874299562807.459\n",
      "Iteration 1134: Loss = 12874126036359.705\n",
      "Iteration 1135: Loss = 12873952559933.393\n",
      "Iteration 1136: Loss = 12873779133513.877\n",
      "Iteration 1137: Loss = 12873605757086.527\n",
      "Iteration 1138: Loss = 12873432430636.705\n",
      "Iteration 1139: Loss = 12873259154149.787\n",
      "Iteration 1140: Loss = 12873085927611.14\n",
      "Iteration 1141: Loss = 12872912751006.152\n",
      "Iteration 1142: Loss = 12872739624320.205\n",
      "Iteration 1143: Loss = 12872566547538.69\n",
      "Iteration 1144: Loss = 12872393520646.994\n",
      "Iteration 1145: Loss = 12872220543630.518\n",
      "Iteration 1146: Loss = 12872047616474.662\n",
      "Iteration 1147: Loss = 12871874739164.834\n",
      "Iteration 1148: Loss = 12871701911686.441\n",
      "Iteration 1149: Loss = 12871529134024.902\n",
      "Iteration 1150: Loss = 12871356406165.633\n",
      "Iteration 1151: Loss = 12871183728094.055\n",
      "Iteration 1152: Loss = 12871011099795.596\n",
      "Iteration 1153: Loss = 12870838521255.691\n",
      "Iteration 1154: Loss = 12870665992459.775\n",
      "Iteration 1155: Loss = 12870493513393.291\n",
      "Iteration 1156: Loss = 12870321084041.674\n",
      "Iteration 1157: Loss = 12870148704390.38\n",
      "Iteration 1158: Loss = 12869976374424.863\n",
      "Iteration 1159: Loss = 12869804094130.576\n",
      "Iteration 1160: Loss = 12869631863492.984\n",
      "Iteration 1161: Loss = 12869459682497.553\n",
      "Iteration 1162: Loss = 12869287551129.752\n",
      "Iteration 1163: Loss = 12869115469375.057\n",
      "Iteration 1164: Loss = 12868943437218.943\n",
      "Iteration 1165: Loss = 12868771454646.9\n",
      "Iteration 1166: Loss = 12868599521644.41\n",
      "Iteration 1167: Loss = 12868427638196.97\n",
      "Iteration 1168: Loss = 12868255804290.07\n",
      "Iteration 1169: Loss = 12868084019909.215\n",
      "Iteration 1170: Loss = 12867912285039.904\n",
      "Iteration 1171: Loss = 12867740599667.654\n",
      "Iteration 1172: Loss = 12867568963777.97\n",
      "Iteration 1173: Loss = 12867397377356.377\n",
      "Iteration 1174: Loss = 12867225840388.39\n",
      "Iteration 1175: Loss = 12867054352859.54\n",
      "Iteration 1176: Loss = 12866882914755.352\n",
      "Iteration 1177: Loss = 12866711526061.367\n",
      "Iteration 1178: Loss = 12866540186763.12\n",
      "Iteration 1179: Loss = 12866368896846.154\n",
      "Iteration 1180: Loss = 12866197656296.02\n",
      "Iteration 1181: Loss = 12866026465098.264\n",
      "Iteration 1182: Loss = 12865855323238.443\n",
      "Iteration 1183: Loss = 12865684230702.125\n",
      "Iteration 1184: Loss = 12865513187474.863\n",
      "Iteration 1185: Loss = 12865342193542.234\n",
      "Iteration 1186: Loss = 12865171248889.809\n",
      "Iteration 1187: Loss = 12865000353503.162\n",
      "Iteration 1188: Loss = 12864829507367.877\n",
      "Iteration 1189: Loss = 12864658710469.54\n",
      "Iteration 1190: Loss = 12864487962793.742\n",
      "Iteration 1191: Loss = 12864317264326.076\n",
      "Iteration 1192: Loss = 12864146615052.137\n",
      "Iteration 1193: Loss = 12863976014957.533\n",
      "Iteration 1194: Loss = 12863805464027.87\n",
      "Iteration 1195: Loss = 12863634962248.756\n",
      "Iteration 1196: Loss = 12863464509605.81\n",
      "Iteration 1197: Loss = 12863294106084.652\n",
      "Iteration 1198: Loss = 12863123751670.904\n",
      "Iteration 1199: Loss = 12862953446350.195\n",
      "Iteration 1200: Loss = 12862783190108.158\n",
      "Iteration 1201: Loss = 12862612982930.428\n",
      "Iteration 1202: Loss = 12862442824802.646\n",
      "Iteration 1203: Loss = 12862272715710.459\n",
      "Iteration 1204: Loss = 12862102655639.516\n",
      "Iteration 1205: Loss = 12861932644575.47\n",
      "Iteration 1206: Loss = 12861762682503.984\n",
      "Iteration 1207: Loss = 12861592769410.713\n",
      "Iteration 1208: Loss = 12861422905281.324\n",
      "Iteration 1209: Loss = 12861253090101.49\n",
      "Iteration 1210: Loss = 12861083323856.889\n",
      "Iteration 1211: Loss = 12860913606533.191\n",
      "Iteration 1212: Loss = 12860743938116.088\n",
      "Iteration 1213: Loss = 12860574318591.264\n",
      "Iteration 1214: Loss = 12860404747944.41\n",
      "Iteration 1215: Loss = 12860235226161.227\n",
      "Iteration 1216: Loss = 12860065753227.408\n",
      "Iteration 1217: Loss = 12859896329128.662\n",
      "Iteration 1218: Loss = 12859726953850.697\n",
      "Iteration 1219: Loss = 12859557627379.227\n",
      "Iteration 1220: Loss = 12859388349699.963\n",
      "Iteration 1221: Loss = 12859219120798.637\n",
      "Iteration 1222: Loss = 12859049940660.965\n",
      "Iteration 1223: Loss = 12858880809272.682\n",
      "Iteration 1224: Loss = 12858711726619.518\n",
      "Iteration 1225: Loss = 12858542692687.213\n",
      "Iteration 1226: Loss = 12858373707461.512\n",
      "Iteration 1227: Loss = 12858204770928.156\n",
      "Iteration 1228: Loss = 12858035883072.902\n",
      "Iteration 1229: Loss = 12857867043881.5\n",
      "Iteration 1230: Loss = 12857698253339.717\n",
      "Iteration 1231: Loss = 12857529511433.307\n",
      "Iteration 1232: Loss = 12857360818148.04\n",
      "Iteration 1233: Loss = 12857192173469.69\n",
      "Iteration 1234: Loss = 12857023577384.033\n",
      "Iteration 1235: Loss = 12856855029876.85\n",
      "Iteration 1236: Loss = 12856686530933.924\n",
      "Iteration 1237: Loss = 12856518080541.043\n",
      "Iteration 1238: Loss = 12856349678683.998\n",
      "Iteration 1239: Loss = 12856181325348.592\n",
      "Iteration 1240: Loss = 12856013020520.62\n",
      "Iteration 1241: Loss = 12855844764185.89\n",
      "Iteration 1242: Loss = 12855676556330.217\n",
      "Iteration 1243: Loss = 12855508396939.402\n",
      "Iteration 1244: Loss = 12855340285999.277\n",
      "Iteration 1245: Loss = 12855172223495.654\n",
      "Iteration 1246: Loss = 12855004209414.365\n",
      "Iteration 1247: Loss = 12854836243741.238\n",
      "Iteration 1248: Loss = 12854668326462.105\n",
      "Iteration 1249: Loss = 12854500457562.812\n",
      "Iteration 1250: Loss = 12854332637029.197\n",
      "Iteration 1251: Loss = 12854164864847.111\n",
      "Iteration 1252: Loss = 12853997141002.402\n",
      "Iteration 1253: Loss = 12853829465480.926\n",
      "Iteration 1254: Loss = 12853661838268.543\n",
      "Iteration 1255: Loss = 12853494259351.12\n",
      "Iteration 1256: Loss = 12853326728714.518\n",
      "Iteration 1257: Loss = 12853159246344.62\n",
      "Iteration 1258: Loss = 12852991812227.29\n",
      "Iteration 1259: Loss = 12852824426348.418\n",
      "Iteration 1260: Loss = 12852657088693.889\n",
      "Iteration 1261: Loss = 12852489799249.586\n",
      "Iteration 1262: Loss = 12852322558001.402\n",
      "Iteration 1263: Loss = 12852155364935.24\n",
      "Iteration 1264: Loss = 12851988220037.002\n",
      "Iteration 1265: Loss = 12851821123292.588\n",
      "Iteration 1266: Loss = 12851654074687.908\n",
      "Iteration 1267: Loss = 12851487074208.883\n",
      "Iteration 1268: Loss = 12851320121841.422\n",
      "Iteration 1269: Loss = 12851153217571.455\n",
      "Iteration 1270: Loss = 12850986361384.902\n",
      "Iteration 1271: Loss = 12850819553267.701\n",
      "Iteration 1272: Loss = 12850652793205.78\n",
      "Iteration 1273: Loss = 12850486081185.08\n",
      "Iteration 1274: Loss = 12850319417191.549\n",
      "Iteration 1275: Loss = 12850152801211.127\n",
      "Iteration 1276: Loss = 12849986233229.766\n",
      "Iteration 1277: Loss = 12849819713233.428\n",
      "Iteration 1278: Loss = 12849653241208.062\n",
      "Iteration 1279: Loss = 12849486817139.645\n",
      "Iteration 1280: Loss = 12849320441014.137\n",
      "Iteration 1281: Loss = 12849154112817.508\n",
      "Iteration 1282: Loss = 12848987832535.738\n",
      "Iteration 1283: Loss = 12848821600154.807\n",
      "Iteration 1284: Loss = 12848655415660.701\n",
      "Iteration 1285: Loss = 12848489279039.406\n",
      "Iteration 1286: Loss = 12848323190276.918\n",
      "Iteration 1287: Loss = 12848157149359.229\n",
      "Iteration 1288: Loss = 12847991156272.344\n",
      "Iteration 1289: Loss = 12847825211002.266\n",
      "Iteration 1290: Loss = 12847659313535.006\n",
      "Iteration 1291: Loss = 12847493463856.58\n",
      "Iteration 1292: Loss = 12847327661952.994\n",
      "Iteration 1293: Loss = 12847161907810.285\n",
      "Iteration 1294: Loss = 12846996201414.469\n",
      "Iteration 1295: Loss = 12846830542751.582\n",
      "Iteration 1296: Loss = 12846664931807.652\n",
      "Iteration 1297: Loss = 12846499368568.72\n",
      "Iteration 1298: Loss = 12846333853020.83\n",
      "Iteration 1299: Loss = 12846168385150.03\n",
      "Iteration 1300: Loss = 12846002964942.36\n",
      "Iteration 1301: Loss = 12845837592383.887\n",
      "Iteration 1302: Loss = 12845672267460.664\n",
      "Iteration 1303: Loss = 12845506990158.756\n",
      "Iteration 1304: Loss = 12845341760464.229\n",
      "Iteration 1305: Loss = 12845176578363.154\n",
      "Iteration 1306: Loss = 12845011443841.605\n",
      "Iteration 1307: Loss = 12844846356885.662\n",
      "Iteration 1308: Loss = 12844681317481.412\n",
      "Iteration 1309: Loss = 12844516325614.941\n",
      "Iteration 1310: Loss = 12844351381272.338\n",
      "Iteration 1311: Loss = 12844186484439.7\n",
      "Iteration 1312: Loss = 12844021635103.127\n",
      "Iteration 1313: Loss = 12843856833248.725\n",
      "Iteration 1314: Loss = 12843692078862.598\n",
      "Iteration 1315: Loss = 12843527371930.863\n",
      "Iteration 1316: Loss = 12843362712439.633\n",
      "Iteration 1317: Loss = 12843198100375.031\n",
      "Iteration 1318: Loss = 12843033535723.178\n",
      "Iteration 1319: Loss = 12842869018470.205\n",
      "Iteration 1320: Loss = 12842704548602.246\n",
      "Iteration 1321: Loss = 12842540126105.436\n",
      "Iteration 1322: Loss = 12842375750965.916\n",
      "Iteration 1323: Loss = 12842211423169.83\n",
      "Iteration 1324: Loss = 12842047142703.328\n",
      "Iteration 1325: Loss = 12841882909552.56\n",
      "Iteration 1326: Loss = 12841718723703.695\n",
      "Iteration 1327: Loss = 12841554585142.88\n",
      "Iteration 1328: Loss = 12841390493856.287\n",
      "Iteration 1329: Loss = 12841226449830.086\n",
      "Iteration 1330: Loss = 12841062453050.45\n",
      "Iteration 1331: Loss = 12840898503503.555\n",
      "Iteration 1332: Loss = 12840734601175.584\n",
      "Iteration 1333: Loss = 12840570746052.723\n",
      "Iteration 1334: Loss = 12840406938121.16\n",
      "Iteration 1335: Loss = 12840243177367.094\n",
      "Iteration 1336: Loss = 12840079463776.715\n",
      "Iteration 1337: Loss = 12839915797336.234\n",
      "Iteration 1338: Loss = 12839752178031.854\n",
      "Iteration 1339: Loss = 12839588605849.78\n",
      "Iteration 1340: Loss = 12839425080776.232\n",
      "Iteration 1341: Loss = 12839261602797.432\n",
      "Iteration 1342: Loss = 12839098171899.592\n",
      "Iteration 1343: Loss = 12838934788068.947\n",
      "Iteration 1344: Loss = 12838771451291.725\n",
      "Iteration 1345: Loss = 12838608161554.16\n",
      "Iteration 1346: Loss = 12838444918842.492\n",
      "Iteration 1347: Loss = 12838281723142.963\n",
      "Iteration 1348: Loss = 12838118574441.82\n",
      "Iteration 1349: Loss = 12837955472725.314\n",
      "Iteration 1350: Loss = 12837792417979.7\n",
      "Iteration 1351: Loss = 12837629410191.236\n",
      "Iteration 1352: Loss = 12837466449346.188\n",
      "Iteration 1353: Loss = 12837303535430.822\n",
      "Iteration 1354: Loss = 12837140668431.408\n",
      "Iteration 1355: Loss = 12836977848334.22\n",
      "Iteration 1356: Loss = 12836815075125.541\n",
      "Iteration 1357: Loss = 12836652348791.654\n",
      "Iteration 1358: Loss = 12836489669318.844\n",
      "Iteration 1359: Loss = 12836327036693.404\n",
      "Iteration 1360: Loss = 12836164450901.627\n",
      "Iteration 1361: Loss = 12836001911929.818\n",
      "Iteration 1362: Loss = 12835839419764.275\n",
      "Iteration 1363: Loss = 12835676974391.31\n",
      "Iteration 1364: Loss = 12835514575797.23\n",
      "Iteration 1365: Loss = 12835352223968.355\n",
      "Iteration 1366: Loss = 12835189918891.006\n",
      "Iteration 1367: Loss = 12835027660551.5\n",
      "Iteration 1368: Loss = 12834865448936.172\n",
      "Iteration 1369: Loss = 12834703284031.348\n",
      "Iteration 1370: Loss = 12834541165823.371\n",
      "Iteration 1371: Loss = 12834379094298.572\n",
      "Iteration 1372: Loss = 12834217069443.303\n",
      "Iteration 1373: Loss = 12834055091243.908\n",
      "Iteration 1374: Loss = 12833893159686.742\n",
      "Iteration 1375: Loss = 12833731274758.154\n",
      "Iteration 1376: Loss = 12833569436444.514\n",
      "Iteration 1377: Loss = 12833407644732.178\n",
      "Iteration 1378: Loss = 12833245899607.518\n",
      "Iteration 1379: Loss = 12833084201056.906\n",
      "Iteration 1380: Loss = 12832922549066.719\n",
      "Iteration 1381: Loss = 12832760943623.334\n",
      "Iteration 1382: Loss = 12832599384713.14\n",
      "Iteration 1383: Loss = 12832437872322.521\n",
      "Iteration 1384: Loss = 12832276406437.871\n",
      "Iteration 1385: Loss = 12832114987045.59\n",
      "Iteration 1386: Loss = 12831953614132.068\n",
      "Iteration 1387: Loss = 12831792287683.72\n",
      "Iteration 1388: Loss = 12831631007686.951\n",
      "Iteration 1389: Loss = 12831469774128.172\n",
      "Iteration 1390: Loss = 12831308586993.799\n",
      "Iteration 1391: Loss = 12831147446270.254\n",
      "Iteration 1392: Loss = 12830986351943.959\n",
      "Iteration 1393: Loss = 12830825304001.348\n",
      "Iteration 1394: Loss = 12830664302428.848\n",
      "Iteration 1395: Loss = 12830503347212.895\n",
      "Iteration 1396: Loss = 12830342438339.934\n",
      "Iteration 1397: Loss = 12830181575796.408\n",
      "Iteration 1398: Loss = 12830020759568.76\n",
      "Iteration 1399: Loss = 12829859989643.45\n",
      "Iteration 1400: Loss = 12829699266006.932\n",
      "Iteration 1401: Loss = 12829538588645.666\n",
      "Iteration 1402: Loss = 12829377957546.113\n",
      "Iteration 1403: Loss = 12829217372694.748\n",
      "Iteration 1404: Loss = 12829056834078.037\n",
      "Iteration 1405: Loss = 12828896341682.463\n",
      "Iteration 1406: Loss = 12828735895494.5\n",
      "Iteration 1407: Loss = 12828575495500.635\n",
      "Iteration 1408: Loss = 12828415141687.357\n",
      "Iteration 1409: Loss = 12828254834041.16\n",
      "Iteration 1410: Loss = 12828094572548.535\n",
      "Iteration 1411: Loss = 12827934357195.988\n",
      "Iteration 1412: Loss = 12827774187970.021\n",
      "Iteration 1413: Loss = 12827614064857.14\n",
      "Iteration 1414: Loss = 12827453987843.863\n",
      "Iteration 1415: Loss = 12827293956916.701\n",
      "Iteration 1416: Loss = 12827133972062.176\n",
      "Iteration 1417: Loss = 12826974033266.812\n",
      "Iteration 1418: Loss = 12826814140517.137\n",
      "Iteration 1419: Loss = 12826654293799.688\n",
      "Iteration 1420: Loss = 12826494493100.99\n",
      "Iteration 1421: Loss = 12826334738407.596\n",
      "Iteration 1422: Loss = 12826175029706.041\n",
      "Iteration 1423: Loss = 12826015366982.877\n",
      "Iteration 1424: Loss = 12825855750224.656\n",
      "Iteration 1425: Loss = 12825696179417.934\n",
      "Iteration 1426: Loss = 12825536654549.27\n",
      "Iteration 1427: Loss = 12825377175605.227\n",
      "Iteration 1428: Loss = 12825217742572.375\n",
      "Iteration 1429: Loss = 12825058355437.285\n",
      "Iteration 1430: Loss = 12824899014186.531\n",
      "Iteration 1431: Loss = 12824739718806.697\n",
      "Iteration 1432: Loss = 12824580469284.367\n",
      "Iteration 1433: Loss = 12824421265606.123\n",
      "Iteration 1434: Loss = 12824262107758.56\n",
      "Iteration 1435: Loss = 12824102995728.275\n",
      "Iteration 1436: Loss = 12823943929501.863\n",
      "Iteration 1437: Loss = 12823784909065.934\n",
      "Iteration 1438: Loss = 12823625934407.094\n",
      "Iteration 1439: Loss = 12823467005511.951\n",
      "Iteration 1440: Loss = 12823308122367.121\n",
      "Iteration 1441: Loss = 12823149284959.227\n",
      "Iteration 1442: Loss = 12822990493274.885\n",
      "Iteration 1443: Loss = 12822831747300.734\n",
      "Iteration 1444: Loss = 12822673047023.395\n",
      "Iteration 1445: Loss = 12822514392429.508\n",
      "Iteration 1446: Loss = 12822355783505.71\n",
      "Iteration 1447: Loss = 12822197220238.645\n",
      "Iteration 1448: Loss = 12822038702614.957\n",
      "Iteration 1449: Loss = 12821880230621.303\n",
      "Iteration 1450: Loss = 12821721804244.338\n",
      "Iteration 1451: Loss = 12821563423470.713\n",
      "Iteration 1452: Loss = 12821405088287.094\n",
      "Iteration 1453: Loss = 12821246798680.15\n",
      "Iteration 1454: Loss = 12821088554636.555\n",
      "Iteration 1455: Loss = 12820930356142.973\n",
      "Iteration 1456: Loss = 12820772203186.094\n",
      "Iteration 1457: Loss = 12820614095752.592\n",
      "Iteration 1458: Loss = 12820456033829.156\n",
      "Iteration 1459: Loss = 12820298017402.477\n",
      "Iteration 1460: Loss = 12820140046459.252\n",
      "Iteration 1461: Loss = 12819982120986.172\n",
      "Iteration 1462: Loss = 12819824240969.947\n",
      "Iteration 1463: Loss = 12819666406397.275\n",
      "Iteration 1464: Loss = 12819508617254.875\n",
      "Iteration 1465: Loss = 12819350873529.453\n",
      "Iteration 1466: Loss = 12819193175207.73\n",
      "Iteration 1467: Loss = 12819035522276.43\n",
      "Iteration 1468: Loss = 12818877914722.271\n",
      "Iteration 1469: Loss = 12818720352531.992\n",
      "Iteration 1470: Loss = 12818562835692.32\n",
      "Iteration 1471: Loss = 12818405364189.996\n",
      "Iteration 1472: Loss = 12818247938011.758\n",
      "Iteration 1473: Loss = 12818090557144.354\n",
      "Iteration 1474: Loss = 12817933221574.531\n",
      "Iteration 1475: Loss = 12817775931289.045\n",
      "Iteration 1476: Loss = 12817618686274.648\n",
      "Iteration 1477: Loss = 12817461486518.105\n",
      "Iteration 1478: Loss = 12817304332006.18\n",
      "Iteration 1479: Loss = 12817147222725.643\n",
      "Iteration 1480: Loss = 12816990158663.262\n",
      "Iteration 1481: Loss = 12816833139805.816\n",
      "Iteration 1482: Loss = 12816676166140.086\n",
      "Iteration 1483: Loss = 12816519237652.86\n",
      "Iteration 1484: Loss = 12816362354330.914\n",
      "Iteration 1485: Loss = 12816205516161.053\n",
      "Iteration 1486: Loss = 12816048723130.066\n",
      "Iteration 1487: Loss = 12815891975224.756\n",
      "Iteration 1488: Loss = 12815735272431.926\n",
      "Iteration 1489: Loss = 12815578614738.38\n",
      "Iteration 1490: Loss = 12815422002130.938\n",
      "Iteration 1491: Loss = 12815265434596.404\n",
      "Iteration 1492: Loss = 12815108912121.607\n",
      "Iteration 1493: Loss = 12814952434693.37\n",
      "Iteration 1494: Loss = 12814796002298.512\n",
      "Iteration 1495: Loss = 12814639614923.87\n",
      "Iteration 1496: Loss = 12814483272556.28\n",
      "Iteration 1497: Loss = 12814326975182.572\n",
      "Iteration 1498: Loss = 12814170722789.6\n",
      "Iteration 1499: Loss = 12814014515364.205\n",
      "Iteration 1500: Loss = 12813858352893.24\n",
      "Iteration 1501: Loss = 12813702235363.557\n",
      "Iteration 1502: Loss = 12813546162762.012\n",
      "Iteration 1503: Loss = 12813390135075.469\n",
      "Iteration 1504: Loss = 12813234152290.799\n",
      "Iteration 1505: Loss = 12813078214394.863\n",
      "Iteration 1506: Loss = 12812922321374.54\n",
      "Iteration 1507: Loss = 12812766473216.707\n",
      "Iteration 1508: Loss = 12812610669908.246\n",
      "Iteration 1509: Loss = 12812454911436.041\n",
      "Iteration 1510: Loss = 12812299197786.98\n",
      "Iteration 1511: Loss = 12812143528947.959\n",
      "Iteration 1512: Loss = 12811987904905.871\n",
      "Iteration 1513: Loss = 12811832325647.621\n",
      "Iteration 1514: Loss = 12811676791160.113\n",
      "Iteration 1515: Loss = 12811521301430.254\n",
      "Iteration 1516: Loss = 12811365856444.955\n",
      "Iteration 1517: Loss = 12811210456191.135\n",
      "Iteration 1518: Loss = 12811055100655.715\n",
      "Iteration 1519: Loss = 12810899789825.615\n",
      "Iteration 1520: Loss = 12810744523687.766\n",
      "Iteration 1521: Loss = 12810589302229.098\n",
      "Iteration 1522: Loss = 12810434125436.545\n",
      "Iteration 1523: Loss = 12810278993297.049\n",
      "Iteration 1524: Loss = 12810123905797.557\n",
      "Iteration 1525: Loss = 12809968862925.006\n",
      "Iteration 1526: Loss = 12809813864666.355\n",
      "Iteration 1527: Loss = 12809658911008.559\n",
      "Iteration 1528: Loss = 12809504001938.57\n",
      "Iteration 1529: Loss = 12809349137443.357\n",
      "Iteration 1530: Loss = 12809194317509.885\n",
      "Iteration 1531: Loss = 12809039542125.123\n",
      "Iteration 1532: Loss = 12808884811276.045\n",
      "Iteration 1533: Loss = 12808730124949.627\n",
      "Iteration 1534: Loss = 12808575483132.86\n",
      "Iteration 1535: Loss = 12808420885812.713\n",
      "Iteration 1536: Loss = 12808266332976.19\n",
      "Iteration 1537: Loss = 12808111824610.28\n",
      "Iteration 1538: Loss = 12807957360701.977\n",
      "Iteration 1539: Loss = 12807802941238.283\n",
      "Iteration 1540: Loss = 12807648566206.205\n",
      "Iteration 1541: Loss = 12807494235592.752\n",
      "Iteration 1542: Loss = 12807339949384.934\n",
      "Iteration 1543: Loss = 12807185707569.768\n",
      "Iteration 1544: Loss = 12807031510134.275\n",
      "Iteration 1545: Loss = 12806877357065.477\n",
      "Iteration 1546: Loss = 12806723248350.406\n",
      "Iteration 1547: Loss = 12806569183976.086\n",
      "Iteration 1548: Loss = 12806415163929.56\n",
      "Iteration 1549: Loss = 12806261188197.865\n",
      "Iteration 1550: Loss = 12806107256768.043\n",
      "Iteration 1551: Loss = 12805953369627.139\n",
      "Iteration 1552: Loss = 12805799526762.209\n",
      "Iteration 1553: Loss = 12805645728160.3\n",
      "Iteration 1554: Loss = 12805491973808.48\n",
      "Iteration 1555: Loss = 12805338263693.803\n",
      "Iteration 1556: Loss = 12805184597803.34\n",
      "Iteration 1557: Loss = 12805030976124.158\n",
      "Iteration 1558: Loss = 12804877398643.33\n",
      "Iteration 1559: Loss = 12804723865347.938\n",
      "Iteration 1560: Loss = 12804570376225.057\n",
      "Iteration 1561: Loss = 12804416931261.773\n",
      "Iteration 1562: Loss = 12804263530445.182\n",
      "Iteration 1563: Loss = 12804110173762.371\n",
      "Iteration 1564: Loss = 12803956861200.436\n",
      "Iteration 1565: Loss = 12803803592746.477\n",
      "Iteration 1566: Loss = 12803650368387.604\n",
      "Iteration 1567: Loss = 12803497188110.918\n",
      "Iteration 1568: Loss = 12803344051903.533\n",
      "Iteration 1569: Loss = 12803190959752.564\n",
      "Iteration 1570: Loss = 12803037911645.135\n",
      "Iteration 1571: Loss = 12802884907568.36\n",
      "Iteration 1572: Loss = 12802731947509.377\n",
      "Iteration 1573: Loss = 12802579031455.305\n",
      "Iteration 1574: Loss = 12802426159393.287\n",
      "Iteration 1575: Loss = 12802273331310.46\n",
      "Iteration 1576: Loss = 12802120547193.963\n",
      "Iteration 1577: Loss = 12801967807030.947\n",
      "Iteration 1578: Loss = 12801815110808.557\n",
      "Iteration 1579: Loss = 12801662458513.95\n",
      "Iteration 1580: Loss = 12801509850134.275\n",
      "Iteration 1581: Loss = 12801357285656.703\n",
      "Iteration 1582: Loss = 12801204765068.396\n",
      "Iteration 1583: Loss = 12801052288356.523\n",
      "Iteration 1584: Loss = 12800899855508.254\n",
      "Iteration 1585: Loss = 12800747466510.768\n",
      "Iteration 1586: Loss = 12800595121351.24\n",
      "Iteration 1587: Loss = 12800442820016.863\n",
      "Iteration 1588: Loss = 12800290562494.814\n",
      "Iteration 1589: Loss = 12800138348772.293\n",
      "Iteration 1590: Loss = 12799986178836.488\n",
      "Iteration 1591: Loss = 12799834052674.604\n",
      "Iteration 1592: Loss = 12799681970273.838\n",
      "Iteration 1593: Loss = 12799529931621.406\n",
      "Iteration 1594: Loss = 12799377936704.506\n",
      "Iteration 1595: Loss = 12799225985510.36\n",
      "Iteration 1596: Loss = 12799074078026.184\n",
      "Iteration 1597: Loss = 12798922214239.197\n",
      "Iteration 1598: Loss = 12798770394136.63\n",
      "Iteration 1599: Loss = 12798618617705.71\n",
      "Iteration 1600: Loss = 12798466884933.662\n",
      "Iteration 1601: Loss = 12798315195807.736\n",
      "Iteration 1602: Loss = 12798163550315.162\n",
      "Iteration 1603: Loss = 12798011948443.186\n",
      "Iteration 1604: Loss = 12797860390179.062\n",
      "Iteration 1605: Loss = 12797708875510.037\n",
      "Iteration 1606: Loss = 12797557404423.367\n",
      "Iteration 1607: Loss = 12797405976906.309\n",
      "Iteration 1608: Loss = 12797254592946.129\n",
      "Iteration 1609: Loss = 12797103252530.092\n",
      "Iteration 1610: Loss = 12796951955645.47\n",
      "Iteration 1611: Loss = 12796800702279.535\n",
      "Iteration 1612: Loss = 12796649492419.57\n",
      "Iteration 1613: Loss = 12796498326052.854\n",
      "Iteration 1614: Loss = 12796347203166.672\n",
      "Iteration 1615: Loss = 12796196123748.31\n",
      "Iteration 1616: Loss = 12796045087785.068\n",
      "Iteration 1617: Loss = 12795894095264.234\n",
      "Iteration 1618: Loss = 12795743146173.12\n",
      "Iteration 1619: Loss = 12795592240499.02\n",
      "Iteration 1620: Loss = 12795441378229.244\n",
      "Iteration 1621: Loss = 12795290559351.111\n",
      "Iteration 1622: Loss = 12795139783851.926\n",
      "Iteration 1623: Loss = 12794989051719.016\n",
      "Iteration 1624: Loss = 12794838362939.703\n",
      "Iteration 1625: Loss = 12794687717501.307\n",
      "Iteration 1626: Loss = 12794537115391.168\n",
      "Iteration 1627: Loss = 12794386556596.615\n",
      "Iteration 1628: Loss = 12794236041104.986\n",
      "Iteration 1629: Loss = 12794085568903.625\n",
      "Iteration 1630: Loss = 12793935139979.875\n",
      "Iteration 1631: Loss = 12793784754321.086\n",
      "Iteration 1632: Loss = 12793634411914.611\n",
      "Iteration 1633: Loss = 12793484112747.805\n",
      "Iteration 1634: Loss = 12793333856808.033\n",
      "Iteration 1635: Loss = 12793183644082.654\n",
      "Iteration 1636: Loss = 12793033474559.04\n",
      "Iteration 1637: Loss = 12792883348224.559\n",
      "Iteration 1638: Loss = 12792733265066.584\n",
      "Iteration 1639: Loss = 12792583225072.502\n",
      "Iteration 1640: Loss = 12792433228229.69\n",
      "Iteration 1641: Loss = 12792283274525.533\n",
      "Iteration 1642: Loss = 12792133363947.43\n",
      "Iteration 1643: Loss = 12791983496482.768\n",
      "Iteration 1644: Loss = 12791833672118.94\n",
      "Iteration 1645: Loss = 12791683890843.357\n",
      "Iteration 1646: Loss = 12791534152643.416\n",
      "Iteration 1647: Loss = 12791384457506.533\n",
      "Iteration 1648: Loss = 12791234805420.117\n",
      "Iteration 1649: Loss = 12791085196371.586\n",
      "Iteration 1650: Loss = 12790935630348.354\n",
      "Iteration 1651: Loss = 12790786107337.848\n",
      "Iteration 1652: Loss = 12790636627327.498\n",
      "Iteration 1653: Loss = 12790487190304.732\n",
      "Iteration 1654: Loss = 12790337796256.988\n",
      "Iteration 1655: Loss = 12790188445171.7\n",
      "Iteration 1656: Loss = 12790039137036.316\n",
      "Iteration 1657: Loss = 12789889871838.273\n",
      "Iteration 1658: Loss = 12789740649565.027\n",
      "Iteration 1659: Loss = 12789591470204.033\n",
      "Iteration 1660: Loss = 12789442333742.742\n",
      "Iteration 1661: Loss = 12789293240168.621\n",
      "Iteration 1662: Loss = 12789144189469.127\n",
      "Iteration 1663: Loss = 12788995181631.734\n",
      "Iteration 1664: Loss = 12788846216643.912\n",
      "Iteration 1665: Loss = 12788697294493.137\n",
      "Iteration 1666: Loss = 12788548415166.887\n",
      "Iteration 1667: Loss = 12788399578652.645\n",
      "Iteration 1668: Loss = 12788250784937.896\n",
      "Iteration 1669: Loss = 12788102034010.137\n",
      "Iteration 1670: Loss = 12787953325856.854\n",
      "Iteration 1671: Loss = 12787804660465.549\n",
      "Iteration 1672: Loss = 12787656037823.717\n",
      "Iteration 1673: Loss = 12787507457918.873\n",
      "Iteration 1674: Loss = 12787358920738.521\n",
      "Iteration 1675: Loss = 12787210426270.172\n",
      "Iteration 1676: Loss = 12787061974501.344\n",
      "Iteration 1677: Loss = 12786913565419.555\n",
      "Iteration 1678: Loss = 12786765199012.328\n",
      "Iteration 1679: Loss = 12786616875267.193\n",
      "Iteration 1680: Loss = 12786468594171.678\n",
      "Iteration 1681: Loss = 12786320355713.318\n",
      "Iteration 1682: Loss = 12786172159879.652\n",
      "Iteration 1683: Loss = 12786024006658.225\n",
      "Iteration 1684: Loss = 12785875896036.576\n",
      "Iteration 1685: Loss = 12785727828002.256\n",
      "Iteration 1686: Loss = 12785579802542.822\n",
      "Iteration 1687: Loss = 12785431819645.828\n",
      "Iteration 1688: Loss = 12785283879298.834\n",
      "Iteration 1689: Loss = 12785135981489.398\n",
      "Iteration 1690: Loss = 12784988126205.098\n",
      "Iteration 1691: Loss = 12784840313433.5\n",
      "Iteration 1692: Loss = 12784692543162.178\n",
      "Iteration 1693: Loss = 12784544815378.71\n",
      "Iteration 1694: Loss = 12784397130070.682\n",
      "Iteration 1695: Loss = 12784249487225.68\n",
      "Iteration 1696: Loss = 12784101886831.287\n",
      "Iteration 1697: Loss = 12783954328875.105\n",
      "Iteration 1698: Loss = 12783806813344.725\n",
      "Iteration 1699: Loss = 12783659340227.746\n",
      "Iteration 1700: Loss = 12783511909511.775\n",
      "Iteration 1701: Loss = 12783364521184.422\n",
      "Iteration 1702: Loss = 12783217175233.295\n",
      "Iteration 1703: Loss = 12783069871646.014\n",
      "Iteration 1704: Loss = 12782922610410.191\n",
      "Iteration 1705: Loss = 12782775391513.451\n",
      "Iteration 1706: Loss = 12782628214943.424\n",
      "Iteration 1707: Loss = 12782481080687.732\n",
      "Iteration 1708: Loss = 12782333988734.016\n",
      "Iteration 1709: Loss = 12782186939069.908\n",
      "Iteration 1710: Loss = 12782039931683.053\n",
      "Iteration 1711: Loss = 12781892966561.09\n",
      "Iteration 1712: Loss = 12781746043691.672\n",
      "Iteration 1713: Loss = 12781599163062.447\n",
      "Iteration 1714: Loss = 12781452324661.072\n",
      "Iteration 1715: Loss = 12781305528475.207\n",
      "Iteration 1716: Loss = 12781158774492.512\n",
      "Iteration 1717: Loss = 12781012062700.652\n",
      "Iteration 1718: Loss = 12780865393087.307\n",
      "Iteration 1719: Loss = 12780718765640.137\n",
      "Iteration 1720: Loss = 12780572180346.824\n",
      "Iteration 1721: Loss = 12780425637195.05\n",
      "Iteration 1722: Loss = 12780279136172.5\n",
      "Iteration 1723: Loss = 12780132677266.861\n",
      "Iteration 1724: Loss = 12779986260465.828\n",
      "Iteration 1725: Loss = 12779839885757.088\n",
      "Iteration 1726: Loss = 12779693553128.346\n",
      "Iteration 1727: Loss = 12779547262567.303\n",
      "Iteration 1728: Loss = 12779401014061.664\n",
      "Iteration 1729: Loss = 12779254807599.14\n",
      "Iteration 1730: Loss = 12779108643167.45\n",
      "Iteration 1731: Loss = 12778962520754.3\n",
      "Iteration 1732: Loss = 12778816440347.418\n",
      "Iteration 1733: Loss = 12778670401934.53\n",
      "Iteration 1734: Loss = 12778524405503.355\n",
      "Iteration 1735: Loss = 12778378451041.633\n",
      "Iteration 1736: Loss = 12778232538537.098\n",
      "Iteration 1737: Loss = 12778086667977.484\n",
      "Iteration 1738: Loss = 12777940839350.535\n",
      "Iteration 1739: Loss = 12777795052644.002\n",
      "Iteration 1740: Loss = 12777649307845.629\n",
      "Iteration 1741: Loss = 12777503604943.172\n",
      "Iteration 1742: Loss = 12777357943924.385\n",
      "Iteration 1743: Loss = 12777212324777.031\n",
      "Iteration 1744: Loss = 12777066747488.877\n",
      "Iteration 1745: Loss = 12776921212047.684\n",
      "Iteration 1746: Loss = 12776775718441.227\n",
      "Iteration 1747: Loss = 12776630266657.281\n",
      "Iteration 1748: Loss = 12776484856683.621\n",
      "Iteration 1749: Loss = 12776339488508.037\n",
      "Iteration 1750: Loss = 12776194162118.307\n",
      "Iteration 1751: Loss = 12776048877502.22\n",
      "Iteration 1752: Loss = 12775903634647.578\n",
      "Iteration 1753: Loss = 12775758433542.166\n",
      "Iteration 1754: Loss = 12775613274173.793\n",
      "Iteration 1755: Loss = 12775468156530.258\n",
      "Iteration 1756: Loss = 12775323080599.37\n",
      "Iteration 1757: Loss = 12775178046368.94\n",
      "Iteration 1758: Loss = 12775033053826.78\n",
      "Iteration 1759: Loss = 12774888102960.713\n",
      "Iteration 1760: Loss = 12774743193758.557\n",
      "Iteration 1761: Loss = 12774598326208.137\n",
      "Iteration 1762: Loss = 12774453500297.283\n",
      "Iteration 1763: Loss = 12774308716013.828\n",
      "Iteration 1764: Loss = 12774163973345.604\n",
      "Iteration 1765: Loss = 12774019272280.459\n",
      "Iteration 1766: Loss = 12773874612806.23\n",
      "Iteration 1767: Loss = 12773729994910.766\n",
      "Iteration 1768: Loss = 12773585418581.916\n",
      "Iteration 1769: Loss = 12773440883807.533\n",
      "Iteration 1770: Loss = 12773296390575.477\n",
      "Iteration 1771: Loss = 12773151938873.611\n",
      "Iteration 1772: Loss = 12773007528689.795\n",
      "Iteration 1773: Loss = 12772863160011.896\n",
      "Iteration 1774: Loss = 12772718832827.795\n",
      "Iteration 1775: Loss = 12772574547125.357\n",
      "Iteration 1776: Loss = 12772430302892.47\n",
      "Iteration 1777: Loss = 12772286100117.012\n",
      "Iteration 1778: Loss = 12772141938786.867\n",
      "Iteration 1779: Loss = 12771997818889.93\n",
      "Iteration 1780: Loss = 12771853740414.092\n",
      "Iteration 1781: Loss = 12771709703347.25\n",
      "Iteration 1782: Loss = 12771565707677.3\n",
      "Iteration 1783: Loss = 12771421753392.154\n",
      "Iteration 1784: Loss = 12771277840479.713\n",
      "Iteration 1785: Loss = 12771133968927.895\n",
      "Iteration 1786: Loss = 12770990138724.61\n",
      "Iteration 1787: Loss = 12770846349857.775\n",
      "Iteration 1788: Loss = 12770702602315.318\n",
      "Iteration 1789: Loss = 12770558896085.158\n",
      "Iteration 1790: Loss = 12770415231155.227\n",
      "Iteration 1791: Loss = 12770271607513.457\n",
      "Iteration 1792: Loss = 12770128025147.785\n",
      "Iteration 1793: Loss = 12769984484046.15\n",
      "Iteration 1794: Loss = 12769840984196.494\n",
      "Iteration 1795: Loss = 12769697525586.768\n",
      "Iteration 1796: Loss = 12769554108204.916\n",
      "Iteration 1797: Loss = 12769410732038.895\n",
      "Iteration 1798: Loss = 12769267397076.664\n",
      "Iteration 1799: Loss = 12769124103306.188\n",
      "Iteration 1800: Loss = 12768980850715.42\n",
      "Iteration 1801: Loss = 12768837639292.338\n",
      "Iteration 1802: Loss = 12768694469024.91\n",
      "Iteration 1803: Loss = 12768551339901.11\n",
      "Iteration 1804: Loss = 12768408251908.922\n",
      "Iteration 1805: Loss = 12768265205036.318\n",
      "Iteration 1806: Loss = 12768122199271.297\n",
      "Iteration 1807: Loss = 12767979234601.84\n",
      "Iteration 1808: Loss = 12767836311015.941\n",
      "Iteration 1809: Loss = 12767693428501.598\n",
      "Iteration 1810: Loss = 12767550587046.809\n",
      "Iteration 1811: Loss = 12767407786639.582\n",
      "Iteration 1812: Loss = 12767265027267.92\n",
      "Iteration 1813: Loss = 12767122308919.834\n",
      "Iteration 1814: Loss = 12766979631583.34\n",
      "Iteration 1815: Loss = 12766836995246.453\n",
      "Iteration 1816: Loss = 12766694399897.2\n",
      "Iteration 1817: Loss = 12766551845523.598\n",
      "Iteration 1818: Loss = 12766409332113.68\n",
      "Iteration 1819: Loss = 12766266859655.48\n",
      "Iteration 1820: Loss = 12766124428137.027\n",
      "Iteration 1821: Loss = 12765982037546.365\n",
      "Iteration 1822: Loss = 12765839687871.533\n",
      "Iteration 1823: Loss = 12765697379100.58\n",
      "Iteration 1824: Loss = 12765555111221.557\n",
      "Iteration 1825: Loss = 12765412884222.51\n",
      "Iteration 1826: Loss = 12765270698091.5\n",
      "Iteration 1827: Loss = 12765128552816.59\n",
      "Iteration 1828: Loss = 12764986448385.84\n",
      "Iteration 1829: Loss = 12764844384787.318\n",
      "Iteration 1830: Loss = 12764702362009.092\n",
      "Iteration 1831: Loss = 12764560380039.24\n",
      "Iteration 1832: Loss = 12764418438865.836\n",
      "Iteration 1833: Loss = 12764276538476.967\n",
      "Iteration 1834: Loss = 12764134678860.713\n",
      "Iteration 1835: Loss = 12763992860005.164\n",
      "Iteration 1836: Loss = 12763851081898.408\n",
      "Iteration 1837: Loss = 12763709344528.545\n",
      "Iteration 1838: Loss = 12763567647883.674\n",
      "Iteration 1839: Loss = 12763425991951.895\n",
      "Iteration 1840: Loss = 12763284376721.309\n",
      "Iteration 1841: Loss = 12763142802180.037\n",
      "Iteration 1842: Loss = 12763001268316.18\n",
      "Iteration 1843: Loss = 12762859775117.863\n",
      "Iteration 1844: Loss = 12762718322573.201\n",
      "Iteration 1845: Loss = 12762576910670.318\n",
      "Iteration 1846: Loss = 12762435539397.346\n",
      "Iteration 1847: Loss = 12762294208742.406\n",
      "Iteration 1848: Loss = 12762152918693.639\n",
      "Iteration 1849: Loss = 12762011669239.178\n",
      "Iteration 1850: Loss = 12761870460367.168\n",
      "Iteration 1851: Loss = 12761729292065.75\n",
      "Iteration 1852: Loss = 12761588164323.076\n",
      "Iteration 1853: Loss = 12761447077127.29\n",
      "Iteration 1854: Loss = 12761306030466.553\n",
      "Iteration 1855: Loss = 12761165024329.021\n",
      "Iteration 1856: Loss = 12761024058702.86\n",
      "Iteration 1857: Loss = 12760883133576.225\n",
      "Iteration 1858: Loss = 12760742248937.295\n",
      "Iteration 1859: Loss = 12760601404774.236\n",
      "Iteration 1860: Loss = 12760460601075.229\n",
      "Iteration 1861: Loss = 12760319837828.451\n",
      "Iteration 1862: Loss = 12760179115022.084\n",
      "Iteration 1863: Loss = 12760038432644.314\n",
      "Iteration 1864: Loss = 12759897790683.328\n",
      "Iteration 1865: Loss = 12759757189127.328\n",
      "Iteration 1866: Loss = 12759616627964.502\n",
      "Iteration 1867: Loss = 12759476107183.053\n",
      "Iteration 1868: Loss = 12759335626771.186\n",
      "Iteration 1869: Loss = 12759195186717.107\n",
      "Iteration 1870: Loss = 12759054787009.027\n",
      "Iteration 1871: Loss = 12758914427635.158\n",
      "Iteration 1872: Loss = 12758774108583.72\n",
      "Iteration 1873: Loss = 12758633829842.936\n",
      "Iteration 1874: Loss = 12758493591401.025\n",
      "Iteration 1875: Loss = 12758353393246.219\n",
      "Iteration 1876: Loss = 12758213235366.746\n",
      "Iteration 1877: Loss = 12758073117750.846\n",
      "Iteration 1878: Loss = 12757933040386.75\n",
      "Iteration 1879: Loss = 12757793003262.709\n",
      "Iteration 1880: Loss = 12757653006366.965\n",
      "Iteration 1881: Loss = 12757513049687.762\n",
      "Iteration 1882: Loss = 12757373133213.355\n",
      "Iteration 1883: Loss = 12757233256932.002\n",
      "Iteration 1884: Loss = 12757093420831.959\n",
      "Iteration 1885: Loss = 12756953624901.494\n",
      "Iteration 1886: Loss = 12756813869128.865\n",
      "Iteration 1887: Loss = 12756674153502.348\n",
      "Iteration 1888: Loss = 12756534478010.215\n",
      "Iteration 1889: Loss = 12756394842640.742\n",
      "Iteration 1890: Loss = 12756255247382.205\n",
      "Iteration 1891: Loss = 12756115692222.895\n",
      "Iteration 1892: Loss = 12755976177151.092\n",
      "Iteration 1893: Loss = 12755836702155.09\n",
      "Iteration 1894: Loss = 12755697267223.182\n",
      "Iteration 1895: Loss = 12755557872343.662\n",
      "Iteration 1896: Loss = 12755418517504.838\n",
      "Iteration 1897: Loss = 12755279202695.01\n",
      "Iteration 1898: Loss = 12755139927902.48\n",
      "Iteration 1899: Loss = 12755000693115.57\n",
      "Iteration 1900: Loss = 12754861498322.588\n",
      "Iteration 1901: Loss = 12754722343511.85\n",
      "Iteration 1902: Loss = 12754583228671.682\n",
      "Iteration 1903: Loss = 12754444153790.408\n",
      "Iteration 1904: Loss = 12754305118856.352\n",
      "Iteration 1905: Loss = 12754166123857.852\n",
      "Iteration 1906: Loss = 12754027168783.24\n",
      "Iteration 1907: Loss = 12753888253620.854\n",
      "Iteration 1908: Loss = 12753749378359.037\n",
      "Iteration 1909: Loss = 12753610542986.133\n",
      "Iteration 1910: Loss = 12753471747490.492\n",
      "Iteration 1911: Loss = 12753332991860.467\n",
      "Iteration 1912: Loss = 12753194276084.414\n",
      "Iteration 1913: Loss = 12753055600150.691\n",
      "Iteration 1914: Loss = 12752916964047.66\n",
      "Iteration 1915: Loss = 12752778367763.688\n",
      "Iteration 1916: Loss = 12752639811287.146\n",
      "Iteration 1917: Loss = 12752501294606.406\n",
      "Iteration 1918: Loss = 12752362817709.842\n",
      "Iteration 1919: Loss = 12752224380585.836\n",
      "Iteration 1920: Loss = 12752085983222.771\n",
      "Iteration 1921: Loss = 12751947625609.035\n",
      "Iteration 1922: Loss = 12751809307733.016\n",
      "Iteration 1923: Loss = 12751671029583.107\n",
      "Iteration 1924: Loss = 12751532791147.707\n",
      "Iteration 1925: Loss = 12751394592415.213\n",
      "Iteration 1926: Loss = 12751256433374.03\n",
      "Iteration 1927: Loss = 12751118314012.572\n",
      "Iteration 1928: Loss = 12750980234319.24\n",
      "Iteration 1929: Loss = 12750842194282.451\n",
      "Iteration 1930: Loss = 12750704193890.623\n",
      "Iteration 1931: Loss = 12750566233132.18\n",
      "Iteration 1932: Loss = 12750428311995.541\n",
      "Iteration 1933: Loss = 12750290430469.14\n",
      "Iteration 1934: Loss = 12750152588541.402\n",
      "Iteration 1935: Loss = 12750014786200.762\n",
      "Iteration 1936: Loss = 12749877023435.662\n",
      "Iteration 1937: Loss = 12749739300234.537\n",
      "Iteration 1938: Loss = 12749601616585.838\n",
      "Iteration 1939: Loss = 12749463972478.012\n",
      "Iteration 1940: Loss = 12749326367899.508\n",
      "Iteration 1941: Loss = 12749188802838.783\n",
      "Iteration 1942: Loss = 12749051277284.297\n",
      "Iteration 1943: Loss = 12748913791224.506\n",
      "Iteration 1944: Loss = 12748776344647.877\n",
      "Iteration 1945: Loss = 12748638937542.883\n",
      "Iteration 1946: Loss = 12748501569897.992\n",
      "Iteration 1947: Loss = 12748364241701.682\n",
      "Iteration 1948: Loss = 12748226952942.428\n",
      "Iteration 1949: Loss = 12748089703608.715\n",
      "Iteration 1950: Loss = 12747952493689.03\n",
      "Iteration 1951: Loss = 12747815323171.86\n",
      "Iteration 1952: Loss = 12747678192045.693\n",
      "Iteration 1953: Loss = 12747541100299.031\n",
      "Iteration 1954: Loss = 12747404047920.371\n",
      "Iteration 1955: Loss = 12747267034898.217\n",
      "Iteration 1956: Loss = 12747130061221.074\n",
      "Iteration 1957: Loss = 12746993126877.45\n",
      "Iteration 1958: Loss = 12746856231855.857\n",
      "Iteration 1959: Loss = 12746719376144.814\n",
      "Iteration 1960: Loss = 12746582559732.844\n",
      "Iteration 1961: Loss = 12746445782608.459\n",
      "Iteration 1962: Loss = 12746309044760.195\n",
      "Iteration 1963: Loss = 12746172346176.576\n",
      "Iteration 1964: Loss = 12746035686846.137\n",
      "Iteration 1965: Loss = 12745899066757.412\n",
      "Iteration 1966: Loss = 12745762485898.947\n",
      "Iteration 1967: Loss = 12745625944259.281\n",
      "Iteration 1968: Loss = 12745489441826.957\n",
      "Iteration 1969: Loss = 12745352978590.533\n",
      "Iteration 1970: Loss = 12745216554538.559\n",
      "Iteration 1971: Loss = 12745080169659.588\n",
      "Iteration 1972: Loss = 12744943823942.184\n",
      "Iteration 1973: Loss = 12744807517374.906\n",
      "Iteration 1974: Loss = 12744671249946.326\n",
      "Iteration 1975: Loss = 12744535021645.014\n",
      "Iteration 1976: Loss = 12744398832459.537\n",
      "Iteration 1977: Loss = 12744262682378.48\n",
      "Iteration 1978: Loss = 12744126571390.42\n",
      "Iteration 1979: Loss = 12743990499483.943\n",
      "Iteration 1980: Loss = 12743854466647.629\n",
      "Iteration 1981: Loss = 12743718472870.074\n",
      "Iteration 1982: Loss = 12743582518139.871\n",
      "Iteration 1983: Loss = 12743446602445.62\n",
      "Iteration 1984: Loss = 12743310725775.914\n",
      "Iteration 1985: Loss = 12743174888119.361\n",
      "Iteration 1986: Loss = 12743039089464.568\n",
      "Iteration 1987: Loss = 12742903329800.146\n",
      "Iteration 1988: Loss = 12742767609114.71\n",
      "Iteration 1989: Loss = 12742631927396.877\n",
      "Iteration 1990: Loss = 12742496284635.266\n",
      "Iteration 1991: Loss = 12742360680818.5\n",
      "Iteration 1992: Loss = 12742225115935.207\n",
      "Iteration 1993: Loss = 12742089589974.02\n",
      "Iteration 1994: Loss = 12741954102923.572\n",
      "Iteration 1995: Loss = 12741818654772.5\n",
      "Iteration 1996: Loss = 12741683245509.443\n",
      "Iteration 1997: Loss = 12741547875123.049\n",
      "Iteration 1998: Loss = 12741412543601.963\n",
      "Iteration 1999: Loss = 12741277250934.834\n",
      "Iteration 2000: Loss = 12741141997110.318\n",
      "Iteration 2001: Loss = 12741006782117.074\n",
      "Iteration 2002: Loss = 12740871605943.762\n",
      "Iteration 2003: Loss = 12740736468579.045\n",
      "Iteration 2004: Loss = 12740601370011.59\n",
      "Iteration 2005: Loss = 12740466310230.07\n",
      "Iteration 2006: Loss = 12740331289223.156\n",
      "Iteration 2007: Loss = 12740196306979.531\n",
      "Iteration 2008: Loss = 12740061363487.871\n",
      "Iteration 2009: Loss = 12739926458736.863\n",
      "Iteration 2010: Loss = 12739791592715.193\n",
      "Iteration 2011: Loss = 12739656765411.553\n",
      "Iteration 2012: Loss = 12739521976814.633\n",
      "Iteration 2013: Loss = 12739387226913.137\n",
      "Iteration 2014: Loss = 12739252515695.762\n",
      "Iteration 2015: Loss = 12739117843151.213\n",
      "Iteration 2016: Loss = 12738983209268.197\n",
      "Iteration 2017: Loss = 12738848614035.428\n",
      "Iteration 2018: Loss = 12738714057441.615\n",
      "Iteration 2019: Loss = 12738579539475.48\n",
      "Iteration 2020: Loss = 12738445060125.742\n",
      "Iteration 2021: Loss = 12738310619381.125\n",
      "Iteration 2022: Loss = 12738176217230.355\n",
      "Iteration 2023: Loss = 12738041853662.168\n",
      "Iteration 2024: Loss = 12737907528665.291\n",
      "Iteration 2025: Loss = 12737773242228.47\n",
      "Iteration 2026: Loss = 12737638994340.44\n",
      "Iteration 2027: Loss = 12737504784989.945\n",
      "Iteration 2028: Loss = 12737370614165.736\n",
      "Iteration 2029: Loss = 12737236481856.559\n",
      "Iteration 2030: Loss = 12737102388051.17\n",
      "Iteration 2031: Loss = 12736968332738.328\n",
      "Iteration 2032: Loss = 12736834315906.793\n",
      "Iteration 2033: Loss = 12736700337545.33\n",
      "Iteration 2034: Loss = 12736566397642.705\n",
      "Iteration 2035: Loss = 12736432496187.684\n",
      "Iteration 2036: Loss = 12736298633169.049\n",
      "Iteration 2037: Loss = 12736164808575.576\n",
      "Iteration 2038: Loss = 12736031022396.043\n",
      "Iteration 2039: Loss = 12735897274619.232\n",
      "Iteration 2040: Loss = 12735763565233.934\n",
      "Iteration 2041: Loss = 12735629894228.94\n",
      "Iteration 2042: Loss = 12735496261593.04\n",
      "Iteration 2043: Loss = 12735362667315.035\n",
      "Iteration 2044: Loss = 12735229111383.719\n",
      "Iteration 2045: Loss = 12735095593787.904\n",
      "Iteration 2046: Loss = 12734962114516.395\n",
      "Iteration 2047: Loss = 12734828673557.996\n",
      "Iteration 2048: Loss = 12734695270901.527\n",
      "Iteration 2049: Loss = 12734561906535.805\n",
      "Iteration 2050: Loss = 12734428580449.648\n",
      "Iteration 2051: Loss = 12734295292631.879\n",
      "Iteration 2052: Loss = 12734162043071.324\n",
      "Iteration 2053: Loss = 12734028831756.818\n",
      "Iteration 2054: Loss = 12733895658677.19\n",
      "Iteration 2055: Loss = 12733762523821.28\n",
      "Iteration 2056: Loss = 12733629427177.922\n",
      "Iteration 2057: Loss = 12733496368735.963\n",
      "Iteration 2058: Loss = 12733363348484.252\n",
      "Iteration 2059: Loss = 12733230366411.637\n",
      "Iteration 2060: Loss = 12733097422506.97\n",
      "Iteration 2061: Loss = 12732964516759.11\n",
      "Iteration 2062: Loss = 12732831649156.912\n",
      "Iteration 2063: Loss = 12732698819689.244\n",
      "Iteration 2064: Loss = 12732566028344.973\n",
      "Iteration 2065: Loss = 12732433275112.96\n",
      "Iteration 2066: Loss = 12732300559982.088\n",
      "Iteration 2067: Loss = 12732167882941.227\n",
      "Iteration 2068: Loss = 12732035243979.258\n",
      "Iteration 2069: Loss = 12731902643085.062\n",
      "Iteration 2070: Loss = 12731770080247.531\n",
      "Iteration 2071: Loss = 12731637555455.549\n",
      "Iteration 2072: Loss = 12731505068698.008\n",
      "Iteration 2073: Loss = 12731372619963.805\n",
      "Iteration 2074: Loss = 12731240209241.842\n",
      "Iteration 2075: Loss = 12731107836521.018\n",
      "Iteration 2076: Loss = 12730975501790.238\n",
      "Iteration 2077: Loss = 12730843205038.418\n",
      "Iteration 2078: Loss = 12730710946254.459\n",
      "Iteration 2079: Loss = 12730578725427.281\n",
      "Iteration 2080: Loss = 12730446542545.809\n",
      "Iteration 2081: Loss = 12730314397598.955\n",
      "Iteration 2082: Loss = 12730182290575.652\n",
      "Iteration 2083: Loss = 12730050221464.824\n",
      "Iteration 2084: Loss = 12729918190255.406\n",
      "Iteration 2085: Loss = 12729786196936.33\n",
      "Iteration 2086: Loss = 12729654241496.533\n",
      "Iteration 2087: Loss = 12729522323924.96\n",
      "Iteration 2088: Loss = 12729390444210.559\n",
      "Iteration 2089: Loss = 12729258602342.271\n",
      "Iteration 2090: Loss = 12729126798309.05\n",
      "Iteration 2091: Loss = 12728995032099.854\n",
      "Iteration 2092: Loss = 12728863303703.633\n",
      "Iteration 2093: Loss = 12728731613109.354\n",
      "Iteration 2094: Loss = 12728599960305.982\n",
      "Iteration 2095: Loss = 12728468345282.48\n",
      "Iteration 2096: Loss = 12728336768027.828\n",
      "Iteration 2097: Loss = 12728205228530.988\n",
      "Iteration 2098: Loss = 12728073726780.947\n",
      "Iteration 2099: Loss = 12727942262766.676\n",
      "Iteration 2100: Loss = 12727810836477.168\n",
      "Iteration 2101: Loss = 12727679447901.408\n",
      "Iteration 2102: Loss = 12727548097028.383\n",
      "Iteration 2103: Loss = 12727416783847.09\n",
      "Iteration 2104: Loss = 12727285508346.525\n",
      "Iteration 2105: Loss = 12727154270515.684\n",
      "Iteration 2106: Loss = 12727023070343.578\n",
      "Iteration 2107: Loss = 12726891907819.207\n",
      "Iteration 2108: Loss = 12726760782931.588\n",
      "Iteration 2109: Loss = 12726629695669.727\n",
      "Iteration 2110: Loss = 12726498646022.643\n",
      "Iteration 2111: Loss = 12726367633979.354\n",
      "Iteration 2112: Loss = 12726236659528.885\n",
      "Iteration 2113: Loss = 12726105722660.26\n",
      "Iteration 2114: Loss = 12725974823362.51\n",
      "Iteration 2115: Loss = 12725843961624.668\n",
      "Iteration 2116: Loss = 12725713137435.768\n",
      "Iteration 2117: Loss = 12725582350784.848\n",
      "Iteration 2118: Loss = 12725451601660.955\n",
      "Iteration 2119: Loss = 12725320890053.13\n",
      "Iteration 2120: Loss = 12725190215950.422\n",
      "Iteration 2121: Loss = 12725059579341.885\n",
      "Iteration 2122: Loss = 12724928980216.57\n",
      "Iteration 2123: Loss = 12724798418563.541\n",
      "Iteration 2124: Loss = 12724667894371.857\n",
      "Iteration 2125: Loss = 12724537407630.582\n",
      "Iteration 2126: Loss = 12724406958328.781\n",
      "Iteration 2127: Loss = 12724276546455.533\n",
      "Iteration 2128: Loss = 12724146171999.908\n",
      "Iteration 2129: Loss = 12724015834950.984\n",
      "Iteration 2130: Loss = 12723885535297.838\n",
      "Iteration 2131: Loss = 12723755273029.56\n",
      "Iteration 2132: Loss = 12723625048135.236\n",
      "Iteration 2133: Loss = 12723494860603.955\n",
      "Iteration 2134: Loss = 12723364710424.814\n",
      "Iteration 2135: Loss = 12723234597586.904\n",
      "Iteration 2136: Loss = 12723104522079.332\n",
      "Iteration 2137: Loss = 12722974483891.197\n",
      "Iteration 2138: Loss = 12722844483011.604\n",
      "Iteration 2139: Loss = 12722714519429.67\n",
      "Iteration 2140: Loss = 12722584593134.5\n",
      "Iteration 2141: Loss = 12722454704115.217\n",
      "Iteration 2142: Loss = 12722324852360.932\n",
      "Iteration 2143: Loss = 12722195037860.777\n",
      "Iteration 2144: Loss = 12722065260603.875\n",
      "Iteration 2145: Loss = 12721935520579.354\n",
      "Iteration 2146: Loss = 12721805817776.346\n",
      "Iteration 2147: Loss = 12721676152183.984\n",
      "Iteration 2148: Loss = 12721546523791.41\n",
      "Iteration 2149: Loss = 12721416932587.77\n",
      "Iteration 2150: Loss = 12721287378562.2\n",
      "Iteration 2151: Loss = 12721157861703.854\n",
      "Iteration 2152: Loss = 12721028382001.88\n",
      "Iteration 2153: Loss = 12720898939445.44\n",
      "Iteration 2154: Loss = 12720769534023.682\n",
      "Iteration 2155: Loss = 12720640165725.775\n",
      "Iteration 2156: Loss = 12720510834540.88\n",
      "Iteration 2157: Loss = 12720381540458.166\n",
      "Iteration 2158: Loss = 12720252283466.8\n",
      "Iteration 2159: Loss = 12720123063555.959\n",
      "Iteration 2160: Loss = 12719993880714.818\n",
      "Iteration 2161: Loss = 12719864734932.56\n",
      "Iteration 2162: Loss = 12719735626198.37\n",
      "Iteration 2163: Loss = 12719606554501.428\n",
      "Iteration 2164: Loss = 12719477519830.928\n",
      "Iteration 2165: Loss = 12719348522176.064\n",
      "Iteration 2166: Loss = 12719219561526.03\n",
      "Iteration 2167: Loss = 12719090637870.025\n",
      "Iteration 2168: Loss = 12718961751197.256\n",
      "Iteration 2169: Loss = 12718832901496.922\n",
      "Iteration 2170: Loss = 12718704088758.236\n",
      "Iteration 2171: Loss = 12718575312970.41\n",
      "Iteration 2172: Loss = 12718446574122.66\n",
      "Iteration 2173: Loss = 12718317872204.203\n",
      "Iteration 2174: Loss = 12718189207204.258\n",
      "Iteration 2175: Loss = 12718060579112.055\n",
      "Iteration 2176: Loss = 12717931987916.82\n",
      "Iteration 2177: Loss = 12717803433607.783\n",
      "Iteration 2178: Loss = 12717674916174.18\n",
      "Iteration 2179: Loss = 12717546435605.25\n",
      "Iteration 2180: Loss = 12717417991890.227\n",
      "Iteration 2181: Loss = 12717289585018.363\n",
      "Iteration 2182: Loss = 12717161214978.898\n",
      "Iteration 2183: Loss = 12717032881761.088\n",
      "Iteration 2184: Loss = 12716904585354.184\n",
      "Iteration 2185: Loss = 12716776325747.443\n",
      "Iteration 2186: Loss = 12716648102930.125\n",
      "Iteration 2187: Loss = 12716519916891.49\n",
      "Iteration 2188: Loss = 12716391767620.807\n",
      "Iteration 2189: Loss = 12716263655107.346\n",
      "Iteration 2190: Loss = 12716135579340.377\n",
      "Iteration 2191: Loss = 12716007540309.174\n",
      "Iteration 2192: Loss = 12715879538003.02\n",
      "Iteration 2193: Loss = 12715751572411.197\n",
      "Iteration 2194: Loss = 12715623643522.984\n",
      "Iteration 2195: Loss = 12715495751327.674\n",
      "Iteration 2196: Loss = 12715367895814.56\n",
      "Iteration 2197: Loss = 12715240076972.932\n",
      "Iteration 2198: Loss = 12715112294792.09\n",
      "Iteration 2199: Loss = 12714984549261.334\n",
      "Iteration 2200: Loss = 12714856840369.97\n",
      "Iteration 2201: Loss = 12714729168107.303\n",
      "Iteration 2202: Loss = 12714601532462.646\n",
      "Iteration 2203: Loss = 12714473933425.305\n",
      "Iteration 2204: Loss = 12714346370984.605\n",
      "Iteration 2205: Loss = 12714218845129.863\n",
      "Iteration 2206: Loss = 12714091355850.4\n",
      "Iteration 2207: Loss = 12713963903135.545\n",
      "Iteration 2208: Loss = 12713836486974.623\n",
      "Iteration 2209: Loss = 12713709107356.97\n",
      "Iteration 2210: Loss = 12713581764271.924\n",
      "Iteration 2211: Loss = 12713454457708.814\n",
      "Iteration 2212: Loss = 12713327187656.99\n",
      "Iteration 2213: Loss = 12713199954105.797\n",
      "Iteration 2214: Loss = 12713072757044.58\n",
      "Iteration 2215: Loss = 12712945596462.688\n",
      "Iteration 2216: Loss = 12712818472349.482\n",
      "Iteration 2217: Loss = 12712691384694.312\n",
      "Iteration 2218: Loss = 12712564333486.543\n",
      "Iteration 2219: Loss = 12712437318715.54\n",
      "Iteration 2220: Loss = 12712310340370.668\n",
      "Iteration 2221: Loss = 12712183398441.291\n",
      "Iteration 2222: Loss = 12712056492916.793\n",
      "Iteration 2223: Loss = 12711929623786.543\n",
      "Iteration 2224: Loss = 12711802791039.924\n",
      "Iteration 2225: Loss = 12711675994666.314\n",
      "Iteration 2226: Loss = 12711549234655.105\n",
      "Iteration 2227: Loss = 12711422510995.68\n",
      "Iteration 2228: Loss = 12711295823677.436\n",
      "Iteration 2229: Loss = 12711169172689.76\n",
      "Iteration 2230: Loss = 12711042558022.06\n",
      "Iteration 2231: Loss = 12710915979663.732\n",
      "Iteration 2232: Loss = 12710789437604.18\n",
      "Iteration 2233: Loss = 12710662931832.812\n",
      "Iteration 2234: Loss = 12710536462339.045\n",
      "Iteration 2235: Loss = 12710410029112.28\n",
      "Iteration 2236: Loss = 12710283632141.945\n",
      "Iteration 2237: Loss = 12710157271417.455\n",
      "Iteration 2238: Loss = 12710030946928.234\n",
      "Iteration 2239: Loss = 12709904658663.707\n",
      "Iteration 2240: Loss = 12709778406613.31\n",
      "Iteration 2241: Loss = 12709652190766.465\n",
      "Iteration 2242: Loss = 12709526011112.613\n",
      "Iteration 2243: Loss = 12709399867641.197\n",
      "Iteration 2244: Loss = 12709273760341.648\n",
      "Iteration 2245: Loss = 12709147689203.422\n",
      "Iteration 2246: Loss = 12709021654215.959\n",
      "Iteration 2247: Loss = 12708895655368.719\n",
      "Iteration 2248: Loss = 12708769692651.145\n",
      "Iteration 2249: Loss = 12708643766052.705\n",
      "Iteration 2250: Loss = 12708517875562.852\n",
      "Iteration 2251: Loss = 12708392021171.053\n",
      "Iteration 2252: Loss = 12708266202866.773\n",
      "Iteration 2253: Loss = 12708140420639.482\n",
      "Iteration 2254: Loss = 12708014674478.656\n",
      "Iteration 2255: Loss = 12707888964373.77\n",
      "Iteration 2256: Loss = 12707763290314.299\n",
      "Iteration 2257: Loss = 12707637652289.73\n",
      "Iteration 2258: Loss = 12707512050289.545\n",
      "Iteration 2259: Loss = 12707386484303.232\n",
      "Iteration 2260: Loss = 12707260954320.29\n",
      "Iteration 2261: Loss = 12707135460330.201\n",
      "Iteration 2262: Loss = 12707010002322.473\n",
      "Iteration 2263: Loss = 12706884580286.604\n",
      "Iteration 2264: Loss = 12706759194212.096\n",
      "Iteration 2265: Loss = 12706633844088.459\n",
      "Iteration 2266: Loss = 12706508529905.2\n",
      "Iteration 2267: Loss = 12706383251651.834\n",
      "Iteration 2268: Loss = 12706258009317.877\n",
      "Iteration 2269: Loss = 12706132802892.846\n",
      "Iteration 2270: Loss = 12706007632366.268\n",
      "Iteration 2271: Loss = 12705882497727.666\n",
      "Iteration 2272: Loss = 12705757398966.568\n",
      "Iteration 2273: Loss = 12705632336072.508\n",
      "Iteration 2274: Loss = 12705507309035.02\n",
      "Iteration 2275: Loss = 12705382317843.637\n",
      "Iteration 2276: Loss = 12705257362487.91\n",
      "Iteration 2277: Loss = 12705132442957.373\n",
      "Iteration 2278: Loss = 12705007559241.58\n",
      "Iteration 2279: Loss = 12704882711330.076\n",
      "Iteration 2280: Loss = 12704757899212.418\n",
      "Iteration 2281: Loss = 12704633122878.162\n",
      "Iteration 2282: Loss = 12704508382316.867\n",
      "Iteration 2283: Loss = 12704383677518.096\n",
      "Iteration 2284: Loss = 12704259008471.408\n",
      "Iteration 2285: Loss = 12704134375166.383\n",
      "Iteration 2286: Loss = 12704009777592.588\n",
      "Iteration 2287: Loss = 12703885215739.594\n",
      "Iteration 2288: Loss = 12703760689596.984\n",
      "Iteration 2289: Loss = 12703636199154.336\n",
      "Iteration 2290: Loss = 12703511744401.236\n",
      "Iteration 2291: Loss = 12703387325327.27\n",
      "Iteration 2292: Loss = 12703262941922.025\n",
      "Iteration 2293: Loss = 12703138594175.102\n",
      "Iteration 2294: Loss = 12703014282076.092\n",
      "Iteration 2295: Loss = 12702890005614.594\n",
      "Iteration 2296: Loss = 12702765764780.213\n",
      "Iteration 2297: Loss = 12702641559562.55\n",
      "Iteration 2298: Loss = 12702517389951.223\n",
      "Iteration 2299: Loss = 12702393255935.834\n",
      "Iteration 2300: Loss = 12702269157505.998\n",
      "Iteration 2301: Loss = 12702145094651.34\n",
      "Iteration 2302: Loss = 12702021067361.477\n",
      "Iteration 2303: Loss = 12701897075626.033\n",
      "Iteration 2304: Loss = 12701773119434.633\n",
      "Iteration 2305: Loss = 12701649198776.906\n",
      "Iteration 2306: Loss = 12701525313642.492\n",
      "Iteration 2307: Loss = 12701401464021.021\n",
      "Iteration 2308: Loss = 12701277649902.135\n",
      "Iteration 2309: Loss = 12701153871275.475\n",
      "Iteration 2310: Loss = 12701030128130.686\n",
      "Iteration 2311: Loss = 12700906420457.416\n",
      "Iteration 2312: Loss = 12700782748245.318\n",
      "Iteration 2313: Loss = 12700659111484.045\n",
      "Iteration 2314: Loss = 12700535510163.258\n",
      "Iteration 2315: Loss = 12700411944272.611\n",
      "Iteration 2316: Loss = 12700288413801.775\n",
      "Iteration 2317: Loss = 12700164918740.41\n",
      "Iteration 2318: Loss = 12700041459078.191\n",
      "Iteration 2319: Loss = 12699918034804.783\n",
      "Iteration 2320: Loss = 12699794645909.875\n",
      "Iteration 2321: Loss = 12699671292383.13\n",
      "Iteration 2322: Loss = 12699547974214.242\n",
      "Iteration 2323: Loss = 12699424691392.893\n",
      "Iteration 2324: Loss = 12699301443908.768\n",
      "Iteration 2325: Loss = 12699178231751.555\n",
      "Iteration 2326: Loss = 12699055054910.955\n",
      "Iteration 2327: Loss = 12698931913376.664\n",
      "Iteration 2328: Loss = 12698808807138.38\n",
      "Iteration 2329: Loss = 12698685736185.805\n",
      "Iteration 2330: Loss = 12698562700508.65\n",
      "Iteration 2331: Loss = 12698439700096.62\n",
      "Iteration 2332: Loss = 12698316734939.426\n",
      "Iteration 2333: Loss = 12698193805026.787\n",
      "Iteration 2334: Loss = 12698070910348.42\n",
      "Iteration 2335: Loss = 12697948050894.043\n",
      "Iteration 2336: Loss = 12697825226653.387\n",
      "Iteration 2337: Loss = 12697702437616.172\n",
      "Iteration 2338: Loss = 12697579683772.137\n",
      "Iteration 2339: Loss = 12697456965111.008\n",
      "Iteration 2340: Loss = 12697334281622.525\n",
      "Iteration 2341: Loss = 12697211633296.426\n",
      "Iteration 2342: Loss = 12697089020122.451\n",
      "Iteration 2343: Loss = 12696966442090.352\n",
      "Iteration 2344: Loss = 12696843899189.871\n",
      "Iteration 2345: Loss = 12696721391410.768\n",
      "Iteration 2346: Loss = 12696598918742.787\n",
      "Iteration 2347: Loss = 12696476481175.691\n",
      "Iteration 2348: Loss = 12696354078699.244\n",
      "Iteration 2349: Loss = 12696231711303.203\n",
      "Iteration 2350: Loss = 12696109378977.34\n",
      "Iteration 2351: Loss = 12695987081711.42\n",
      "Iteration 2352: Loss = 12695864819495.219\n",
      "Iteration 2353: Loss = 12695742592318.51\n",
      "Iteration 2354: Loss = 12695620400171.076\n",
      "Iteration 2355: Loss = 12695498243042.695\n",
      "Iteration 2356: Loss = 12695376120923.156\n",
      "Iteration 2357: Loss = 12695254033802.238\n",
      "Iteration 2358: Loss = 12695131981669.742\n",
      "Iteration 2359: Loss = 12695009964515.453\n",
      "Iteration 2360: Loss = 12694887982329.176\n",
      "Iteration 2361: Loss = 12694766035100.705\n",
      "Iteration 2362: Loss = 12694644122819.846\n",
      "Iteration 2363: Loss = 12694522245476.402\n",
      "Iteration 2364: Loss = 12694400403060.184\n",
      "Iteration 2365: Loss = 12694278595561.002\n",
      "Iteration 2366: Loss = 12694156822968.672\n",
      "Iteration 2367: Loss = 12694035085273.014\n",
      "Iteration 2368: Loss = 12693913382463.838\n",
      "Iteration 2369: Loss = 12693791714530.984\n",
      "Iteration 2370: Loss = 12693670081464.271\n",
      "Iteration 2371: Loss = 12693548483253.53\n",
      "Iteration 2372: Loss = 12693426919888.588\n",
      "Iteration 2373: Loss = 12693305391359.287\n",
      "Iteration 2374: Loss = 12693183897655.463\n",
      "Iteration 2375: Loss = 12693062438766.96\n",
      "Iteration 2376: Loss = 12692941014683.621\n",
      "Iteration 2377: Loss = 12692819625395.297\n",
      "Iteration 2378: Loss = 12692698270891.834\n",
      "Iteration 2379: Loss = 12692576951163.09\n",
      "Iteration 2380: Loss = 12692455666198.922\n",
      "Iteration 2381: Loss = 12692334415989.182\n",
      "Iteration 2382: Loss = 12692213200523.74\n",
      "Iteration 2383: Loss = 12692092019792.463\n",
      "Iteration 2384: Loss = 12691970873785.21\n",
      "Iteration 2385: Loss = 12691849762491.865\n",
      "Iteration 2386: Loss = 12691728685902.297\n",
      "Iteration 2387: Loss = 12691607644006.383\n",
      "Iteration 2388: Loss = 12691486636794.008\n",
      "Iteration 2389: Loss = 12691365664255.045\n",
      "Iteration 2390: Loss = 12691244726379.39\n",
      "Iteration 2391: Loss = 12691123823156.934\n",
      "Iteration 2392: Loss = 12691002954577.56\n",
      "Iteration 2393: Loss = 12690882120631.172\n",
      "Iteration 2394: Loss = 12690761321307.666\n",
      "Iteration 2395: Loss = 12690640556596.943\n",
      "Iteration 2396: Loss = 12690519826488.91\n",
      "Iteration 2397: Loss = 12690399130973.47\n",
      "Iteration 2398: Loss = 12690278470040.535\n",
      "Iteration 2399: Loss = 12690157843680.021\n",
      "Iteration 2400: Loss = 12690037251881.842\n",
      "Iteration 2401: Loss = 12689916694635.918\n",
      "Iteration 2402: Loss = 12689796171932.172\n",
      "Iteration 2403: Loss = 12689675683760.527\n",
      "Iteration 2404: Loss = 12689555230110.91\n",
      "Iteration 2405: Loss = 12689434810973.258\n",
      "Iteration 2406: Loss = 12689314426337.502\n",
      "Iteration 2407: Loss = 12689194076193.578\n",
      "Iteration 2408: Loss = 12689073760531.428\n",
      "Iteration 2409: Loss = 12688953479340.994\n",
      "Iteration 2410: Loss = 12688833232612.225\n",
      "Iteration 2411: Loss = 12688713020335.064\n",
      "Iteration 2412: Loss = 12688592842499.469\n",
      "Iteration 2413: Loss = 12688472699095.39\n",
      "Iteration 2414: Loss = 12688352590112.79\n",
      "Iteration 2415: Loss = 12688232515541.623\n",
      "Iteration 2416: Loss = 12688112475371.861\n",
      "Iteration 2417: Loss = 12687992469593.465\n",
      "Iteration 2418: Loss = 12687872498196.408\n",
      "Iteration 2419: Loss = 12687752561170.656\n",
      "Iteration 2420: Loss = 12687632658506.195\n",
      "Iteration 2421: Loss = 12687512790192.996\n",
      "Iteration 2422: Loss = 12687392956221.045\n",
      "Iteration 2423: Loss = 12687273156580.32\n",
      "Iteration 2424: Loss = 12687153391260.818\n",
      "Iteration 2425: Loss = 12687033660252.521\n",
      "Iteration 2426: Loss = 12686913963545.428\n",
      "Iteration 2427: Loss = 12686794301129.531\n",
      "Iteration 2428: Loss = 12686674672994.83\n",
      "Iteration 2429: Loss = 12686555079131.332\n",
      "Iteration 2430: Loss = 12686435519529.035\n",
      "Iteration 2431: Loss = 12686315994177.951\n",
      "Iteration 2432: Loss = 12686196503068.092\n",
      "Iteration 2433: Loss = 12686077046189.47\n",
      "Iteration 2434: Loss = 12685957623532.104\n",
      "Iteration 2435: Loss = 12685838235086.01\n",
      "Iteration 2436: Loss = 12685718880841.21\n",
      "Iteration 2437: Loss = 12685599560787.74\n",
      "Iteration 2438: Loss = 12685480274915.62\n",
      "Iteration 2439: Loss = 12685361023214.877\n",
      "Iteration 2440: Loss = 12685241805675.557\n",
      "Iteration 2441: Loss = 12685122622287.69\n",
      "Iteration 2442: Loss = 12685003473041.32\n",
      "Iteration 2443: Loss = 12684884357926.488\n",
      "Iteration 2444: Loss = 12684765276933.242\n",
      "Iteration 2445: Loss = 12684646230051.629\n",
      "Iteration 2446: Loss = 12684527217271.705\n",
      "Iteration 2447: Loss = 12684408238583.521\n",
      "Iteration 2448: Loss = 12684289293977.137\n",
      "Iteration 2449: Loss = 12684170383442.613\n",
      "Iteration 2450: Loss = 12684051506970.016\n",
      "Iteration 2451: Loss = 12683932664549.408\n",
      "Iteration 2452: Loss = 12683813856170.863\n",
      "Iteration 2453: Loss = 12683695081824.45\n",
      "Iteration 2454: Loss = 12683576341500.246\n",
      "Iteration 2455: Loss = 12683457635188.332\n",
      "Iteration 2456: Loss = 12683338962878.787\n",
      "Iteration 2457: Loss = 12683220324561.693\n",
      "Iteration 2458: Loss = 12683101720227.145\n",
      "Iteration 2459: Loss = 12682983149865.223\n",
      "Iteration 2460: Loss = 12682864613466.027\n",
      "Iteration 2461: Loss = 12682746111019.652\n",
      "Iteration 2462: Loss = 12682627642516.195\n",
      "Iteration 2463: Loss = 12682509207945.762\n",
      "Iteration 2464: Loss = 12682390807298.453\n",
      "Iteration 2465: Loss = 12682272440564.377\n",
      "Iteration 2466: Loss = 12682154107733.646\n",
      "Iteration 2467: Loss = 12682035808796.373\n",
      "Iteration 2468: Loss = 12681917543742.672\n",
      "Iteration 2469: Loss = 12681799312562.664\n",
      "Iteration 2470: Loss = 12681681115246.477\n",
      "Iteration 2471: Loss = 12681562951784.227\n",
      "Iteration 2472: Loss = 12681444822166.049\n",
      "Iteration 2473: Loss = 12681326726382.07\n",
      "Iteration 2474: Loss = 12681208664422.424\n",
      "Iteration 2475: Loss = 12681090636277.252\n",
      "Iteration 2476: Loss = 12680972641936.686\n",
      "Iteration 2477: Loss = 12680854681390.875\n",
      "Iteration 2478: Loss = 12680736754629.963\n",
      "Iteration 2479: Loss = 12680618861644.1\n",
      "Iteration 2480: Loss = 12680501002423.436\n",
      "Iteration 2481: Loss = 12680383176958.121\n",
      "Iteration 2482: Loss = 12680265385238.318\n",
      "Iteration 2483: Loss = 12680147627254.186\n",
      "Iteration 2484: Loss = 12680029902995.883\n",
      "Iteration 2485: Loss = 12679912212453.586\n",
      "Iteration 2486: Loss = 12679794555617.45\n",
      "Iteration 2487: Loss = 12679676932477.654\n",
      "Iteration 2488: Loss = 12679559343024.375\n",
      "Iteration 2489: Loss = 12679441787247.783\n",
      "Iteration 2490: Loss = 12679324265138.066\n",
      "Iteration 2491: Loss = 12679206776685.402\n",
      "Iteration 2492: Loss = 12679089321879.98\n",
      "Iteration 2493: Loss = 12678971900711.984\n",
      "Iteration 2494: Loss = 12678854513171.615\n",
      "Iteration 2495: Loss = 12678737159249.06\n",
      "Iteration 2496: Loss = 12678619838934.52\n",
      "Iteration 2497: Loss = 12678502552218.193\n",
      "Iteration 2498: Loss = 12678385299090.287\n",
      "Iteration 2499: Loss = 12678268079541.0\n",
      "Iteration 2500: Loss = 12678150893560.55\n",
      "Iteration 2501: Loss = 12678033741139.145\n",
      "Iteration 2502: Loss = 12677916622267.006\n",
      "Iteration 2503: Loss = 12677799536934.34\n",
      "Iteration 2504: Loss = 12677682485131.375\n",
      "Iteration 2505: Loss = 12677565466848.33\n",
      "Iteration 2506: Loss = 12677448482075.44\n",
      "Iteration 2507: Loss = 12677331530802.926\n",
      "Iteration 2508: Loss = 12677214613021.021\n",
      "Iteration 2509: Loss = 12677097728719.967\n",
      "Iteration 2510: Loss = 12676980877889.996\n",
      "Iteration 2511: Loss = 12676864060521.35\n",
      "Iteration 2512: Loss = 12676747276604.271\n",
      "Iteration 2513: Loss = 12676630526129.014\n",
      "Iteration 2514: Loss = 12676513809085.816\n",
      "Iteration 2515: Loss = 12676397125464.936\n",
      "Iteration 2516: Loss = 12676280475256.63\n",
      "Iteration 2517: Loss = 12676163858451.158\n",
      "Iteration 2518: Loss = 12676047275038.773\n",
      "Iteration 2519: Loss = 12675930725009.746\n",
      "Iteration 2520: Loss = 12675814208354.344\n",
      "Iteration 2521: Loss = 12675697725062.834\n",
      "Iteration 2522: Loss = 12675581275125.486\n",
      "Iteration 2523: Loss = 12675464858532.58\n",
      "Iteration 2524: Loss = 12675348475274.393\n",
      "Iteration 2525: Loss = 12675232125341.203\n",
      "Iteration 2526: Loss = 12675115808723.299\n",
      "Iteration 2527: Loss = 12674999525410.967\n",
      "Iteration 2528: Loss = 12674883275394.492\n",
      "Iteration 2529: Loss = 12674767058664.168\n",
      "Iteration 2530: Loss = 12674650875210.297\n",
      "Iteration 2531: Loss = 12674534725023.17\n",
      "Iteration 2532: Loss = 12674418608093.086\n",
      "Iteration 2533: Loss = 12674302524410.355\n",
      "Iteration 2534: Loss = 12674186473965.287\n",
      "Iteration 2535: Loss = 12674070456748.184\n",
      "Iteration 2536: Loss = 12673954472749.36\n",
      "Iteration 2537: Loss = 12673838521959.135\n",
      "Iteration 2538: Loss = 12673722604367.816\n",
      "Iteration 2539: Loss = 12673606719965.74\n",
      "Iteration 2540: Loss = 12673490868743.22\n",
      "Iteration 2541: Loss = 12673375050690.588\n",
      "Iteration 2542: Loss = 12673259265798.166\n",
      "Iteration 2543: Loss = 12673143514056.297\n",
      "Iteration 2544: Loss = 12673027795455.31\n",
      "Iteration 2545: Loss = 12672912109985.543\n",
      "Iteration 2546: Loss = 12672796457637.34\n",
      "Iteration 2547: Loss = 12672680838401.045\n",
      "Iteration 2548: Loss = 12672565252266.998\n",
      "Iteration 2549: Loss = 12672449699225.559\n",
      "Iteration 2550: Loss = 12672334179267.07\n",
      "Iteration 2551: Loss = 12672218692381.895\n",
      "Iteration 2552: Loss = 12672103238560.387\n",
      "Iteration 2553: Loss = 12671987817792.908\n",
      "Iteration 2554: Loss = 12671872430069.824\n",
      "Iteration 2555: Loss = 12671757075381.498\n",
      "Iteration 2556: Loss = 12671641753718.305\n",
      "Iteration 2557: Loss = 12671526465070.61\n",
      "Iteration 2558: Loss = 12671411209428.793\n",
      "Iteration 2559: Loss = 12671295986783.229\n",
      "Iteration 2560: Loss = 12671180797124.305\n",
      "Iteration 2561: Loss = 12671065640442.396\n",
      "Iteration 2562: Loss = 12670950516727.89\n",
      "Iteration 2563: Loss = 12670835425971.186\n",
      "Iteration 2564: Loss = 12670720368162.664\n",
      "Iteration 2565: Loss = 12670605343292.727\n",
      "Iteration 2566: Loss = 12670490351351.766\n",
      "Iteration 2567: Loss = 12670375392330.19\n",
      "Iteration 2568: Loss = 12670260466218.393\n",
      "Iteration 2569: Loss = 12670145573006.791\n",
      "Iteration 2570: Loss = 12670030712685.785\n",
      "Iteration 2571: Loss = 12669915885245.791\n",
      "Iteration 2572: Loss = 12669801090677.223\n",
      "Iteration 2573: Loss = 12669686328970.5\n",
      "Iteration 2574: Loss = 12669571600116.04\n",
      "Iteration 2575: Loss = 12669456904104.264\n",
      "Iteration 2576: Loss = 12669342240925.607\n",
      "Iteration 2577: Loss = 12669227610570.488\n",
      "Iteration 2578: Loss = 12669113013029.344\n",
      "Iteration 2579: Loss = 12668998448292.61\n",
      "Iteration 2580: Loss = 12668883916350.719\n",
      "Iteration 2581: Loss = 12668769417194.113\n",
      "Iteration 2582: Loss = 12668654950813.24\n",
      "Iteration 2583: Loss = 12668540517198.54\n",
      "Iteration 2584: Loss = 12668426116340.46\n",
      "Iteration 2585: Loss = 12668311748229.459\n",
      "Iteration 2586: Loss = 12668197412855.984\n",
      "Iteration 2587: Loss = 12668083110210.494\n",
      "Iteration 2588: Loss = 12667968840283.451\n",
      "Iteration 2589: Loss = 12667854603065.318\n",
      "Iteration 2590: Loss = 12667740398546.555\n",
      "Iteration 2591: Loss = 12667626226717.637\n",
      "Iteration 2592: Loss = 12667512087569.03\n",
      "Iteration 2593: Loss = 12667397981091.21\n",
      "Iteration 2594: Loss = 12667283907274.652\n",
      "Iteration 2595: Loss = 12667169866109.842\n",
      "Iteration 2596: Loss = 12667055857587.256\n",
      "Iteration 2597: Loss = 12666941881697.379\n",
      "Iteration 2598: Loss = 12666827938430.701\n",
      "Iteration 2599: Loss = 12666714027777.713\n",
      "Iteration 2600: Loss = 12666600149728.904\n",
      "Iteration 2601: Loss = 12666486304274.777\n",
      "Iteration 2602: Loss = 12666372491405.83\n",
      "Iteration 2603: Loss = 12666258711112.559\n",
      "Iteration 2604: Loss = 12666144963385.475\n",
      "Iteration 2605: Loss = 12666031248215.082\n",
      "Iteration 2606: Loss = 12665917565591.895\n",
      "Iteration 2607: Loss = 12665803915506.422\n",
      "Iteration 2608: Loss = 12665690297949.178\n",
      "Iteration 2609: Loss = 12665576712910.686\n",
      "Iteration 2610: Loss = 12665463160381.467\n",
      "Iteration 2611: Loss = 12665349640352.041\n",
      "Iteration 2612: Loss = 12665236152812.94\n",
      "Iteration 2613: Loss = 12665122697754.69\n",
      "Iteration 2614: Loss = 12665009275167.826\n",
      "Iteration 2615: Loss = 12664895885042.887\n",
      "Iteration 2616: Loss = 12664782527370.404\n",
      "Iteration 2617: Loss = 12664669202140.922\n",
      "Iteration 2618: Loss = 12664555909344.98\n",
      "Iteration 2619: Loss = 12664442648973.135\n",
      "Iteration 2620: Loss = 12664329421015.926\n",
      "Iteration 2621: Loss = 12664216225463.908\n",
      "Iteration 2622: Loss = 12664103062307.64\n",
      "Iteration 2623: Loss = 12663989931537.678\n",
      "Iteration 2624: Loss = 12663876833144.578\n",
      "Iteration 2625: Loss = 12663763767118.908\n",
      "Iteration 2626: Loss = 12663650733451.229\n",
      "Iteration 2627: Loss = 12663537732132.117\n",
      "Iteration 2628: Loss = 12663424763152.14\n",
      "Iteration 2629: Loss = 12663311826501.871\n",
      "Iteration 2630: Loss = 12663198922171.893\n",
      "Iteration 2631: Loss = 12663086050152.777\n",
      "Iteration 2632: Loss = 12662973210435.111\n",
      "Iteration 2633: Loss = 12662860403009.482\n",
      "Iteration 2634: Loss = 12662747627866.473\n",
      "Iteration 2635: Loss = 12662634884996.682\n",
      "Iteration 2636: Loss = 12662522174390.695\n",
      "Iteration 2637: Loss = 12662409496039.115\n",
      "Iteration 2638: Loss = 12662296849932.537\n",
      "Iteration 2639: Loss = 12662184236061.566\n",
      "Iteration 2640: Loss = 12662071654416.805\n",
      "Iteration 2641: Loss = 12661959104988.863\n",
      "Iteration 2642: Loss = 12661846587768.348\n",
      "Iteration 2643: Loss = 12661734102745.88\n",
      "Iteration 2644: Loss = 12661621649912.066\n",
      "Iteration 2645: Loss = 12661509229257.533\n",
      "Iteration 2646: Loss = 12661396840772.895\n",
      "Iteration 2647: Loss = 12661284484448.777\n",
      "Iteration 2648: Loss = 12661172160275.812\n",
      "Iteration 2649: Loss = 12661059868244.627\n",
      "Iteration 2650: Loss = 12660947608345.854\n",
      "Iteration 2651: Loss = 12660835380570.127\n",
      "Iteration 2652: Loss = 12660723184908.084\n",
      "Iteration 2653: Loss = 12660611021350.37\n",
      "Iteration 2654: Loss = 12660498889887.621\n",
      "Iteration 2655: Loss = 12660386790510.492\n",
      "Iteration 2656: Loss = 12660274723209.625\n",
      "Iteration 2657: Loss = 12660162687975.676\n",
      "Iteration 2658: Loss = 12660050684799.299\n",
      "Iteration 2659: Loss = 12659938713671.15\n",
      "Iteration 2660: Loss = 12659826774581.893\n",
      "Iteration 2661: Loss = 12659714867522.182\n",
      "Iteration 2662: Loss = 12659602992482.69\n",
      "Iteration 2663: Loss = 12659491149454.088\n",
      "Iteration 2664: Loss = 12659379338427.04\n",
      "Iteration 2665: Loss = 12659267559392.219\n",
      "Iteration 2666: Loss = 12659155812340.309\n",
      "Iteration 2667: Loss = 12659044097261.984\n",
      "Iteration 2668: Loss = 12658932414147.926\n",
      "Iteration 2669: Loss = 12658820762988.822\n",
      "Iteration 2670: Loss = 12658709143775.361\n",
      "Iteration 2671: Loss = 12658597556498.227\n",
      "Iteration 2672: Loss = 12658486001148.121\n",
      "Iteration 2673: Loss = 12658374477715.736\n",
      "Iteration 2674: Loss = 12658262986191.77\n",
      "Iteration 2675: Loss = 12658151526566.918\n",
      "Iteration 2676: Loss = 12658040098831.895\n",
      "Iteration 2677: Loss = 12657928702977.398\n",
      "Iteration 2678: Loss = 12657817338994.145\n",
      "Iteration 2679: Loss = 12657706006872.842\n",
      "Iteration 2680: Loss = 12657594706604.209\n",
      "Iteration 2681: Loss = 12657483438178.957\n",
      "Iteration 2682: Loss = 12657372201587.812\n",
      "Iteration 2683: Loss = 12657260996821.498\n",
      "Iteration 2684: Loss = 12657149823870.734\n",
      "Iteration 2685: Loss = 12657038682726.258\n",
      "Iteration 2686: Loss = 12656927573378.791\n",
      "Iteration 2687: Loss = 12656816495819.076\n",
      "Iteration 2688: Loss = 12656705450037.846\n",
      "Iteration 2689: Loss = 12656594436025.84\n",
      "Iteration 2690: Loss = 12656483453773.803\n",
      "Iteration 2691: Loss = 12656372503272.473\n",
      "Iteration 2692: Loss = 12656261584512.607\n",
      "Iteration 2693: Loss = 12656150697484.951\n",
      "Iteration 2694: Loss = 12656039842180.258\n",
      "Iteration 2695: Loss = 12655929018589.283\n",
      "Iteration 2696: Loss = 12655818226702.787\n",
      "Iteration 2697: Loss = 12655707466511.531\n",
      "Iteration 2698: Loss = 12655596738006.275\n",
      "Iteration 2699: Loss = 12655486041177.793\n",
      "Iteration 2700: Loss = 12655375376016.85\n",
      "Iteration 2701: Loss = 12655264742514.219\n",
      "Iteration 2702: Loss = 12655154140660.672\n",
      "Iteration 2703: Loss = 12655043570446.99\n",
      "Iteration 2704: Loss = 12654933031863.955\n",
      "Iteration 2705: Loss = 12654822524902.348\n",
      "Iteration 2706: Loss = 12654712049552.955\n",
      "Iteration 2707: Loss = 12654601605806.56\n",
      "Iteration 2708: Loss = 12654491193653.963\n",
      "Iteration 2709: Loss = 12654380813085.953\n",
      "Iteration 2710: Loss = 12654270464093.322\n",
      "Iteration 2711: Loss = 12654160146666.88\n",
      "Iteration 2712: Loss = 12654049860797.422\n",
      "Iteration 2713: Loss = 12653939606475.754\n",
      "Iteration 2714: Loss = 12653829383692.684\n",
      "Iteration 2715: Loss = 12653719192439.02\n",
      "Iteration 2716: Loss = 12653609032705.58\n",
      "Iteration 2717: Loss = 12653498904483.174\n",
      "Iteration 2718: Loss = 12653388807762.625\n",
      "Iteration 2719: Loss = 12653278742534.748\n",
      "Iteration 2720: Loss = 12653168708790.377\n",
      "Iteration 2721: Loss = 12653058706520.326\n",
      "Iteration 2722: Loss = 12652948735715.434\n",
      "Iteration 2723: Loss = 12652838796366.525\n",
      "Iteration 2724: Loss = 12652728888464.44\n",
      "Iteration 2725: Loss = 12652619012000.016\n",
      "Iteration 2726: Loss = 12652509166964.084\n",
      "Iteration 2727: Loss = 12652399353347.5\n",
      "Iteration 2728: Loss = 12652289571141.098\n",
      "Iteration 2729: Loss = 12652179820335.732\n",
      "Iteration 2730: Loss = 12652070100922.252\n",
      "Iteration 2731: Loss = 12651960412891.512\n",
      "Iteration 2732: Loss = 12651850756234.363\n",
      "Iteration 2733: Loss = 12651741130941.67\n",
      "Iteration 2734: Loss = 12651631537004.291\n",
      "Iteration 2735: Loss = 12651521974413.094\n",
      "Iteration 2736: Loss = 12651412443158.94\n",
      "Iteration 2737: Loss = 12651302943232.705\n",
      "Iteration 2738: Loss = 12651193474625.256\n",
      "Iteration 2739: Loss = 12651084037327.469\n",
      "Iteration 2740: Loss = 12650974631330.223\n",
      "Iteration 2741: Loss = 12650865256624.4\n",
      "Iteration 2742: Loss = 12650755913200.879\n",
      "Iteration 2743: Loss = 12650646601050.547\n",
      "Iteration 2744: Loss = 12650537320164.293\n",
      "Iteration 2745: Loss = 12650428070533.01\n",
      "Iteration 2746: Loss = 12650318852147.59\n",
      "Iteration 2747: Loss = 12650209664998.924\n",
      "Iteration 2748: Loss = 12650100509077.924\n",
      "Iteration 2749: Loss = 12649991384375.477\n",
      "Iteration 2750: Loss = 12649882290882.498\n",
      "Iteration 2751: Loss = 12649773228589.889\n",
      "Iteration 2752: Loss = 12649664197488.562\n",
      "Iteration 2753: Loss = 12649555197569.43\n",
      "Iteration 2754: Loss = 12649446228823.406\n",
      "Iteration 2755: Loss = 12649337291241.408\n",
      "Iteration 2756: Loss = 12649228384814.36\n",
      "Iteration 2757: Loss = 12649119509533.182\n",
      "Iteration 2758: Loss = 12649010665388.8\n",
      "Iteration 2759: Loss = 12648901852372.143\n",
      "Iteration 2760: Loss = 12648793070474.14\n",
      "Iteration 2761: Loss = 12648684319685.73\n",
      "Iteration 2762: Loss = 12648575599997.848\n",
      "Iteration 2763: Loss = 12648466911401.428\n",
      "Iteration 2764: Loss = 12648358253887.418\n",
      "Iteration 2765: Loss = 12648249627446.76\n",
      "Iteration 2766: Loss = 12648141032070.404\n",
      "Iteration 2767: Loss = 12648032467749.295\n",
      "Iteration 2768: Loss = 12647923934474.387\n",
      "Iteration 2769: Loss = 12647815432236.639\n",
      "Iteration 2770: Loss = 12647706961027.004\n",
      "Iteration 2771: Loss = 12647598520836.443\n",
      "Iteration 2772: Loss = 12647490111655.924\n",
      "Iteration 2773: Loss = 12647381733476.408\n",
      "Iteration 2774: Loss = 12647273386288.87\n",
      "Iteration 2775: Loss = 12647165070084.27\n",
      "Iteration 2776: Loss = 12647056784853.592\n",
      "Iteration 2777: Loss = 12646948530587.809\n",
      "Iteration 2778: Loss = 12646840307277.896\n",
      "Iteration 2779: Loss = 12646732114914.844\n",
      "Iteration 2780: Loss = 12646623953489.627\n",
      "Iteration 2781: Loss = 12646515822993.242\n",
      "Iteration 2782: Loss = 12646407723416.676\n",
      "Iteration 2783: Loss = 12646299654750.914\n",
      "Iteration 2784: Loss = 12646191616986.96\n",
      "Iteration 2785: Loss = 12646083610115.809\n",
      "Iteration 2786: Loss = 12645975634128.459\n",
      "Iteration 2787: Loss = 12645867689015.916\n",
      "Iteration 2788: Loss = 12645759774769.186\n",
      "Iteration 2789: Loss = 12645651891379.273\n",
      "Iteration 2790: Loss = 12645544038837.193\n",
      "Iteration 2791: Loss = 12645436217133.957\n",
      "Iteration 2792: Loss = 12645328426260.582\n",
      "Iteration 2793: Loss = 12645220666208.086\n",
      "Iteration 2794: Loss = 12645112936967.494\n",
      "Iteration 2795: Loss = 12645005238529.826\n",
      "Iteration 2796: Loss = 12644897570886.11\n",
      "Iteration 2797: Loss = 12644789934027.377\n",
      "Iteration 2798: Loss = 12644682327944.656\n",
      "Iteration 2799: Loss = 12644574752628.986\n",
      "Iteration 2800: Loss = 12644467208071.402\n",
      "Iteration 2801: Loss = 12644359694262.945\n",
      "Iteration 2802: Loss = 12644252211194.656\n",
      "Iteration 2803: Loss = 12644144758857.584\n",
      "Iteration 2804: Loss = 12644037337242.773\n",
      "Iteration 2805: Loss = 12643929946341.275\n",
      "Iteration 2806: Loss = 12643822586144.143\n",
      "Iteration 2807: Loss = 12643715256642.436\n",
      "Iteration 2808: Loss = 12643607957827.207\n",
      "Iteration 2809: Loss = 12643500689689.52\n",
      "Iteration 2810: Loss = 12643393452220.44\n",
      "Iteration 2811: Loss = 12643286245411.031\n",
      "Iteration 2812: Loss = 12643179069252.363\n",
      "Iteration 2813: Loss = 12643071923735.508\n",
      "Iteration 2814: Loss = 12642964808851.541\n",
      "Iteration 2815: Loss = 12642857724591.537\n",
      "Iteration 2816: Loss = 12642750670946.58\n",
      "Iteration 2817: Loss = 12642643647907.746\n",
      "Iteration 2818: Loss = 12642536655466.123\n",
      "Iteration 2819: Loss = 12642429693612.799\n",
      "Iteration 2820: Loss = 12642322762338.863\n",
      "Iteration 2821: Loss = 12642215861635.408\n",
      "Iteration 2822: Loss = 12642108991493.53\n",
      "Iteration 2823: Loss = 12642002151904.322\n",
      "Iteration 2824: Loss = 12641895342858.895\n",
      "Iteration 2825: Loss = 12641788564348.342\n",
      "Iteration 2826: Loss = 12641681816363.773\n",
      "Iteration 2827: Loss = 12641575098896.299\n",
      "Iteration 2828: Loss = 12641468411937.025\n",
      "Iteration 2829: Loss = 12641361755477.072\n",
      "Iteration 2830: Loss = 12641255129507.547\n",
      "Iteration 2831: Loss = 12641148534019.576\n",
      "Iteration 2832: Loss = 12641041969004.28\n",
      "Iteration 2833: Loss = 12640935434452.781\n",
      "Iteration 2834: Loss = 12640828930356.203\n",
      "Iteration 2835: Loss = 12640722456705.684\n",
      "Iteration 2836: Loss = 12640616013492.348\n",
      "Iteration 2837: Loss = 12640509600707.332\n",
      "Iteration 2838: Loss = 12640403218341.775\n",
      "Iteration 2839: Loss = 12640296866386.814\n",
      "Iteration 2840: Loss = 12640190544833.592\n",
      "Iteration 2841: Loss = 12640084253673.256\n",
      "Iteration 2842: Loss = 12639977992896.951\n",
      "Iteration 2843: Loss = 12639871762495.83\n",
      "Iteration 2844: Loss = 12639765562461.043\n",
      "Iteration 2845: Loss = 12639659392783.746\n",
      "Iteration 2846: Loss = 12639553253455.102\n",
      "Iteration 2847: Loss = 12639447144466.264\n",
      "Iteration 2848: Loss = 12639341065808.396\n",
      "Iteration 2849: Loss = 12639235017472.67\n",
      "Iteration 2850: Loss = 12639128999450.254\n",
      "Iteration 2851: Loss = 12639023011732.309\n",
      "Iteration 2852: Loss = 12638917054310.018\n",
      "Iteration 2853: Loss = 12638811127174.557\n",
      "Iteration 2854: Loss = 12638705230317.102\n",
      "Iteration 2855: Loss = 12638599363728.834\n",
      "Iteration 2856: Loss = 12638493527400.94\n",
      "Iteration 2857: Loss = 12638387721324.604\n",
      "Iteration 2858: Loss = 12638281945491.016\n",
      "Iteration 2859: Loss = 12638176199891.367\n",
      "Iteration 2860: Loss = 12638070484516.854\n",
      "Iteration 2861: Loss = 12637964799358.67\n",
      "Iteration 2862: Loss = 12637859144408.018\n",
      "Iteration 2863: Loss = 12637753519656.1\n",
      "Iteration 2864: Loss = 12637647925094.12\n",
      "Iteration 2865: Loss = 12637542360713.285\n",
      "Iteration 2866: Loss = 12637436826504.803\n",
      "Iteration 2867: Loss = 12637331322459.89\n",
      "Iteration 2868: Loss = 12637225848569.762\n",
      "Iteration 2869: Loss = 12637120404825.633\n",
      "Iteration 2870: Loss = 12637014991218.723\n",
      "Iteration 2871: Loss = 12636909607740.26\n",
      "Iteration 2872: Loss = 12636804254381.465\n",
      "Iteration 2873: Loss = 12636698931133.566\n",
      "Iteration 2874: Loss = 12636593637987.799\n",
      "Iteration 2875: Loss = 12636488374935.39\n",
      "Iteration 2876: Loss = 12636383141967.578\n",
      "Iteration 2877: Loss = 12636277939075.604\n",
      "Iteration 2878: Loss = 12636172766250.705\n",
      "Iteration 2879: Loss = 12636067623484.125\n",
      "Iteration 2880: Loss = 12635962510767.113\n",
      "Iteration 2881: Loss = 12635857428090.916\n",
      "Iteration 2882: Loss = 12635752375446.781\n",
      "Iteration 2883: Loss = 12635647352825.97\n",
      "Iteration 2884: Loss = 12635542360219.732\n",
      "Iteration 2885: Loss = 12635437397619.334\n",
      "Iteration 2886: Loss = 12635332465016.031\n",
      "Iteration 2887: Loss = 12635227562401.09\n",
      "Iteration 2888: Loss = 12635122689765.775\n",
      "Iteration 2889: Loss = 12635017847101.355\n",
      "Iteration 2890: Loss = 12634913034399.11\n",
      "Iteration 2891: Loss = 12634808251650.305\n",
      "Iteration 2892: Loss = 12634703498846.22\n",
      "Iteration 2893: Loss = 12634598775978.137\n",
      "Iteration 2894: Loss = 12634494083037.336\n",
      "Iteration 2895: Loss = 12634389420015.1\n",
      "Iteration 2896: Loss = 12634284786902.719\n",
      "Iteration 2897: Loss = 12634180183691.482\n",
      "Iteration 2898: Loss = 12634075610372.684\n",
      "Iteration 2899: Loss = 12633971066937.615\n",
      "Iteration 2900: Loss = 12633866553377.574\n",
      "Iteration 2901: Loss = 12633762069683.863\n",
      "Iteration 2902: Loss = 12633657615847.785\n",
      "Iteration 2903: Loss = 12633553191860.643\n",
      "Iteration 2904: Loss = 12633448797713.744\n",
      "Iteration 2905: Loss = 12633344433398.404\n",
      "Iteration 2906: Loss = 12633240098905.93\n",
      "Iteration 2907: Loss = 12633135794227.637\n",
      "Iteration 2908: Loss = 12633031519354.848\n",
      "Iteration 2909: Loss = 12632927274278.88\n",
      "Iteration 2910: Loss = 12632823058991.059\n",
      "Iteration 2911: Loss = 12632718873482.707\n",
      "Iteration 2912: Loss = 12632614717745.152\n",
      "Iteration 2913: Loss = 12632510591769.732\n",
      "Iteration 2914: Loss = 12632406495547.77\n",
      "Iteration 2915: Loss = 12632302429070.613\n",
      "Iteration 2916: Loss = 12632198392329.586\n",
      "Iteration 2917: Loss = 12632094385316.045\n",
      "Iteration 2918: Loss = 12631990408021.32\n",
      "Iteration 2919: Loss = 12631886460436.766\n",
      "Iteration 2920: Loss = 12631782542553.727\n",
      "Iteration 2921: Loss = 12631678654363.557\n",
      "Iteration 2922: Loss = 12631574795857.611\n",
      "Iteration 2923: Loss = 12631470967027.238\n",
      "Iteration 2924: Loss = 12631367167863.805\n",
      "Iteration 2925: Loss = 12631263398358.668\n",
      "Iteration 2926: Loss = 12631159658503.195\n",
      "Iteration 2927: Loss = 12631055948288.75\n",
      "Iteration 2928: Loss = 12630952267706.701\n",
      "Iteration 2929: Loss = 12630848616748.426\n",
      "Iteration 2930: Loss = 12630744995405.29\n",
      "Iteration 2931: Loss = 12630641403668.672\n",
      "Iteration 2932: Loss = 12630537841529.955\n",
      "Iteration 2933: Loss = 12630434308980.518\n",
      "Iteration 2934: Loss = 12630330806011.744\n",
      "Iteration 2935: Loss = 12630227332615.023\n",
      "Iteration 2936: Loss = 12630123888781.746\n",
      "Iteration 2937: Loss = 12630020474503.299\n",
      "Iteration 2938: Loss = 12629917089771.078\n",
      "Iteration 2939: Loss = 12629813734576.482\n",
      "Iteration 2940: Loss = 12629710408910.91\n",
      "Iteration 2941: Loss = 12629607112765.764\n",
      "Iteration 2942: Loss = 12629503846132.445\n",
      "Iteration 2943: Loss = 12629400609002.365\n",
      "Iteration 2944: Loss = 12629297401366.93\n",
      "Iteration 2945: Loss = 12629194223217.557\n",
      "Iteration 2946: Loss = 12629091074545.652\n",
      "Iteration 2947: Loss = 12628987955342.64\n",
      "Iteration 2948: Loss = 12628884865599.938\n",
      "Iteration 2949: Loss = 12628781805308.967\n",
      "Iteration 2950: Loss = 12628678774461.154\n",
      "Iteration 2951: Loss = 12628575773047.926\n",
      "Iteration 2952: Loss = 12628472801060.709\n",
      "Iteration 2953: Loss = 12628369858490.94\n",
      "Iteration 2954: Loss = 12628266945330.05\n",
      "Iteration 2955: Loss = 12628164061569.482\n",
      "Iteration 2956: Loss = 12628061207200.672\n",
      "Iteration 2957: Loss = 12627958382215.059\n",
      "Iteration 2958: Loss = 12627855586604.094\n",
      "Iteration 2959: Loss = 12627752820359.22\n",
      "Iteration 2960: Loss = 12627650083471.89\n",
      "Iteration 2961: Loss = 12627547375933.557\n",
      "Iteration 2962: Loss = 12627444697735.672\n",
      "Iteration 2963: Loss = 12627342048869.695\n",
      "Iteration 2964: Loss = 12627239429327.084\n",
      "Iteration 2965: Loss = 12627136839099.309\n",
      "Iteration 2966: Loss = 12627034278177.824\n",
      "Iteration 2967: Loss = 12626931746554.104\n",
      "Iteration 2968: Loss = 12626829244219.617\n",
      "Iteration 2969: Loss = 12626726771165.834\n",
      "Iteration 2970: Loss = 12626624327384.23\n",
      "Iteration 2971: Loss = 12626521912866.287\n",
      "Iteration 2972: Loss = 12626419527603.482\n",
      "Iteration 2973: Loss = 12626317171587.299\n",
      "Iteration 2974: Loss = 12626214844809.219\n",
      "Iteration 2975: Loss = 12626112547260.734\n",
      "Iteration 2976: Loss = 12626010278933.332\n",
      "Iteration 2977: Loss = 12625908039818.506\n",
      "Iteration 2978: Loss = 12625805829907.754\n",
      "Iteration 2979: Loss = 12625703649192.568\n",
      "Iteration 2980: Loss = 12625601497664.455\n",
      "Iteration 2981: Loss = 12625499375314.91\n",
      "Iteration 2982: Loss = 12625397282135.441\n",
      "Iteration 2983: Loss = 12625295218117.56\n",
      "Iteration 2984: Loss = 12625193183252.773\n",
      "Iteration 2985: Loss = 12625091177532.592\n",
      "Iteration 2986: Loss = 12624989200948.533\n",
      "Iteration 2987: Loss = 12624887253492.115\n",
      "Iteration 2988: Loss = 12624785335154.861\n",
      "Iteration 2989: Loss = 12624683445928.285\n",
      "Iteration 2990: Loss = 12624581585803.918\n",
      "Iteration 2991: Loss = 12624479754773.287\n",
      "Iteration 2992: Loss = 12624377952827.924\n",
      "Iteration 2993: Loss = 12624276179959.355\n",
      "Iteration 2994: Loss = 12624174436159.123\n",
      "Iteration 2995: Loss = 12624072721418.764\n",
      "Iteration 2996: Loss = 12623971035729.812\n",
      "Iteration 2997: Loss = 12623869379083.812\n",
      "Iteration 2998: Loss = 12623767751472.316\n",
      "Iteration 2999: Loss = 12623666152886.865\n",
      "Iteration 3000: Loss = 12623564583319.01\n",
      "Iteration 3001: Loss = 12623463042760.303\n",
      "Iteration 3002: Loss = 12623361531202.3\n",
      "Iteration 3003: Loss = 12623260048636.559\n",
      "Iteration 3004: Loss = 12623158595054.64\n",
      "Iteration 3005: Loss = 12623057170448.105\n",
      "Iteration 3006: Loss = 12622955774808.518\n",
      "Iteration 3007: Loss = 12622854408127.45\n",
      "Iteration 3008: Loss = 12622753070396.463\n",
      "Iteration 3009: Loss = 12622651761607.137\n",
      "Iteration 3010: Loss = 12622550481751.045\n",
      "Iteration 3011: Loss = 12622449230819.766\n",
      "Iteration 3012: Loss = 12622348008804.875\n",
      "Iteration 3013: Loss = 12622246815697.959\n",
      "Iteration 3014: Loss = 12622145651490.598\n",
      "Iteration 3015: Loss = 12622044516174.385\n",
      "Iteration 3016: Loss = 12621943409740.904\n",
      "Iteration 3017: Loss = 12621842332181.752\n",
      "Iteration 3018: Loss = 12621741283488.523\n",
      "Iteration 3019: Loss = 12621640263652.81\n",
      "Iteration 3020: Loss = 12621539272666.219\n",
      "Iteration 3021: Loss = 12621438310520.348\n",
      "Iteration 3022: Loss = 12621337377206.8\n",
      "Iteration 3023: Loss = 12621236472717.186\n",
      "Iteration 3024: Loss = 12621135597043.113\n",
      "Iteration 3025: Loss = 12621034750176.197\n",
      "Iteration 3026: Loss = 12620933932108.045\n",
      "Iteration 3027: Loss = 12620833142830.283\n",
      "Iteration 3028: Loss = 12620732382334.523\n",
      "Iteration 3029: Loss = 12620631650612.389\n",
      "Iteration 3030: Loss = 12620530947655.508\n",
      "Iteration 3031: Loss = 12620430273455.504\n",
      "Iteration 3032: Loss = 12620329628004.01\n",
      "Iteration 3033: Loss = 12620229011292.65\n",
      "Iteration 3034: Loss = 12620128423313.068\n",
      "Iteration 3035: Loss = 12620027864056.893\n",
      "Iteration 3036: Loss = 12619927333515.766\n",
      "Iteration 3037: Loss = 12619826831681.332\n",
      "Iteration 3038: Loss = 12619726358545.229\n",
      "Iteration 3039: Loss = 12619625914099.111\n",
      "Iteration 3040: Loss = 12619525498334.621\n",
      "Iteration 3041: Loss = 12619425111243.41\n",
      "Iteration 3042: Loss = 12619324752817.139\n",
      "Iteration 3043: Loss = 12619224423047.46\n",
      "Iteration 3044: Loss = 12619124121926.03\n",
      "Iteration 3045: Loss = 12619023849444.51\n",
      "Iteration 3046: Loss = 12618923605594.564\n",
      "Iteration 3047: Loss = 12618823390367.863\n",
      "Iteration 3048: Loss = 12618723203756.072\n",
      "Iteration 3049: Loss = 12618623045750.863\n",
      "Iteration 3050: Loss = 12618522916343.906\n",
      "Iteration 3051: Loss = 12618422815526.88\n",
      "Iteration 3052: Loss = 12618322743291.465\n",
      "Iteration 3053: Loss = 12618222699629.34\n",
      "Iteration 3054: Loss = 12618122684532.188\n",
      "Iteration 3055: Loss = 12618022697991.691\n",
      "Iteration 3056: Loss = 12617922739999.549\n",
      "Iteration 3057: Loss = 12617822810547.44\n",
      "Iteration 3058: Loss = 12617722909627.064\n",
      "Iteration 3059: Loss = 12617623037230.113\n",
      "Iteration 3060: Loss = 12617523193348.287\n",
      "Iteration 3061: Loss = 12617423377973.287\n",
      "Iteration 3062: Loss = 12617323591096.816\n",
      "Iteration 3063: Loss = 12617223832710.576\n",
      "Iteration 3064: Loss = 12617124102806.28\n",
      "Iteration 3065: Loss = 12617024401375.633\n",
      "Iteration 3066: Loss = 12616924728410.35\n",
      "Iteration 3067: Loss = 12616825083902.146\n",
      "Iteration 3068: Loss = 12616725467842.74\n",
      "Iteration 3069: Loss = 12616625880223.848\n",
      "Iteration 3070: Loss = 12616526321037.197\n",
      "Iteration 3071: Loss = 12616426790274.508\n",
      "Iteration 3072: Loss = 12616327287927.512\n",
      "Iteration 3073: Loss = 12616227813987.934\n",
      "Iteration 3074: Loss = 12616128368447.51\n",
      "Iteration 3075: Loss = 12616028951297.977\n",
      "Iteration 3076: Loss = 12615929562531.062\n",
      "Iteration 3077: Loss = 12615830202138.518\n",
      "Iteration 3078: Loss = 12615730870112.074\n",
      "Iteration 3079: Loss = 12615631566443.482\n",
      "Iteration 3080: Loss = 12615532291124.486\n",
      "Iteration 3081: Loss = 12615433044146.836\n",
      "Iteration 3082: Loss = 12615333825502.287\n",
      "Iteration 3083: Loss = 12615234635182.586\n",
      "Iteration 3084: Loss = 12615135473179.492\n",
      "Iteration 3085: Loss = 12615036339484.768\n",
      "Iteration 3086: Loss = 12614937234090.168\n",
      "Iteration 3087: Loss = 12614838156987.465\n",
      "Iteration 3088: Loss = 12614739108168.414\n",
      "Iteration 3089: Loss = 12614640087624.793\n",
      "Iteration 3090: Loss = 12614541095348.371\n",
      "Iteration 3091: Loss = 12614442131330.918\n",
      "Iteration 3092: Loss = 12614343195564.21\n",
      "Iteration 3093: Loss = 12614244288040.031\n",
      "Iteration 3094: Loss = 12614145408750.156\n",
      "Iteration 3095: Loss = 12614046557686.371\n",
      "Iteration 3096: Loss = 12613947734840.459\n",
      "Iteration 3097: Loss = 12613848940204.209\n",
      "Iteration 3098: Loss = 12613750173769.41\n",
      "Iteration 3099: Loss = 12613651435527.861\n",
      "Iteration 3100: Loss = 12613552725471.35\n",
      "Iteration 3101: Loss = 12613454043591.678\n",
      "Iteration 3102: Loss = 12613355389880.643\n",
      "Iteration 3103: Loss = 12613256764330.047\n",
      "Iteration 3104: Loss = 12613158166931.7\n",
      "Iteration 3105: Loss = 12613059597677.402\n",
      "Iteration 3106: Loss = 12612961056558.969\n",
      "Iteration 3107: Loss = 12612862543568.209\n",
      "Iteration 3108: Loss = 12612764058696.94\n",
      "Iteration 3109: Loss = 12612665601936.973\n",
      "Iteration 3110: Loss = 12612567173280.135\n",
      "Iteration 3111: Loss = 12612468772718.238\n",
      "Iteration 3112: Loss = 12612370400243.115\n",
      "Iteration 3113: Loss = 12612272055846.592\n",
      "Iteration 3114: Loss = 12612173739520.492\n",
      "Iteration 3115: Loss = 12612075451256.652\n",
      "Iteration 3116: Loss = 12611977191046.9\n",
      "Iteration 3117: Loss = 12611878958883.078\n",
      "Iteration 3118: Loss = 12611780754757.02\n",
      "Iteration 3119: Loss = 12611682578660.57\n",
      "Iteration 3120: Loss = 12611584430585.57\n",
      "Iteration 3121: Loss = 12611486310523.867\n",
      "Iteration 3122: Loss = 12611388218467.314\n",
      "Iteration 3123: Loss = 12611290154407.744\n",
      "Iteration 3124: Loss = 12611192118337.03\n",
      "Iteration 3125: Loss = 12611094110247.016\n",
      "Iteration 3126: Loss = 12610996130129.562\n",
      "Iteration 3127: Loss = 12610898177976.53\n",
      "Iteration 3128: Loss = 12610800253779.781\n",
      "Iteration 3129: Loss = 12610702357531.182\n",
      "Iteration 3130: Loss = 12610604489222.594\n",
      "Iteration 3131: Loss = 12610506648845.895\n",
      "Iteration 3132: Loss = 12610408836392.955\n",
      "Iteration 3133: Loss = 12610311051855.643\n",
      "Iteration 3134: Loss = 12610213295225.838\n",
      "Iteration 3135: Loss = 12610115566495.426\n",
      "Iteration 3136: Loss = 12610017865656.281\n",
      "Iteration 3137: Loss = 12609920192700.291\n",
      "Iteration 3138: Loss = 12609822547619.344\n",
      "Iteration 3139: Loss = 12609724930405.32\n",
      "Iteration 3140: Loss = 12609627341050.12\n",
      "Iteration 3141: Loss = 12609529779545.633\n",
      "Iteration 3142: Loss = 12609432245883.758\n",
      "Iteration 3143: Loss = 12609334740056.385\n",
      "Iteration 3144: Loss = 12609237262055.428\n",
      "Iteration 3145: Loss = 12609139811872.777\n",
      "Iteration 3146: Loss = 12609042389500.348\n",
      "Iteration 3147: Loss = 12608944994930.043\n",
      "Iteration 3148: Loss = 12608847628153.773\n",
      "Iteration 3149: Loss = 12608750289163.455\n",
      "Iteration 3150: Loss = 12608652977950.996\n",
      "Iteration 3151: Loss = 12608555694508.32\n",
      "Iteration 3152: Loss = 12608458438827.346\n",
      "Iteration 3153: Loss = 12608361210899.992\n",
      "Iteration 3154: Loss = 12608264010718.19\n",
      "Iteration 3155: Loss = 12608166838273.86\n",
      "Iteration 3156: Loss = 12608069693558.932\n",
      "Iteration 3157: Loss = 12607972576565.34\n",
      "Iteration 3158: Loss = 12607875487285.018\n",
      "Iteration 3159: Loss = 12607778425709.904\n",
      "Iteration 3160: Loss = 12607681391831.934\n",
      "Iteration 3161: Loss = 12607584385643.049\n",
      "Iteration 3162: Loss = 12607487407135.193\n",
      "Iteration 3163: Loss = 12607390456300.312\n",
      "Iteration 3164: Loss = 12607293533130.357\n",
      "Iteration 3165: Loss = 12607196637617.273\n",
      "Iteration 3166: Loss = 12607099769753.02\n",
      "Iteration 3167: Loss = 12607002929529.549\n",
      "Iteration 3168: Loss = 12606906116938.818\n",
      "Iteration 3169: Loss = 12606809331972.79\n",
      "Iteration 3170: Loss = 12606712574623.42\n",
      "Iteration 3171: Loss = 12606615844882.682\n",
      "Iteration 3172: Loss = 12606519142742.537\n",
      "Iteration 3173: Loss = 12606422468194.957\n",
      "Iteration 3174: Loss = 12606325821231.92\n",
      "Iteration 3175: Loss = 12606229201845.387\n",
      "Iteration 3176: Loss = 12606132610027.348\n",
      "Iteration 3177: Loss = 12606036045769.77\n",
      "Iteration 3178: Loss = 12605939509064.643\n",
      "Iteration 3179: Loss = 12605842999903.95\n",
      "Iteration 3180: Loss = 12605746518279.674\n",
      "Iteration 3181: Loss = 12605650064183.8\n",
      "Iteration 3182: Loss = 12605553637608.328\n",
      "Iteration 3183: Loss = 12605457238545.244\n",
      "Iteration 3184: Loss = 12605360866986.55\n",
      "Iteration 3185: Loss = 12605264522924.24\n",
      "Iteration 3186: Loss = 12605168206350.312\n",
      "Iteration 3187: Loss = 12605071917256.771\n",
      "Iteration 3188: Loss = 12604975655635.623\n",
      "Iteration 3189: Loss = 12604879421478.875\n",
      "Iteration 3190: Loss = 12604783214778.537\n",
      "Iteration 3191: Loss = 12604687035526.62\n",
      "Iteration 3192: Loss = 12604590883715.137\n",
      "Iteration 3193: Loss = 12604494759336.105\n",
      "Iteration 3194: Loss = 12604398662381.547\n",
      "Iteration 3195: Loss = 12604302592843.48\n",
      "Iteration 3196: Loss = 12604206550713.926\n",
      "Iteration 3197: Loss = 12604110535984.922\n",
      "Iteration 3198: Loss = 12604014548648.486\n",
      "Iteration 3199: Loss = 12603918588696.65\n",
      "Iteration 3200: Loss = 12603822656121.453\n",
      "Iteration 3201: Loss = 12603726750914.922\n",
      "Iteration 3202: Loss = 12603630873069.102\n",
      "Iteration 3203: Loss = 12603535022576.027\n",
      "Iteration 3204: Loss = 12603439199427.746\n",
      "Iteration 3205: Loss = 12603343403616.303\n",
      "Iteration 3206: Loss = 12603247635133.74\n",
      "Iteration 3207: Loss = 12603151893972.107\n",
      "Iteration 3208: Loss = 12603056180123.467\n",
      "Iteration 3209: Loss = 12602960493579.86\n",
      "Iteration 3210: Loss = 12602864834333.348\n",
      "Iteration 3211: Loss = 12602769202375.992\n",
      "Iteration 3212: Loss = 12602673597699.852\n",
      "Iteration 3213: Loss = 12602578020296.988\n",
      "Iteration 3214: Loss = 12602482470159.475\n",
      "Iteration 3215: Loss = 12602386947279.37\n",
      "Iteration 3216: Loss = 12602291451648.75\n",
      "Iteration 3217: Loss = 12602195983259.688\n",
      "Iteration 3218: Loss = 12602100542104.258\n",
      "Iteration 3219: Loss = 12602005128174.535\n",
      "Iteration 3220: Loss = 12601909741462.605\n",
      "Iteration 3221: Loss = 12601814381960.547\n",
      "Iteration 3222: Loss = 12601719049660.443\n",
      "Iteration 3223: Loss = 12601623744554.38\n",
      "Iteration 3224: Loss = 12601528466634.457\n",
      "Iteration 3225: Loss = 12601433215892.752\n",
      "Iteration 3226: Loss = 12601337992321.365\n",
      "Iteration 3227: Loss = 12601242795912.395\n",
      "Iteration 3228: Loss = 12601147626657.938\n",
      "Iteration 3229: Loss = 12601052484550.092\n",
      "Iteration 3230: Loss = 12600957369580.963\n",
      "Iteration 3231: Loss = 12600862281742.656\n",
      "Iteration 3232: Loss = 12600767221027.281\n",
      "Iteration 3233: Loss = 12600672187426.945\n",
      "Iteration 3234: Loss = 12600577180933.762\n",
      "Iteration 3235: Loss = 12600482201539.846\n",
      "Iteration 3236: Loss = 12600387249237.314\n",
      "Iteration 3237: Loss = 12600292324018.287\n",
      "Iteration 3238: Loss = 12600197425874.889\n",
      "Iteration 3239: Loss = 12600102554799.234\n",
      "Iteration 3240: Loss = 12600007710783.46\n",
      "Iteration 3241: Loss = 12599912893819.688\n",
      "Iteration 3242: Loss = 12599818103900.057\n",
      "Iteration 3243: Loss = 12599723341016.69\n",
      "Iteration 3244: Loss = 12599628605161.73\n",
      "Iteration 3245: Loss = 12599533896327.314\n",
      "Iteration 3246: Loss = 12599439214505.582\n",
      "Iteration 3247: Loss = 12599344559688.672\n",
      "Iteration 3248: Loss = 12599249931868.736\n",
      "Iteration 3249: Loss = 12599155331037.916\n",
      "Iteration 3250: Loss = 12599060757188.365\n",
      "Iteration 3251: Loss = 12598966210312.232\n",
      "Iteration 3252: Loss = 12598871690401.674\n",
      "Iteration 3253: Loss = 12598777197448.844\n",
      "Iteration 3254: Loss = 12598682731445.904\n",
      "Iteration 3255: Loss = 12598588292385.016\n",
      "Iteration 3256: Loss = 12598493880258.338\n",
      "Iteration 3257: Loss = 12598399495058.041\n",
      "Iteration 3258: Loss = 12598305136776.287\n",
      "Iteration 3259: Loss = 12598210805405.256\n",
      "Iteration 3260: Loss = 12598116500937.107\n",
      "Iteration 3261: Loss = 12598022223364.03\n",
      "Iteration 3262: Loss = 12597927972678.191\n",
      "Iteration 3263: Loss = 12597833748871.773\n",
      "Iteration 3264: Loss = 12597739551936.96\n",
      "Iteration 3265: Loss = 12597645381865.934\n",
      "Iteration 3266: Loss = 12597551238650.88\n",
      "Iteration 3267: Loss = 12597457122283.988\n",
      "Iteration 3268: Loss = 12597363032757.453\n",
      "Iteration 3269: Loss = 12597268970063.46\n",
      "Iteration 3270: Loss = 12597174934194.21\n",
      "Iteration 3271: Loss = 12597080925141.902\n",
      "Iteration 3272: Loss = 12596986942898.734\n",
      "Iteration 3273: Loss = 12596892987456.91\n",
      "Iteration 3274: Loss = 12596799058808.633\n",
      "Iteration 3275: Loss = 12596705156946.11\n",
      "Iteration 3276: Loss = 12596611281861.55\n",
      "Iteration 3277: Loss = 12596517433547.166\n",
      "Iteration 3278: Loss = 12596423611995.172\n",
      "Iteration 3279: Loss = 12596329817197.783\n",
      "Iteration 3280: Loss = 12596236049147.22\n",
      "Iteration 3281: Loss = 12596142307835.703\n",
      "Iteration 3282: Loss = 12596048593255.45\n",
      "Iteration 3283: Loss = 12595954905398.693\n",
      "Iteration 3284: Loss = 12595861244257.656\n",
      "Iteration 3285: Loss = 12595767609824.576\n",
      "Iteration 3286: Loss = 12595674002091.676\n",
      "Iteration 3287: Loss = 12595580421051.193\n",
      "Iteration 3288: Loss = 12595486866695.365\n",
      "Iteration 3289: Loss = 12595393339016.434\n",
      "Iteration 3290: Loss = 12595299838006.635\n",
      "Iteration 3291: Loss = 12595206363658.217\n",
      "Iteration 3292: Loss = 12595112915963.422\n",
      "Iteration 3293: Loss = 12595019494914.5\n",
      "Iteration 3294: Loss = 12594926100503.705\n",
      "Iteration 3295: Loss = 12594832732723.285\n",
      "Iteration 3296: Loss = 12594739391565.498\n",
      "Iteration 3297: Loss = 12594646077022.598\n",
      "Iteration 3298: Loss = 12594552789086.848\n",
      "Iteration 3299: Loss = 12594459527750.51\n",
      "Iteration 3300: Loss = 12594366293005.842\n",
      "Iteration 3301: Loss = 12594273084845.12\n",
      "Iteration 3302: Loss = 12594179903260.605\n",
      "Iteration 3303: Loss = 12594086748244.572\n",
      "Iteration 3304: Loss = 12593993619789.295\n",
      "Iteration 3305: Loss = 12593900517887.045\n",
      "Iteration 3306: Loss = 12593807442530.104\n",
      "Iteration 3307: Loss = 12593714393710.752\n",
      "Iteration 3308: Loss = 12593621371421.27\n",
      "Iteration 3309: Loss = 12593528375653.941\n",
      "Iteration 3310: Loss = 12593435406401.055\n",
      "Iteration 3311: Loss = 12593342463654.898\n",
      "Iteration 3312: Loss = 12593249547407.766\n",
      "Iteration 3313: Loss = 12593156657651.951\n",
      "Iteration 3314: Loss = 12593063794379.744\n",
      "Iteration 3315: Loss = 12592970957583.45\n",
      "Iteration 3316: Loss = 12592878147255.37\n",
      "Iteration 3317: Loss = 12592785363387.799\n",
      "Iteration 3318: Loss = 12592692605973.047\n",
      "Iteration 3319: Loss = 12592599875003.42\n",
      "Iteration 3320: Loss = 12592507170471.232\n",
      "Iteration 3321: Loss = 12592414492368.79\n",
      "Iteration 3322: Loss = 12592321840688.408\n",
      "Iteration 3323: Loss = 12592229215422.408\n",
      "Iteration 3324: Loss = 12592136616563.102\n",
      "Iteration 3325: Loss = 12592044044102.814\n",
      "Iteration 3326: Loss = 12591951498033.863\n",
      "Iteration 3327: Loss = 12591858978348.584\n",
      "Iteration 3328: Loss = 12591766485039.293\n",
      "Iteration 3329: Loss = 12591674018098.33\n",
      "Iteration 3330: Loss = 12591581577518.021\n",
      "Iteration 3331: Loss = 12591489163290.701\n",
      "Iteration 3332: Loss = 12591396775408.713\n",
      "Iteration 3333: Loss = 12591304413864.38\n",
      "Iteration 3334: Loss = 12591212078650.062\n",
      "Iteration 3335: Loss = 12591119769758.096\n",
      "Iteration 3336: Loss = 12591027487180.822\n",
      "Iteration 3337: Loss = 12590935230910.592\n",
      "Iteration 3338: Loss = 12590843000939.76\n",
      "Iteration 3339: Loss = 12590750797260.672\n",
      "Iteration 3340: Loss = 12590658619865.686\n",
      "Iteration 3341: Loss = 12590566468747.158\n",
      "Iteration 3342: Loss = 12590474343897.447\n",
      "Iteration 3343: Loss = 12590382245308.918\n",
      "Iteration 3344: Loss = 12590290172973.926\n",
      "Iteration 3345: Loss = 12590198126884.848\n",
      "Iteration 3346: Loss = 12590106107034.043\n",
      "Iteration 3347: Loss = 12590014113413.889\n",
      "Iteration 3348: Loss = 12589922146016.752\n",
      "Iteration 3349: Loss = 12589830204835.012\n",
      "Iteration 3350: Loss = 12589738289861.043\n",
      "Iteration 3351: Loss = 12589646401087.227\n",
      "Iteration 3352: Loss = 12589554538505.941\n",
      "Iteration 3353: Loss = 12589462702109.574\n",
      "Iteration 3354: Loss = 12589370891890.51\n",
      "Iteration 3355: Loss = 12589279107841.137\n",
      "Iteration 3356: Loss = 12589187349953.846\n",
      "Iteration 3357: Loss = 12589095618221.03\n",
      "Iteration 3358: Loss = 12589003912635.082\n",
      "Iteration 3359: Loss = 12588912233188.406\n",
      "Iteration 3360: Loss = 12588820579873.393\n",
      "Iteration 3361: Loss = 12588728952682.451\n",
      "Iteration 3362: Loss = 12588637351607.98\n",
      "Iteration 3363: Loss = 12588545776642.389\n",
      "Iteration 3364: Loss = 12588454227778.084\n",
      "Iteration 3365: Loss = 12588362705007.479\n",
      "Iteration 3366: Loss = 12588271208322.984\n",
      "Iteration 3367: Loss = 12588179737717.016\n",
      "Iteration 3368: Loss = 12588088293181.992\n",
      "Iteration 3369: Loss = 12587996874710.332\n",
      "Iteration 3370: Loss = 12587905482294.455\n",
      "Iteration 3371: Loss = 12587814115926.79\n",
      "Iteration 3372: Loss = 12587722775599.758\n",
      "Iteration 3373: Loss = 12587631461305.793\n",
      "Iteration 3374: Loss = 12587540173037.32\n",
      "Iteration 3375: Loss = 12587448910786.777\n",
      "Iteration 3376: Loss = 12587357674546.6\n",
      "Iteration 3377: Loss = 12587266464309.22\n",
      "Iteration 3378: Loss = 12587175280067.084\n",
      "Iteration 3379: Loss = 12587084121812.627\n",
      "Iteration 3380: Loss = 12586992989538.299\n",
      "Iteration 3381: Loss = 12586901883236.543\n",
      "Iteration 3382: Loss = 12586810802899.809\n",
      "Iteration 3383: Loss = 12586719748520.549\n",
      "Iteration 3384: Loss = 12586628720091.213\n",
      "Iteration 3385: Loss = 12586537717604.258\n",
      "Iteration 3386: Loss = 12586446741052.14\n",
      "Iteration 3387: Loss = 12586355790427.32\n",
      "Iteration 3388: Loss = 12586264865722.264\n",
      "Iteration 3389: Loss = 12586173966929.428\n",
      "Iteration 3390: Loss = 12586083094041.281\n",
      "Iteration 3391: Loss = 12585992247050.297\n",
      "Iteration 3392: Loss = 12585901425948.94\n",
      "Iteration 3393: Loss = 12585810630729.686\n",
      "Iteration 3394: Loss = 12585719861385.01\n",
      "Iteration 3395: Loss = 12585629117907.387\n",
      "Iteration 3396: Loss = 12585538400289.305\n",
      "Iteration 3397: Loss = 12585447708523.232\n",
      "Iteration 3398: Loss = 12585357042601.666\n",
      "Iteration 3399: Loss = 12585266402517.084\n",
      "Iteration 3400: Loss = 12585175788261.979\n",
      "Iteration 3401: Loss = 12585085199828.838\n",
      "Iteration 3402: Loss = 12584994637210.158\n",
      "Iteration 3403: Loss = 12584904100398.434\n",
      "Iteration 3404: Loss = 12584813589386.158\n",
      "Iteration 3405: Loss = 12584723104165.834\n",
      "Iteration 3406: Loss = 12584632644729.963\n",
      "Iteration 3407: Loss = 12584542211071.05\n",
      "Iteration 3408: Loss = 12584451803181.598\n",
      "Iteration 3409: Loss = 12584361421054.117\n",
      "Iteration 3410: Loss = 12584271064681.12\n",
      "Iteration 3411: Loss = 12584180734055.115\n",
      "Iteration 3412: Loss = 12584090429168.621\n",
      "Iteration 3413: Loss = 12584000150014.154\n",
      "Iteration 3414: Loss = 12583909896584.23\n",
      "Iteration 3415: Loss = 12583819668871.377\n",
      "Iteration 3416: Loss = 12583729466868.113\n",
      "Iteration 3417: Loss = 12583639290566.967\n",
      "Iteration 3418: Loss = 12583549139960.467\n",
      "Iteration 3419: Loss = 12583459015041.14\n",
      "Iteration 3420: Loss = 12583368915801.523\n",
      "Iteration 3421: Loss = 12583278842234.146\n",
      "Iteration 3422: Loss = 12583188794331.553\n",
      "Iteration 3423: Loss = 12583098772086.277\n",
      "Iteration 3424: Loss = 12583008775490.863\n",
      "Iteration 3425: Loss = 12582918804537.852\n",
      "Iteration 3426: Loss = 12582828859219.791\n",
      "Iteration 3427: Loss = 12582738939529.227\n",
      "Iteration 3428: Loss = 12582649045458.71\n",
      "Iteration 3429: Loss = 12582559177000.793\n",
      "Iteration 3430: Loss = 12582469334148.03\n",
      "Iteration 3431: Loss = 12582379516892.98\n",
      "Iteration 3432: Loss = 12582289725228.197\n",
      "Iteration 3433: Loss = 12582199959146.246\n",
      "Iteration 3434: Loss = 12582110218639.688\n",
      "Iteration 3435: Loss = 12582020503701.09\n",
      "Iteration 3436: Loss = 12581930814323.02\n",
      "Iteration 3437: Loss = 12581841150498.047\n",
      "Iteration 3438: Loss = 12581751512218.742\n",
      "Iteration 3439: Loss = 12581661899477.682\n",
      "Iteration 3440: Loss = 12581572312267.44\n",
      "Iteration 3441: Loss = 12581482750580.596\n",
      "Iteration 3442: Loss = 12581393214409.729\n",
      "Iteration 3443: Loss = 12581303703747.426\n",
      "Iteration 3444: Loss = 12581214218586.268\n",
      "Iteration 3445: Loss = 12581124758918.848\n",
      "Iteration 3446: Loss = 12581035324737.746\n",
      "Iteration 3447: Loss = 12580945916035.56\n",
      "Iteration 3448: Loss = 12580856532804.885\n",
      "Iteration 3449: Loss = 12580767175038.312\n",
      "Iteration 3450: Loss = 12580677842728.443\n",
      "Iteration 3451: Loss = 12580588535867.88\n",
      "Iteration 3452: Loss = 12580499254449.22\n",
      "Iteration 3453: Loss = 12580409998465.072\n",
      "Iteration 3454: Loss = 12580320767908.041\n",
      "Iteration 3455: Loss = 12580231562770.734\n",
      "Iteration 3456: Loss = 12580142383045.768\n",
      "Iteration 3457: Loss = 12580053228725.748\n",
      "Iteration 3458: Loss = 12579964099803.295\n",
      "Iteration 3459: Loss = 12579874996271.031\n",
      "Iteration 3460: Loss = 12579785918121.566\n",
      "Iteration 3461: Loss = 12579696865347.531\n",
      "Iteration 3462: Loss = 12579607837941.545\n",
      "Iteration 3463: Loss = 12579518835896.236\n",
      "Iteration 3464: Loss = 12579429859204.23\n",
      "Iteration 3465: Loss = 12579340907858.166\n",
      "Iteration 3466: Loss = 12579251981850.664\n",
      "Iteration 3467: Loss = 12579163081174.37\n",
      "Iteration 3468: Loss = 12579074205821.916\n",
      "Iteration 3469: Loss = 12578985355785.943\n",
      "Iteration 3470: Loss = 12578896531059.092\n",
      "Iteration 3471: Loss = 12578807731634.008\n",
      "Iteration 3472: Loss = 12578718957503.336\n",
      "Iteration 3473: Loss = 12578630208659.725\n",
      "Iteration 3474: Loss = 12578541485095.818\n",
      "Iteration 3475: Loss = 12578452786804.281\n",
      "Iteration 3476: Loss = 12578364113777.758\n",
      "Iteration 3477: Loss = 12578275466008.908\n",
      "Iteration 3478: Loss = 12578186843490.393\n",
      "Iteration 3479: Loss = 12578098246214.87\n",
      "Iteration 3480: Loss = 12578009674175.006\n",
      "Iteration 3481: Loss = 12577921127363.459\n",
      "Iteration 3482: Loss = 12577832605772.906\n",
      "Iteration 3483: Loss = 12577744109396.012\n",
      "Iteration 3484: Loss = 12577655638225.45\n",
      "Iteration 3485: Loss = 12577567192253.89\n",
      "Iteration 3486: Loss = 12577478771474.016\n",
      "Iteration 3487: Loss = 12577390375878.498\n",
      "Iteration 3488: Loss = 12577302005460.02\n",
      "Iteration 3489: Loss = 12577213660211.266\n",
      "Iteration 3490: Loss = 12577125340124.92\n",
      "Iteration 3491: Loss = 12577037045193.666\n",
      "Iteration 3492: Loss = 12576948775410.195\n",
      "Iteration 3493: Loss = 12576860530767.201\n",
      "Iteration 3494: Loss = 12576772311257.377\n",
      "Iteration 3495: Loss = 12576684116873.41\n",
      "Iteration 3496: Loss = 12576595947608.006\n",
      "Iteration 3497: Loss = 12576507803453.863\n",
      "Iteration 3498: Loss = 12576419684403.684\n",
      "Iteration 3499: Loss = 12576331590450.172\n",
      "Iteration 3500: Loss = 12576243521586.03\n",
      "Iteration 3501: Loss = 12576155477803.97\n",
      "Iteration 3502: Loss = 12576067459096.703\n",
      "Iteration 3503: Loss = 12575979465456.94\n",
      "Iteration 3504: Loss = 12575891496877.396\n",
      "Iteration 3505: Loss = 12575803553350.79\n",
      "Iteration 3506: Loss = 12575715634869.836\n",
      "Iteration 3507: Loss = 12575627741427.258\n",
      "Iteration 3508: Loss = 12575539873015.783\n",
      "Iteration 3509: Loss = 12575452029628.133\n",
      "Iteration 3510: Loss = 12575364211257.035\n",
      "Iteration 3511: Loss = 12575276417895.22\n",
      "Iteration 3512: Loss = 12575188649535.418\n",
      "Iteration 3513: Loss = 12575100906170.367\n",
      "Iteration 3514: Loss = 12575013187792.8\n",
      "Iteration 3515: Loss = 12574925494395.455\n",
      "Iteration 3516: Loss = 12574837825971.076\n",
      "Iteration 3517: Loss = 12574750182512.4\n",
      "Iteration 3518: Loss = 12574662564012.18\n",
      "Iteration 3519: Loss = 12574574970463.152\n",
      "Iteration 3520: Loss = 12574487401858.076\n",
      "Iteration 3521: Loss = 12574399858189.695\n",
      "Iteration 3522: Loss = 12574312339450.766\n",
      "Iteration 3523: Loss = 12574224845634.045\n",
      "Iteration 3524: Loss = 12574137376732.287\n",
      "Iteration 3525: Loss = 12574049932738.254\n",
      "Iteration 3526: Loss = 12573962513644.705\n",
      "Iteration 3527: Loss = 12573875119444.402\n",
      "Iteration 3528: Loss = 12573787750130.121\n",
      "Iteration 3529: Loss = 12573700405694.621\n",
      "Iteration 3530: Loss = 12573613086130.676\n",
      "Iteration 3531: Loss = 12573525791431.055\n",
      "Iteration 3532: Loss = 12573438521588.537\n",
      "Iteration 3533: Loss = 12573351276595.896\n",
      "Iteration 3534: Loss = 12573264056445.912\n",
      "Iteration 3535: Loss = 12573176861131.365\n",
      "Iteration 3536: Loss = 12573089690645.04\n",
      "Iteration 3537: Loss = 12573002544979.717\n",
      "Iteration 3538: Loss = 12572915424128.191\n",
      "Iteration 3539: Loss = 12572828328083.244\n",
      "Iteration 3540: Loss = 12572741256837.672\n",
      "Iteration 3541: Loss = 12572654210384.27\n",
      "Iteration 3542: Loss = 12572567188715.828\n",
      "Iteration 3543: Loss = 12572480191825.145\n",
      "Iteration 3544: Loss = 12572393219705.027\n",
      "Iteration 3545: Loss = 12572306272348.271\n",
      "Iteration 3546: Loss = 12572219349747.682\n",
      "Iteration 3547: Loss = 12572132451896.07\n",
      "Iteration 3548: Loss = 12572045578786.234\n",
      "Iteration 3549: Loss = 12571958730410.994\n",
      "Iteration 3550: Loss = 12571871906763.162\n",
      "Iteration 3551: Loss = 12571785107835.549\n",
      "Iteration 3552: Loss = 12571698333620.97\n",
      "Iteration 3553: Loss = 12571611584112.252\n",
      "Iteration 3554: Loss = 12571524859302.209\n",
      "Iteration 3555: Loss = 12571438159183.666\n",
      "Iteration 3556: Loss = 12571351483749.45\n",
      "Iteration 3557: Loss = 12571264832992.389\n",
      "Iteration 3558: Loss = 12571178206905.31\n",
      "Iteration 3559: Loss = 12571091605481.045\n",
      "Iteration 3560: Loss = 12571005028712.426\n",
      "Iteration 3561: Loss = 12570918476592.297\n",
      "Iteration 3562: Loss = 12570831949113.49\n",
      "Iteration 3563: Loss = 12570745446268.84\n",
      "Iteration 3564: Loss = 12570658968051.2\n",
      "Iteration 3565: Loss = 12570572514453.408\n",
      "Iteration 3566: Loss = 12570486085468.314\n",
      "Iteration 3567: Loss = 12570399681088.76\n",
      "Iteration 3568: Loss = 12570313301307.602\n",
      "Iteration 3569: Loss = 12570226946117.693\n",
      "Iteration 3570: Loss = 12570140615511.887\n",
      "Iteration 3571: Loss = 12570054309483.04\n",
      "Iteration 3572: Loss = 12569968028024.008\n",
      "Iteration 3573: Loss = 12569881771127.658\n",
      "Iteration 3574: Loss = 12569795538786.852\n",
      "Iteration 3575: Loss = 12569709330994.453\n",
      "Iteration 3576: Loss = 12569623147743.33\n",
      "Iteration 3577: Loss = 12569536989026.352\n",
      "Iteration 3578: Loss = 12569450854836.39\n",
      "Iteration 3579: Loss = 12569364745166.318\n",
      "Iteration 3580: Loss = 12569278660009.016\n",
      "Iteration 3581: Loss = 12569192599357.354\n",
      "Iteration 3582: Loss = 12569106563204.217\n",
      "Iteration 3583: Loss = 12569020551542.488\n",
      "Iteration 3584: Loss = 12568934564365.049\n",
      "Iteration 3585: Loss = 12568848601664.79\n",
      "Iteration 3586: Loss = 12568762663434.592\n",
      "Iteration 3587: Loss = 12568676749667.35\n",
      "Iteration 3588: Loss = 12568590860355.959\n",
      "Iteration 3589: Loss = 12568504995493.307\n",
      "Iteration 3590: Loss = 12568419155072.297\n",
      "Iteration 3591: Loss = 12568333339085.828\n",
      "Iteration 3592: Loss = 12568247547526.795\n",
      "Iteration 3593: Loss = 12568161780388.107\n",
      "Iteration 3594: Loss = 12568076037662.664\n",
      "Iteration 3595: Loss = 12567990319343.379\n",
      "Iteration 3596: Loss = 12567904625423.154\n",
      "Iteration 3597: Loss = 12567818955894.906\n",
      "Iteration 3598: Loss = 12567733310751.549\n",
      "Iteration 3599: Loss = 12567647689985.994\n",
      "Iteration 3600: Loss = 12567562093591.162\n",
      "Iteration 3601: Loss = 12567476521559.973\n",
      "Iteration 3602: Loss = 12567390973885.348\n",
      "Iteration 3603: Loss = 12567305450560.205\n",
      "Iteration 3604: Loss = 12567219951577.48\n",
      "Iteration 3605: Loss = 12567134476930.096\n",
      "Iteration 3606: Loss = 12567049026610.982\n",
      "Iteration 3607: Loss = 12566963600613.072\n",
      "Iteration 3608: Loss = 12566878198929.299\n",
      "Iteration 3609: Loss = 12566792821552.6\n",
      "Iteration 3610: Loss = 12566707468475.916\n",
      "Iteration 3611: Loss = 12566622139692.182\n",
      "Iteration 3612: Loss = 12566536835194.346\n",
      "Iteration 3613: Loss = 12566451554975.348\n",
      "Iteration 3614: Loss = 12566366299028.137\n",
      "Iteration 3615: Loss = 12566281067345.662\n",
      "Iteration 3616: Loss = 12566195859920.877\n",
      "Iteration 3617: Loss = 12566110676746.727\n",
      "Iteration 3618: Loss = 12566025517816.17\n",
      "Iteration 3619: Loss = 12565940383122.168\n",
      "Iteration 3620: Loss = 12565855272657.678\n",
      "Iteration 3621: Loss = 12565770186415.656\n",
      "Iteration 3622: Loss = 12565685124389.072\n",
      "Iteration 3623: Loss = 12565600086570.887\n",
      "Iteration 3624: Loss = 12565515072954.07\n",
      "Iteration 3625: Loss = 12565430083531.592\n",
      "Iteration 3626: Loss = 12565345118296.42\n",
      "Iteration 3627: Loss = 12565260177241.531\n",
      "Iteration 3628: Loss = 12565175260359.902\n",
      "Iteration 3629: Loss = 12565090367644.508\n",
      "Iteration 3630: Loss = 12565005499088.33\n",
      "Iteration 3631: Loss = 12564920654684.352\n",
      "Iteration 3632: Loss = 12564835834425.55\n",
      "Iteration 3633: Loss = 12564751038304.922\n",
      "Iteration 3634: Loss = 12564666266315.441\n",
      "Iteration 3635: Loss = 12564581518450.113\n",
      "Iteration 3636: Loss = 12564496794701.924\n",
      "Iteration 3637: Loss = 12564412095063.863\n",
      "Iteration 3638: Loss = 12564327419528.93\n",
      "Iteration 3639: Loss = 12564242768090.127\n",
      "Iteration 3640: Loss = 12564158140740.451\n",
      "Iteration 3641: Loss = 12564073537472.902\n",
      "Iteration 3642: Loss = 12563988958280.49\n",
      "Iteration 3643: Loss = 12563904403156.217\n",
      "Iteration 3644: Loss = 12563819872093.094\n",
      "Iteration 3645: Loss = 12563735365084.133\n",
      "Iteration 3646: Loss = 12563650882122.344\n",
      "Iteration 3647: Loss = 12563566423200.742\n",
      "Iteration 3648: Loss = 12563481988312.346\n",
      "Iteration 3649: Loss = 12563397577450.172\n",
      "Iteration 3650: Loss = 12563313190607.244\n",
      "Iteration 3651: Loss = 12563228827776.586\n",
      "Iteration 3652: Loss = 12563144488951.219\n",
      "Iteration 3653: Loss = 12563060174124.168\n",
      "Iteration 3654: Loss = 12562975883288.47\n",
      "Iteration 3655: Loss = 12562891616437.152\n",
      "Iteration 3656: Loss = 12562807373563.248\n",
      "Iteration 3657: Loss = 12562723154659.791\n",
      "Iteration 3658: Loss = 12562638959719.82\n",
      "Iteration 3659: Loss = 12562554788736.379\n",
      "Iteration 3660: Loss = 12562470641702.5\n",
      "Iteration 3661: Loss = 12562386518611.232\n",
      "Iteration 3662: Loss = 12562302419455.623\n",
      "Iteration 3663: Loss = 12562218344228.717\n",
      "Iteration 3664: Loss = 12562134292923.564\n",
      "Iteration 3665: Loss = 12562050265533.219\n",
      "Iteration 3666: Loss = 12561966262050.729\n",
      "Iteration 3667: Loss = 12561882282469.154\n",
      "Iteration 3668: Loss = 12561798326781.553\n",
      "Iteration 3669: Loss = 12561714394980.984\n",
      "Iteration 3670: Loss = 12561630487060.508\n",
      "Iteration 3671: Loss = 12561546603013.191\n",
      "Iteration 3672: Loss = 12561462742832.1\n",
      "Iteration 3673: Loss = 12561378906510.299\n",
      "Iteration 3674: Loss = 12561295094040.861\n",
      "Iteration 3675: Loss = 12561211305416.857\n",
      "Iteration 3676: Loss = 12561127540631.361\n",
      "Iteration 3677: Loss = 12561043799677.45\n",
      "Iteration 3678: Loss = 12560960082548.201\n",
      "Iteration 3679: Loss = 12560876389236.697\n",
      "Iteration 3680: Loss = 12560792719736.016\n",
      "Iteration 3681: Loss = 12560709074039.246\n",
      "Iteration 3682: Loss = 12560625452139.47\n",
      "Iteration 3683: Loss = 12560541854029.783\n",
      "Iteration 3684: Loss = 12560458279703.266\n",
      "Iteration 3685: Loss = 12560374729153.018\n",
      "Iteration 3686: Loss = 12560291202372.133\n",
      "Iteration 3687: Loss = 12560207699353.705\n",
      "Iteration 3688: Loss = 12560124220090.832\n",
      "Iteration 3689: Loss = 12560040764576.617\n",
      "Iteration 3690: Loss = 12559957332804.162\n",
      "Iteration 3691: Loss = 12559873924766.57\n",
      "Iteration 3692: Loss = 12559790540456.951\n",
      "Iteration 3693: Loss = 12559707179868.41\n",
      "Iteration 3694: Loss = 12559623842994.062\n",
      "Iteration 3695: Loss = 12559540529827.016\n",
      "Iteration 3696: Loss = 12559457240360.385\n",
      "Iteration 3697: Loss = 12559373974587.29\n",
      "Iteration 3698: Loss = 12559290732500.85\n",
      "Iteration 3699: Loss = 12559207514094.184\n",
      "Iteration 3700: Loss = 12559124319360.41\n",
      "Iteration 3701: Loss = 12559041148292.664\n",
      "Iteration 3702: Loss = 12558958000884.064\n",
      "Iteration 3703: Loss = 12558874877127.742\n",
      "Iteration 3704: Loss = 12558791777016.83\n",
      "Iteration 3705: Loss = 12558708700544.455\n",
      "Iteration 3706: Loss = 12558625647703.76\n",
      "Iteration 3707: Loss = 12558542618487.877\n",
      "Iteration 3708: Loss = 12558459612889.947\n",
      "Iteration 3709: Loss = 12558376630903.11\n",
      "Iteration 3710: Loss = 12558293672520.506\n",
      "Iteration 3711: Loss = 12558210737735.287\n",
      "Iteration 3712: Loss = 12558127826540.598\n",
      "Iteration 3713: Loss = 12558044938929.584\n",
      "Iteration 3714: Loss = 12557962074895.398\n",
      "Iteration 3715: Loss = 12557879234431.193\n",
      "Iteration 3716: Loss = 12557796417530.127\n",
      "Iteration 3717: Loss = 12557713624185.354\n",
      "Iteration 3718: Loss = 12557630854390.033\n",
      "Iteration 3719: Loss = 12557548108137.326\n",
      "Iteration 3720: Loss = 12557465385420.396\n",
      "Iteration 3721: Loss = 12557382686232.408\n",
      "Iteration 3722: Loss = 12557300010566.531\n",
      "Iteration 3723: Loss = 12557217358415.934\n",
      "Iteration 3724: Loss = 12557134729773.783\n",
      "Iteration 3725: Loss = 12557052124633.258\n",
      "Iteration 3726: Loss = 12556969542987.533\n",
      "Iteration 3727: Loss = 12556886984829.78\n",
      "Iteration 3728: Loss = 12556804450153.184\n",
      "Iteration 3729: Loss = 12556721938950.924\n",
      "Iteration 3730: Loss = 12556639451216.182\n",
      "Iteration 3731: Loss = 12556556986942.143\n",
      "Iteration 3732: Loss = 12556474546121.996\n",
      "Iteration 3733: Loss = 12556392128748.932\n",
      "Iteration 3734: Loss = 12556309734816.139\n",
      "Iteration 3735: Loss = 12556227364316.81\n",
      "Iteration 3736: Loss = 12556145017244.143\n",
      "Iteration 3737: Loss = 12556062693591.332\n",
      "Iteration 3738: Loss = 12555980393351.578\n",
      "Iteration 3739: Loss = 12555898116518.084\n",
      "Iteration 3740: Loss = 12555815863084.05\n",
      "Iteration 3741: Loss = 12555733633042.682\n",
      "Iteration 3742: Loss = 12555651426387.193\n",
      "Iteration 3743: Loss = 12555569243110.781\n",
      "Iteration 3744: Loss = 12555487083206.668\n",
      "Iteration 3745: Loss = 12555404946668.057\n",
      "Iteration 3746: Loss = 12555322833488.172\n",
      "Iteration 3747: Loss = 12555240743660.227\n",
      "Iteration 3748: Loss = 12555158677177.441\n",
      "Iteration 3749: Loss = 12555076634033.035\n",
      "Iteration 3750: Loss = 12554994614220.23\n",
      "Iteration 3751: Loss = 12554912617732.258\n",
      "Iteration 3752: Loss = 12554830644562.342\n",
      "Iteration 3753: Loss = 12554748694703.707\n",
      "Iteration 3754: Loss = 12554666768149.59\n",
      "Iteration 3755: Loss = 12554584864893.223\n",
      "Iteration 3756: Loss = 12554502984927.838\n",
      "Iteration 3757: Loss = 12554421128246.676\n",
      "Iteration 3758: Loss = 12554339294842.977\n",
      "Iteration 3759: Loss = 12554257484709.975\n",
      "Iteration 3760: Loss = 12554175697840.92\n",
      "Iteration 3761: Loss = 12554093934229.053\n",
      "Iteration 3762: Loss = 12554012193867.625\n",
      "Iteration 3763: Loss = 12553930476749.883\n",
      "Iteration 3764: Loss = 12553848782869.078\n",
      "Iteration 3765: Loss = 12553767112218.463\n",
      "Iteration 3766: Loss = 12553685464791.291\n",
      "Iteration 3767: Loss = 12553603840580.822\n",
      "Iteration 3768: Loss = 12553522239580.316\n",
      "Iteration 3769: Loss = 12553440661783.027\n",
      "Iteration 3770: Loss = 12553359107182.227\n",
      "Iteration 3771: Loss = 12553277575771.172\n",
      "Iteration 3772: Loss = 12553196067543.137\n",
      "Iteration 3773: Loss = 12553114582491.387\n",
      "Iteration 3774: Loss = 12553033120609.191\n",
      "Iteration 3775: Loss = 12552951681889.826\n",
      "Iteration 3776: Loss = 12552870266326.562\n",
      "Iteration 3777: Loss = 12552788873912.68\n",
      "Iteration 3778: Loss = 12552707504641.459\n",
      "Iteration 3779: Loss = 12552626158506.176\n",
      "Iteration 3780: Loss = 12552544835500.113\n",
      "Iteration 3781: Loss = 12552463535616.56\n",
      "Iteration 3782: Loss = 12552382258848.8\n",
      "Iteration 3783: Loss = 12552301005190.125\n",
      "Iteration 3784: Loss = 12552219774633.82\n",
      "Iteration 3785: Loss = 12552138567173.182\n",
      "Iteration 3786: Loss = 12552057382801.502\n",
      "Iteration 3787: Loss = 12551976221512.08\n",
      "Iteration 3788: Loss = 12551895083298.213\n",
      "Iteration 3789: Loss = 12551813968153.2\n",
      "Iteration 3790: Loss = 12551732876070.348\n",
      "Iteration 3791: Loss = 12551651807042.955\n",
      "Iteration 3792: Loss = 12551570761064.334\n",
      "Iteration 3793: Loss = 12551489738127.787\n",
      "Iteration 3794: Loss = 12551408738226.625\n",
      "Iteration 3795: Loss = 12551327761354.166\n",
      "Iteration 3796: Loss = 12551246807503.719\n",
      "Iteration 3797: Loss = 12551165876668.602\n",
      "Iteration 3798: Loss = 12551084968842.135\n",
      "Iteration 3799: Loss = 12551004084017.635\n",
      "Iteration 3800: Loss = 12550923222188.424\n",
      "Iteration 3801: Loss = 12550842383347.828\n",
      "Iteration 3802: Loss = 12550761567489.17\n",
      "Iteration 3803: Loss = 12550680774605.783\n",
      "Iteration 3804: Loss = 12550600004690.992\n",
      "Iteration 3805: Loss = 12550519257738.13\n",
      "Iteration 3806: Loss = 12550438533740.531\n",
      "Iteration 3807: Loss = 12550357832691.533\n",
      "Iteration 3808: Loss = 12550277154584.473\n",
      "Iteration 3809: Loss = 12550196499412.69\n",
      "Iteration 3810: Loss = 12550115867169.523\n",
      "Iteration 3811: Loss = 12550035257848.318\n",
      "Iteration 3812: Loss = 12549954671442.422\n",
      "Iteration 3813: Loss = 12549874107945.182\n",
      "Iteration 3814: Loss = 12549793567349.945\n",
      "Iteration 3815: Loss = 12549713049650.066\n",
      "Iteration 3816: Loss = 12549632554838.895\n",
      "Iteration 3817: Loss = 12549552082909.791\n",
      "Iteration 3818: Loss = 12549471633856.105\n",
      "Iteration 3819: Loss = 12549391207671.203\n",
      "Iteration 3820: Loss = 12549310804348.443\n",
      "Iteration 3821: Loss = 12549230423881.19\n",
      "Iteration 3822: Loss = 12549150066262.807\n",
      "Iteration 3823: Loss = 12549069731486.66\n",
      "Iteration 3824: Loss = 12548989419546.123\n",
      "Iteration 3825: Loss = 12548909130434.562\n",
      "Iteration 3826: Loss = 12548828864145.354\n",
      "Iteration 3827: Loss = 12548748620671.87\n",
      "Iteration 3828: Loss = 12548668400007.488\n",
      "Iteration 3829: Loss = 12548588202145.59\n",
      "Iteration 3830: Loss = 12548508027079.549\n",
      "Iteration 3831: Loss = 12548427874802.756\n",
      "Iteration 3832: Loss = 12548347745308.59\n",
      "Iteration 3833: Loss = 12548267638590.44\n",
      "Iteration 3834: Loss = 12548187554641.695\n",
      "Iteration 3835: Loss = 12548107493455.742\n",
      "Iteration 3836: Loss = 12548027455025.982\n",
      "Iteration 3837: Loss = 12547947439345.799\n",
      "Iteration 3838: Loss = 12547867446408.592\n",
      "Iteration 3839: Loss = 12547787476207.764\n",
      "Iteration 3840: Loss = 12547707528736.709\n",
      "Iteration 3841: Loss = 12547627603988.83\n",
      "Iteration 3842: Loss = 12547547701957.537\n",
      "Iteration 3843: Loss = 12547467822636.229\n",
      "Iteration 3844: Loss = 12547387966018.316\n",
      "Iteration 3845: Loss = 12547308132097.21\n",
      "Iteration 3846: Loss = 12547228320866.318\n",
      "Iteration 3847: Loss = 12547148532319.062\n",
      "Iteration 3848: Loss = 12547068766448.848\n",
      "Iteration 3849: Loss = 12546989023249.102\n",
      "Iteration 3850: Loss = 12546909302713.236\n",
      "Iteration 3851: Loss = 12546829604834.674\n",
      "Iteration 3852: Loss = 12546749929606.84\n",
      "Iteration 3853: Loss = 12546670277023.164\n",
      "Iteration 3854: Loss = 12546590647077.064\n",
      "Iteration 3855: Loss = 12546511039761.977\n",
      "Iteration 3856: Loss = 12546431455071.328\n",
      "Iteration 3857: Loss = 12546351892998.555\n",
      "Iteration 3858: Loss = 12546272353537.088\n",
      "Iteration 3859: Loss = 12546192836680.37\n",
      "Iteration 3860: Loss = 12546113342421.834\n",
      "Iteration 3861: Loss = 12546033870754.924\n",
      "Iteration 3862: Loss = 12545954421673.084\n",
      "Iteration 3863: Loss = 12545874995169.754\n",
      "Iteration 3864: Loss = 12545795591238.383\n",
      "Iteration 3865: Loss = 12545716209872.42\n",
      "Iteration 3866: Loss = 12545636851065.316\n",
      "Iteration 3867: Loss = 12545557514810.521\n",
      "Iteration 3868: Loss = 12545478201101.49\n",
      "Iteration 3869: Loss = 12545398909931.68\n",
      "Iteration 3870: Loss = 12545319641294.547\n",
      "Iteration 3871: Loss = 12545240395183.557\n",
      "Iteration 3872: Loss = 12545161171592.16\n",
      "Iteration 3873: Loss = 12545081970513.832\n",
      "Iteration 3874: Loss = 12545002791942.031\n",
      "Iteration 3875: Loss = 12544923635870.23\n",
      "Iteration 3876: Loss = 12544844502291.895\n",
      "Iteration 3877: Loss = 12544765391200.5\n",
      "Iteration 3878: Loss = 12544686302589.516\n",
      "Iteration 3879: Loss = 12544607236452.418\n",
      "Iteration 3880: Loss = 12544528192782.686\n",
      "Iteration 3881: Loss = 12544449171573.799\n",
      "Iteration 3882: Loss = 12544370172819.234\n",
      "Iteration 3883: Loss = 12544291196512.48\n",
      "Iteration 3884: Loss = 12544212242647.016\n",
      "Iteration 3885: Loss = 12544133311216.334\n",
      "Iteration 3886: Loss = 12544054402213.92\n",
      "Iteration 3887: Loss = 12543975515633.264\n",
      "Iteration 3888: Loss = 12543896651467.861\n",
      "Iteration 3889: Loss = 12543817809711.201\n",
      "Iteration 3890: Loss = 12543738990356.787\n",
      "Iteration 3891: Loss = 12543660193398.111\n",
      "Iteration 3892: Loss = 12543581418828.68\n",
      "Iteration 3893: Loss = 12543502666641.988\n",
      "Iteration 3894: Loss = 12543423936831.545\n",
      "Iteration 3895: Loss = 12543345229390.854\n",
      "Iteration 3896: Loss = 12543266544313.426\n",
      "Iteration 3897: Loss = 12543187881592.768\n",
      "Iteration 3898: Loss = 12543109241222.389\n",
      "Iteration 3899: Loss = 12543030623195.809\n",
      "Iteration 3900: Loss = 12542952027506.541\n",
      "Iteration 3901: Loss = 12542873454148.104\n",
      "Iteration 3902: Loss = 12542794903114.012\n",
      "Iteration 3903: Loss = 12542716374397.785\n",
      "Iteration 3904: Loss = 12542637867992.955\n",
      "Iteration 3905: Loss = 12542559383893.045\n",
      "Iteration 3906: Loss = 12542480922091.574\n",
      "Iteration 3907: Loss = 12542402482582.08\n",
      "Iteration 3908: Loss = 12542324065358.088\n",
      "Iteration 3909: Loss = 12542245670413.135\n",
      "Iteration 3910: Loss = 12542167297740.748\n",
      "Iteration 3911: Loss = 12542088947334.473\n",
      "Iteration 3912: Loss = 12542010619187.842\n",
      "Iteration 3913: Loss = 12541932313294.396\n",
      "Iteration 3914: Loss = 12541854029647.682\n",
      "Iteration 3915: Loss = 12541775768241.234\n",
      "Iteration 3916: Loss = 12541697529068.61\n",
      "Iteration 3917: Loss = 12541619312123.348\n",
      "Iteration 3918: Loss = 12541541117399.002\n",
      "Iteration 3919: Loss = 12541462944889.123\n",
      "Iteration 3920: Loss = 12541384794587.262\n",
      "Iteration 3921: Loss = 12541306666486.982\n",
      "Iteration 3922: Loss = 12541228560581.832\n",
      "Iteration 3923: Loss = 12541150476865.373\n",
      "Iteration 3924: Loss = 12541072415331.168\n",
      "Iteration 3925: Loss = 12540994375972.781\n",
      "Iteration 3926: Loss = 12540916358783.773\n",
      "Iteration 3927: Loss = 12540838363757.713\n",
      "Iteration 3928: Loss = 12540760390888.172\n",
      "Iteration 3929: Loss = 12540682440168.717\n",
      "Iteration 3930: Loss = 12540604511592.922\n",
      "Iteration 3931: Loss = 12540526605154.36\n",
      "Iteration 3932: Loss = 12540448720846.605\n",
      "Iteration 3933: Loss = 12540370858663.24\n",
      "Iteration 3934: Loss = 12540293018597.844\n",
      "Iteration 3935: Loss = 12540215200643.996\n",
      "Iteration 3936: Loss = 12540137404795.283\n",
      "Iteration 3937: Loss = 12540059631045.287\n",
      "Iteration 3938: Loss = 12539981879387.602\n",
      "Iteration 3939: Loss = 12539904149815.809\n",
      "Iteration 3940: Loss = 12539826442323.502\n",
      "Iteration 3941: Loss = 12539748756904.275\n",
      "Iteration 3942: Loss = 12539671093551.729\n",
      "Iteration 3943: Loss = 12539593452259.45\n",
      "Iteration 3944: Loss = 12539515833021.045\n",
      "Iteration 3945: Loss = 12539438235830.11\n",
      "Iteration 3946: Loss = 12539360660680.25\n",
      "Iteration 3947: Loss = 12539283107565.068\n",
      "Iteration 3948: Loss = 12539205576478.168\n",
      "Iteration 3949: Loss = 12539128067413.166\n",
      "Iteration 3950: Loss = 12539050580363.664\n",
      "Iteration 3951: Loss = 12538973115323.281\n",
      "Iteration 3952: Loss = 12538895672285.623\n",
      "Iteration 3953: Loss = 12538818251244.312\n",
      "Iteration 3954: Loss = 12538740852192.96\n",
      "Iteration 3955: Loss = 12538663475125.193\n",
      "Iteration 3956: Loss = 12538586120034.627\n",
      "Iteration 3957: Loss = 12538508786914.889\n",
      "Iteration 3958: Loss = 12538431475759.602\n",
      "Iteration 3959: Loss = 12538354186562.393\n",
      "Iteration 3960: Loss = 12538276919316.893\n",
      "Iteration 3961: Loss = 12538199674016.725\n",
      "Iteration 3962: Loss = 12538122450655.531\n",
      "Iteration 3963: Loss = 12538045249226.943\n",
      "Iteration 3964: Loss = 12537968069724.594\n",
      "Iteration 3965: Loss = 12537890912142.125\n",
      "Iteration 3966: Loss = 12537813776473.172\n",
      "Iteration 3967: Loss = 12537736662711.383\n",
      "Iteration 3968: Loss = 12537659570850.4\n",
      "Iteration 3969: Loss = 12537582500883.867\n",
      "Iteration 3970: Loss = 12537505452805.432\n",
      "Iteration 3971: Loss = 12537428426608.742\n",
      "Iteration 3972: Loss = 12537351422287.453\n",
      "Iteration 3973: Loss = 12537274439835.217\n",
      "Iteration 3974: Loss = 12537197479245.684\n",
      "Iteration 3975: Loss = 12537120540512.518\n",
      "Iteration 3976: Loss = 12537043623629.373\n",
      "Iteration 3977: Loss = 12536966728589.908\n",
      "Iteration 3978: Loss = 12536889855387.793\n",
      "Iteration 3979: Loss = 12536813004016.686\n",
      "Iteration 3980: Loss = 12536736174470.254\n",
      "Iteration 3981: Loss = 12536659366742.166\n",
      "Iteration 3982: Loss = 12536582580826.09\n",
      "Iteration 3983: Loss = 12536505816715.701\n",
      "Iteration 3984: Loss = 12536429074404.672\n",
      "Iteration 3985: Loss = 12536352353886.672\n",
      "Iteration 3986: Loss = 12536275655155.389\n",
      "Iteration 3987: Loss = 12536198978204.496\n",
      "Iteration 3988: Loss = 12536122323027.674\n",
      "Iteration 3989: Loss = 12536045689618.605\n",
      "Iteration 3990: Loss = 12535969077970.979\n",
      "Iteration 3991: Loss = 12535892488078.475\n",
      "Iteration 3992: Loss = 12535815919934.79\n",
      "Iteration 3993: Loss = 12535739373533.607\n",
      "Iteration 3994: Loss = 12535662848868.621\n",
      "Iteration 3995: Loss = 12535586345933.525\n",
      "Iteration 3996: Loss = 12535509864722.016\n",
      "Iteration 3997: Loss = 12535433405227.795\n",
      "Iteration 3998: Loss = 12535356967444.555\n",
      "Iteration 3999: Loss = 12535280551366.0\n",
      "Iteration 4000: Loss = 12535204156985.834\n",
      "Iteration 4001: Loss = 12535127784297.764\n",
      "Iteration 4002: Loss = 12535051433295.492\n",
      "Iteration 4003: Loss = 12534975103972.73\n",
      "Iteration 4004: Loss = 12534898796323.19\n",
      "Iteration 4005: Loss = 12534822510340.58\n",
      "Iteration 4006: Loss = 12534746246018.62\n",
      "Iteration 4007: Loss = 12534670003351.02\n",
      "Iteration 4008: Loss = 12534593782331.5\n",
      "Iteration 4009: Loss = 12534517582953.787\n",
      "Iteration 4010: Loss = 12534441405211.592\n",
      "Iteration 4011: Loss = 12534365249098.648\n",
      "Iteration 4012: Loss = 12534289114608.672\n",
      "Iteration 4013: Loss = 12534213001735.396\n",
      "Iteration 4014: Loss = 12534136910472.55\n",
      "Iteration 4015: Loss = 12534060840813.861\n",
      "Iteration 4016: Loss = 12533984792753.066\n",
      "Iteration 4017: Loss = 12533908766283.896\n",
      "Iteration 4018: Loss = 12533832761400.092\n",
      "Iteration 4019: Loss = 12533756778095.385\n",
      "Iteration 4020: Loss = 12533680816363.523\n",
      "Iteration 4021: Loss = 12533604876198.242\n",
      "Iteration 4022: Loss = 12533528957593.293\n",
      "Iteration 4023: Loss = 12533453060542.414\n",
      "Iteration 4024: Loss = 12533377185039.355\n",
      "Iteration 4025: Loss = 12533301331077.865\n",
      "Iteration 4026: Loss = 12533225498651.701\n",
      "Iteration 4027: Loss = 12533149687754.605\n",
      "Iteration 4028: Loss = 12533073898380.344\n",
      "Iteration 4029: Loss = 12532998130522.664\n",
      "Iteration 4030: Loss = 12532922384175.332\n",
      "Iteration 4031: Loss = 12532846659332.104\n",
      "Iteration 4032: Loss = 12532770955986.74\n",
      "Iteration 4033: Loss = 12532695274133.008\n",
      "Iteration 4034: Loss = 12532619613764.672\n",
      "Iteration 4035: Loss = 12532543974875.5\n",
      "Iteration 4036: Loss = 12532468357459.264\n",
      "Iteration 4037: Loss = 12532392761509.732\n",
      "Iteration 4038: Loss = 12532317187020.678\n",
      "Iteration 4039: Loss = 12532241633985.877\n",
      "Iteration 4040: Loss = 12532166102399.107\n",
      "Iteration 4041: Loss = 12532090592254.146\n",
      "Iteration 4042: Loss = 12532015103544.773\n",
      "Iteration 4043: Loss = 12531939636264.773\n",
      "Iteration 4044: Loss = 12531864190407.928\n",
      "Iteration 4045: Loss = 12531788765968.025\n",
      "Iteration 4046: Loss = 12531713362938.852\n",
      "Iteration 4047: Loss = 12531637981314.197\n",
      "Iteration 4048: Loss = 12531562621087.854\n",
      "Iteration 4049: Loss = 12531487282253.613\n",
      "Iteration 4050: Loss = 12531411964805.27\n",
      "Iteration 4051: Loss = 12531336668736.625\n",
      "Iteration 4052: Loss = 12531261394041.47\n",
      "Iteration 4053: Loss = 12531186140713.62\n",
      "Iteration 4054: Loss = 12531110908746.861\n",
      "Iteration 4055: Loss = 12531035698135.002\n",
      "Iteration 4056: Loss = 12530960508871.854\n",
      "Iteration 4057: Loss = 12530885340951.219\n",
      "Iteration 4058: Loss = 12530810194366.91\n",
      "Iteration 4059: Loss = 12530735069112.738\n",
      "Iteration 4060: Loss = 12530659965182.516\n",
      "Iteration 4061: Loss = 12530584882570.059\n",
      "Iteration 4062: Loss = 12530509821269.186\n",
      "Iteration 4063: Loss = 12530434781273.71\n",
      "Iteration 4064: Loss = 12530359762577.457\n",
      "Iteration 4065: Loss = 12530284765174.246\n",
      "Iteration 4066: Loss = 12530209789057.904\n",
      "Iteration 4067: Loss = 12530134834222.258\n",
      "Iteration 4068: Loss = 12530059900661.129\n",
      "Iteration 4069: Loss = 12529984988368.354\n",
      "Iteration 4070: Loss = 12529910097337.762\n",
      "Iteration 4071: Loss = 12529835227563.186\n",
      "Iteration 4072: Loss = 12529760379038.459\n",
      "Iteration 4073: Loss = 12529685551757.42\n",
      "Iteration 4074: Loss = 12529610745713.91\n",
      "Iteration 4075: Loss = 12529535960901.764\n",
      "Iteration 4076: Loss = 12529461197314.83\n",
      "Iteration 4077: Loss = 12529386454946.95\n",
      "Iteration 4078: Loss = 12529311733791.965\n",
      "Iteration 4079: Loss = 12529237033843.729\n",
      "Iteration 4080: Loss = 12529162355096.092\n",
      "Iteration 4081: Loss = 12529087697542.898\n",
      "Iteration 4082: Loss = 12529013061178.012\n",
      "Iteration 4083: Loss = 12528938445995.277\n",
      "Iteration 4084: Loss = 12528863851988.555\n",
      "Iteration 4085: Loss = 12528789279151.707\n",
      "Iteration 4086: Loss = 12528714727478.588\n",
      "Iteration 4087: Loss = 12528640196963.064\n",
      "Iteration 4088: Loss = 12528565687598.998\n",
      "Iteration 4089: Loss = 12528491199380.256\n",
      "Iteration 4090: Loss = 12528416732300.703\n",
      "Iteration 4091: Loss = 12528342286354.213\n",
      "Iteration 4092: Loss = 12528267861534.652\n",
      "Iteration 4093: Loss = 12528193457835.9\n",
      "Iteration 4094: Loss = 12528119075251.824\n",
      "Iteration 4095: Loss = 12528044713776.305\n",
      "Iteration 4096: Loss = 12527970373403.22\n",
      "Iteration 4097: Loss = 12527896054126.451\n",
      "Iteration 4098: Loss = 12527821755939.875\n",
      "Iteration 4099: Loss = 12527747478837.383\n",
      "Iteration 4100: Loss = 12527673222812.857\n",
      "Iteration 4101: Loss = 12527598987860.184\n",
      "Iteration 4102: Loss = 12527524773973.252\n",
      "Iteration 4103: Loss = 12527450581145.953\n",
      "Iteration 4104: Loss = 12527376409372.182\n",
      "Iteration 4105: Loss = 12527302258645.832\n",
      "Iteration 4106: Loss = 12527228128960.797\n",
      "Iteration 4107: Loss = 12527154020310.977\n",
      "Iteration 4108: Loss = 12527079932690.273\n",
      "Iteration 4109: Loss = 12527005866092.588\n",
      "Iteration 4110: Loss = 12526931820511.82\n",
      "Iteration 4111: Loss = 12526857795941.877\n",
      "Iteration 4112: Loss = 12526783792376.668\n",
      "Iteration 4113: Loss = 12526709809810.102\n",
      "Iteration 4114: Loss = 12526635848236.084\n",
      "Iteration 4115: Loss = 12526561907648.535\n",
      "Iteration 4116: Loss = 12526487988041.363\n",
      "Iteration 4117: Loss = 12526414089408.484\n",
      "Iteration 4118: Loss = 12526340211743.82\n",
      "Iteration 4119: Loss = 12526266355041.287\n",
      "Iteration 4120: Loss = 12526192519294.807\n",
      "Iteration 4121: Loss = 12526118704498.307\n",
      "Iteration 4122: Loss = 12526044910645.707\n",
      "Iteration 4123: Loss = 12525971137730.936\n",
      "Iteration 4124: Loss = 12525897385747.92\n",
      "Iteration 4125: Loss = 12525823654690.592\n",
      "Iteration 4126: Loss = 12525749944552.885\n",
      "Iteration 4127: Loss = 12525676255328.729\n",
      "Iteration 4128: Loss = 12525602587012.062\n",
      "Iteration 4129: Loss = 12525528939596.824\n",
      "Iteration 4130: Loss = 12525455313076.95\n",
      "Iteration 4131: Loss = 12525381707446.379\n",
      "Iteration 4132: Loss = 12525308122699.06\n",
      "Iteration 4133: Loss = 12525234558828.936\n",
      "Iteration 4134: Loss = 12525161015829.953\n",
      "Iteration 4135: Loss = 12525087493696.055\n",
      "Iteration 4136: Loss = 12525013992421.197\n",
      "Iteration 4137: Loss = 12524940511999.328\n",
      "Iteration 4138: Loss = 12524867052424.404\n",
      "Iteration 4139: Loss = 12524793613690.377\n",
      "Iteration 4140: Loss = 12524720195791.205\n",
      "Iteration 4141: Loss = 12524646798720.848\n",
      "Iteration 4142: Loss = 12524573422473.264\n",
      "Iteration 4143: Loss = 12524500067042.418\n",
      "Iteration 4144: Loss = 12524426732422.275\n",
      "Iteration 4145: Loss = 12524353418606.799\n",
      "Iteration 4146: Loss = 12524280125589.955\n",
      "Iteration 4147: Loss = 12524206853365.715\n",
      "Iteration 4148: Loss = 12524133601928.05\n",
      "Iteration 4149: Loss = 12524060371270.938\n",
      "Iteration 4150: Loss = 12523987161388.344\n",
      "Iteration 4151: Loss = 12523913972274.252\n",
      "Iteration 4152: Loss = 12523840803922.635\n",
      "Iteration 4153: Loss = 12523767656327.475\n",
      "Iteration 4154: Loss = 12523694529482.756\n",
      "Iteration 4155: Loss = 12523621423382.46\n",
      "Iteration 4156: Loss = 12523548338020.572\n",
      "Iteration 4157: Loss = 12523475273391.078\n",
      "Iteration 4158: Loss = 12523402229487.973\n",
      "Iteration 4159: Loss = 12523329206305.236\n",
      "Iteration 4160: Loss = 12523256203836.871\n",
      "Iteration 4161: Loss = 12523183222076.863\n",
      "Iteration 4162: Loss = 12523110261019.219\n",
      "Iteration 4163: Loss = 12523037320657.924\n",
      "Iteration 4164: Loss = 12522964400986.984\n",
      "Iteration 4165: Loss = 12522891502000.4\n",
      "Iteration 4166: Loss = 12522818623692.178\n",
      "Iteration 4167: Loss = 12522745766056.316\n",
      "Iteration 4168: Loss = 12522672929086.826\n",
      "Iteration 4169: Loss = 12522600112777.71\n",
      "Iteration 4170: Loss = 12522527317122.984\n",
      "Iteration 4171: Loss = 12522454542116.66\n",
      "Iteration 4172: Loss = 12522381787752.742\n",
      "Iteration 4173: Loss = 12522309054025.258\n",
      "Iteration 4174: Loss = 12522236340928.22\n",
      "Iteration 4175: Loss = 12522163648455.646\n",
      "Iteration 4176: Loss = 12522090976601.557\n",
      "Iteration 4177: Loss = 12522018325359.975\n",
      "Iteration 4178: Loss = 12521945694724.922\n",
      "Iteration 4179: Loss = 12521873084690.43\n",
      "Iteration 4180: Loss = 12521800495250.518\n",
      "Iteration 4181: Loss = 12521727926399.223\n",
      "Iteration 4182: Loss = 12521655378130.57\n",
      "Iteration 4183: Loss = 12521582850438.598\n",
      "Iteration 4184: Loss = 12521510343317.336\n",
      "Iteration 4185: Loss = 12521437856760.826\n",
      "Iteration 4186: Loss = 12521365390763.102\n",
      "Iteration 4187: Loss = 12521292945318.201\n",
      "Iteration 4188: Loss = 12521220520420.172\n",
      "Iteration 4189: Loss = 12521148116063.053\n",
      "Iteration 4190: Loss = 12521075732240.893\n",
      "Iteration 4191: Loss = 12521003368947.732\n",
      "Iteration 4192: Loss = 12520931026177.627\n",
      "Iteration 4193: Loss = 12520858703924.621\n",
      "Iteration 4194: Loss = 12520786402182.775\n",
      "Iteration 4195: Loss = 12520714120946.133\n",
      "Iteration 4196: Loss = 12520641860208.758\n",
      "Iteration 4197: Loss = 12520569619964.701\n",
      "Iteration 4198: Loss = 12520497400208.027\n",
      "Iteration 4199: Loss = 12520425200932.793\n",
      "Iteration 4200: Loss = 12520353022133.062\n",
      "Iteration 4201: Loss = 12520280863802.898\n",
      "Iteration 4202: Loss = 12520208725936.37\n",
      "Iteration 4203: Loss = 12520136608527.543\n",
      "Iteration 4204: Loss = 12520064511570.488\n",
      "Iteration 4205: Loss = 12519992435059.275\n",
      "Iteration 4206: Loss = 12519920378987.979\n",
      "Iteration 4207: Loss = 12519848343350.67\n",
      "Iteration 4208: Loss = 12519776328141.43\n",
      "Iteration 4209: Loss = 12519704333354.334\n",
      "Iteration 4210: Loss = 12519632358983.465\n",
      "Iteration 4211: Loss = 12519560405022.898\n",
      "Iteration 4212: Loss = 12519488471466.727\n",
      "Iteration 4213: Loss = 12519416558309.027\n",
      "Iteration 4214: Loss = 12519344665543.893\n",
      "Iteration 4215: Loss = 12519272793165.406\n",
      "Iteration 4216: Loss = 12519200941167.662\n",
      "Iteration 4217: Loss = 12519129109544.748\n",
      "Iteration 4218: Loss = 12519057298290.768\n",
      "Iteration 4219: Loss = 12518985507399.805\n",
      "Iteration 4220: Loss = 12518913736865.963\n",
      "Iteration 4221: Loss = 12518841986683.344\n",
      "Iteration 4222: Loss = 12518770256846.041\n",
      "Iteration 4223: Loss = 12518698547348.162\n",
      "Iteration 4224: Loss = 12518626858183.809\n",
      "Iteration 4225: Loss = 12518555189347.088\n",
      "Iteration 4226: Loss = 12518483540832.11\n",
      "Iteration 4227: Loss = 12518411912632.979\n",
      "Iteration 4228: Loss = 12518340304743.809\n",
      "Iteration 4229: Loss = 12518268717158.715\n",
      "Iteration 4230: Loss = 12518197149871.809\n",
      "Iteration 4231: Loss = 12518125602877.209\n",
      "Iteration 4232: Loss = 12518054076169.031\n",
      "Iteration 4233: Loss = 12517982569741.395\n",
      "Iteration 4234: Loss = 12517911083588.422\n",
      "Iteration 4235: Loss = 12517839617704.24\n",
      "Iteration 4236: Loss = 12517768172082.969\n",
      "Iteration 4237: Loss = 12517696746718.738\n",
      "Iteration 4238: Loss = 12517625341605.676\n",
      "Iteration 4239: Loss = 12517553956737.908\n",
      "Iteration 4240: Loss = 12517482592109.574\n",
      "Iteration 4241: Loss = 12517411247714.8\n",
      "Iteration 4242: Loss = 12517339923547.729\n",
      "Iteration 4243: Loss = 12517268619602.49\n",
      "Iteration 4244: Loss = 12517197335873.227\n",
      "Iteration 4245: Loss = 12517126072354.078\n",
      "Iteration 4246: Loss = 12517054829039.188\n",
      "Iteration 4247: Loss = 12516983605922.697\n",
      "Iteration 4248: Loss = 12516912402998.756\n",
      "Iteration 4249: Loss = 12516841220261.51\n",
      "Iteration 4250: Loss = 12516770057705.105\n",
      "Iteration 4251: Loss = 12516698915323.693\n",
      "Iteration 4252: Loss = 12516627793111.432\n",
      "Iteration 4253: Loss = 12516556691062.469\n",
      "Iteration 4254: Loss = 12516485609170.96\n",
      "Iteration 4255: Loss = 12516414547431.072\n",
      "Iteration 4256: Loss = 12516343505836.955\n",
      "Iteration 4257: Loss = 12516272484382.775\n",
      "Iteration 4258: Loss = 12516201483062.691\n",
      "Iteration 4259: Loss = 12516130501870.871\n",
      "Iteration 4260: Loss = 12516059540801.477\n",
      "Iteration 4261: Loss = 12515988599848.682\n",
      "Iteration 4262: Loss = 12515917679006.652\n",
      "Iteration 4263: Loss = 12515846778269.562\n",
      "Iteration 4264: Loss = 12515775897631.584\n",
      "Iteration 4265: Loss = 12515705037086.889\n",
      "Iteration 4266: Loss = 12515634196629.658\n",
      "Iteration 4267: Loss = 12515563376254.066\n",
      "Iteration 4268: Loss = 12515492575954.295\n",
      "Iteration 4269: Loss = 12515421795724.527\n",
      "Iteration 4270: Loss = 12515351035558.943\n",
      "Iteration 4271: Loss = 12515280295451.729\n",
      "Iteration 4272: Loss = 12515209575397.076\n",
      "Iteration 4273: Loss = 12515138875389.166\n",
      "Iteration 4274: Loss = 12515068195422.191\n",
      "Iteration 4275: Loss = 12514997535490.348\n",
      "Iteration 4276: Loss = 12514926895587.822\n",
      "Iteration 4277: Loss = 12514856275708.814\n",
      "Iteration 4278: Loss = 12514785675847.523\n",
      "Iteration 4279: Loss = 12514715095998.14\n",
      "Iteration 4280: Loss = 12514644536154.873\n",
      "Iteration 4281: Loss = 12514573996311.922\n",
      "Iteration 4282: Loss = 12514503476463.488\n",
      "Iteration 4283: Loss = 12514432976603.777\n",
      "Iteration 4284: Loss = 12514362496727.0\n",
      "Iteration 4285: Loss = 12514292036827.361\n",
      "Iteration 4286: Loss = 12514221596899.076\n",
      "Iteration 4287: Loss = 12514151176936.354\n",
      "Iteration 4288: Loss = 12514080776933.408\n",
      "Iteration 4289: Loss = 12514010396884.457\n",
      "Iteration 4290: Loss = 12513940036783.715\n",
      "Iteration 4291: Loss = 12513869696625.404\n",
      "Iteration 4292: Loss = 12513799376403.742\n",
      "Iteration 4293: Loss = 12513729076112.955\n",
      "Iteration 4294: Loss = 12513658795747.264\n",
      "Iteration 4295: Loss = 12513588535300.898\n",
      "Iteration 4296: Loss = 12513518294768.08\n",
      "Iteration 4297: Loss = 12513448074143.045\n",
      "Iteration 4298: Loss = 12513377873420.021\n",
      "Iteration 4299: Loss = 12513307692593.24\n",
      "Iteration 4300: Loss = 12513237531656.938\n",
      "Iteration 4301: Loss = 12513167390605.348\n",
      "Iteration 4302: Loss = 12513097269432.713\n",
      "Iteration 4303: Loss = 12513027168133.266\n",
      "Iteration 4304: Loss = 12512957086701.256\n",
      "Iteration 4305: Loss = 12512887025130.918\n",
      "Iteration 4306: Loss = 12512816983416.5\n",
      "Iteration 4307: Loss = 12512746961552.25\n",
      "Iteration 4308: Loss = 12512676959532.41\n",
      "Iteration 4309: Loss = 12512606977351.238\n",
      "Iteration 4310: Loss = 12512537015002.98\n",
      "Iteration 4311: Loss = 12512467072481.89\n",
      "Iteration 4312: Loss = 12512397149782.223\n",
      "Iteration 4313: Loss = 12512327246898.234\n",
      "Iteration 4314: Loss = 12512257363824.18\n",
      "Iteration 4315: Loss = 12512187500554.324\n",
      "Iteration 4316: Loss = 12512117657082.928\n",
      "Iteration 4317: Loss = 12512047833404.252\n",
      "Iteration 4318: Loss = 12511978029512.56\n",
      "Iteration 4319: Loss = 12511908245402.125\n",
      "Iteration 4320: Loss = 12511838481067.205\n",
      "Iteration 4321: Loss = 12511768736502.078\n",
      "Iteration 4322: Loss = 12511699011701.014\n",
      "Iteration 4323: Loss = 12511629306658.283\n",
      "Iteration 4324: Loss = 12511559621368.164\n",
      "Iteration 4325: Loss = 12511489955824.93\n",
      "Iteration 4326: Loss = 12511420310022.861\n",
      "Iteration 4327: Loss = 12511350683956.234\n",
      "Iteration 4328: Loss = 12511281077619.338\n",
      "Iteration 4329: Loss = 12511211491006.447\n",
      "Iteration 4330: Loss = 12511141924111.854\n",
      "Iteration 4331: Loss = 12511072376929.838\n",
      "Iteration 4332: Loss = 12511002849454.693\n",
      "Iteration 4333: Loss = 12510933341680.709\n",
      "Iteration 4334: Loss = 12510863853602.172\n",
      "Iteration 4335: Loss = 12510794385213.383\n",
      "Iteration 4336: Loss = 12510724936508.63\n",
      "Iteration 4337: Loss = 12510655507482.217\n",
      "Iteration 4338: Loss = 12510586098128.434\n",
      "Iteration 4339: Loss = 12510516708441.588\n",
      "Iteration 4340: Loss = 12510447338415.979\n",
      "Iteration 4341: Loss = 12510377988045.904\n",
      "Iteration 4342: Loss = 12510308657325.678\n",
      "Iteration 4343: Loss = 12510239346249.6\n",
      "Iteration 4344: Loss = 12510170054811.984\n",
      "Iteration 4345: Loss = 12510100783007.137\n",
      "Iteration 4346: Loss = 12510031530829.37\n",
      "Iteration 4347: Loss = 12509962298272.996\n",
      "Iteration 4348: Loss = 12509893085332.334\n",
      "Iteration 4349: Loss = 12509823892001.701\n",
      "Iteration 4350: Loss = 12509754718275.408\n",
      "Iteration 4351: Loss = 12509685564147.783\n",
      "Iteration 4352: Loss = 12509616429613.14\n",
      "Iteration 4353: Loss = 12509547314665.814\n",
      "Iteration 4354: Loss = 12509478219300.12\n",
      "Iteration 4355: Loss = 12509409143510.385\n",
      "Iteration 4356: Loss = 12509340087290.943\n",
      "Iteration 4357: Loss = 12509271050636.121\n",
      "Iteration 4358: Loss = 12509202033540.25\n",
      "Iteration 4359: Loss = 12509133035997.666\n",
      "Iteration 4360: Loss = 12509064058002.703\n",
      "Iteration 4361: Loss = 12508995099549.697\n",
      "Iteration 4362: Loss = 12508926160632.984\n",
      "Iteration 4363: Loss = 12508857241246.906\n",
      "Iteration 4364: Loss = 12508788341385.809\n",
      "Iteration 4365: Loss = 12508719461044.03\n",
      "Iteration 4366: Loss = 12508650600215.918\n",
      "Iteration 4367: Loss = 12508581758895.816\n",
      "Iteration 4368: Loss = 12508512937078.078\n",
      "Iteration 4369: Loss = 12508444134757.049\n",
      "Iteration 4370: Loss = 12508375351927.084\n",
      "Iteration 4371: Loss = 12508306588582.533\n",
      "Iteration 4372: Loss = 12508237844717.754\n",
      "Iteration 4373: Loss = 12508169120327.104\n",
      "Iteration 4374: Loss = 12508100415404.936\n",
      "Iteration 4375: Loss = 12508031729945.615\n",
      "Iteration 4376: Loss = 12507963063943.5\n",
      "Iteration 4377: Loss = 12507894417392.957\n",
      "Iteration 4378: Loss = 12507825790288.35\n",
      "Iteration 4379: Loss = 12507757182624.045\n",
      "Iteration 4380: Loss = 12507688594394.408\n",
      "Iteration 4381: Loss = 12507620025593.81\n",
      "Iteration 4382: Loss = 12507551476216.629\n",
      "Iteration 4383: Loss = 12507482946257.227\n",
      "Iteration 4384: Loss = 12507414435709.988\n",
      "Iteration 4385: Loss = 12507345944569.285\n",
      "Iteration 4386: Loss = 12507277472829.496\n",
      "Iteration 4387: Loss = 12507209020484.998\n",
      "Iteration 4388: Loss = 12507140587530.18\n",
      "Iteration 4389: Loss = 12507072173959.418\n",
      "Iteration 4390: Loss = 12507003779767.102\n",
      "Iteration 4391: Loss = 12506935404947.61\n",
      "Iteration 4392: Loss = 12506867049495.338\n",
      "Iteration 4393: Loss = 12506798713404.676\n",
      "Iteration 4394: Loss = 12506730396670.012\n",
      "Iteration 4395: Loss = 12506662099285.736\n",
      "Iteration 4396: Loss = 12506593821246.25\n",
      "Iteration 4397: Loss = 12506525562545.943\n",
      "Iteration 4398: Loss = 12506457323179.219\n",
      "Iteration 4399: Loss = 12506389103140.473\n",
      "Iteration 4400: Loss = 12506320902424.111\n",
      "Iteration 4401: Loss = 12506252721024.531\n",
      "Iteration 4402: Loss = 12506184558936.14\n",
      "Iteration 4403: Loss = 12506116416153.344\n",
      "Iteration 4404: Loss = 12506048292670.55\n",
      "Iteration 4405: Loss = 12505980188482.166\n",
      "Iteration 4406: Loss = 12505912103582.61\n",
      "Iteration 4407: Loss = 12505844037966.285\n",
      "Iteration 4408: Loss = 12505775991627.611\n",
      "Iteration 4409: Loss = 12505707964561.002\n",
      "Iteration 4410: Loss = 12505639956760.877\n",
      "Iteration 4411: Loss = 12505571968221.654\n",
      "Iteration 4412: Loss = 12505503998937.754\n",
      "Iteration 4413: Loss = 12505436048903.602\n",
      "Iteration 4414: Loss = 12505368118113.621\n",
      "Iteration 4415: Loss = 12505300206562.23\n",
      "Iteration 4416: Loss = 12505232314243.867\n",
      "Iteration 4417: Loss = 12505164441152.957\n",
      "Iteration 4418: Loss = 12505096587283.93\n",
      "Iteration 4419: Loss = 12505028752631.219\n",
      "Iteration 4420: Loss = 12504960937189.256\n",
      "Iteration 4421: Loss = 12504893140952.475\n",
      "Iteration 4422: Loss = 12504825363915.32\n",
      "Iteration 4423: Loss = 12504757606072.227\n",
      "Iteration 4424: Loss = 12504689867417.633\n",
      "Iteration 4425: Loss = 12504622147945.984\n",
      "Iteration 4426: Loss = 12504554447651.723\n",
      "Iteration 4427: Loss = 12504486766529.291\n",
      "Iteration 4428: Loss = 12504419104573.145\n",
      "Iteration 4429: Loss = 12504351461777.727\n",
      "Iteration 4430: Loss = 12504283838137.486\n",
      "Iteration 4431: Loss = 12504216233646.875\n",
      "Iteration 4432: Loss = 12504148648300.348\n",
      "Iteration 4433: Loss = 12504081082092.363\n",
      "Iteration 4434: Loss = 12504013535017.373\n",
      "Iteration 4435: Loss = 12503946007069.838\n",
      "Iteration 4436: Loss = 12503878498244.217\n",
      "Iteration 4437: Loss = 12503811008534.975\n",
      "Iteration 4438: Loss = 12503743537936.568\n",
      "Iteration 4439: Loss = 12503676086443.469\n",
      "Iteration 4440: Loss = 12503608654050.14\n",
      "Iteration 4441: Loss = 12503541240751.05\n",
      "Iteration 4442: Loss = 12503473846540.674\n",
      "Iteration 4443: Loss = 12503406471413.473\n",
      "Iteration 4444: Loss = 12503339115363.928\n",
      "Iteration 4445: Loss = 12503271778386.512\n",
      "Iteration 4446: Loss = 12503204460475.7\n",
      "Iteration 4447: Loss = 12503137161625.969\n",
      "Iteration 4448: Loss = 12503069881831.803\n",
      "Iteration 4449: Loss = 12503002621087.678\n",
      "Iteration 4450: Loss = 12502935379388.08\n",
      "Iteration 4451: Loss = 12502868156727.49\n",
      "Iteration 4452: Loss = 12502800953100.398\n",
      "Iteration 4453: Loss = 12502733768501.291\n",
      "Iteration 4454: Loss = 12502666602924.654\n",
      "Iteration 4455: Loss = 12502599456364.986\n",
      "Iteration 4456: Loss = 12502532328816.771\n",
      "Iteration 4457: Loss = 12502465220274.51\n",
      "Iteration 4458: Loss = 12502398130732.693\n",
      "Iteration 4459: Loss = 12502331060185.82\n",
      "Iteration 4460: Loss = 12502264008628.39\n",
      "Iteration 4461: Loss = 12502196976054.9\n",
      "Iteration 4462: Loss = 12502129962459.861\n",
      "Iteration 4463: Loss = 12502062967837.77\n",
      "Iteration 4464: Loss = 12501995992183.13\n",
      "Iteration 4465: Loss = 12501929035490.451\n",
      "Iteration 4466: Loss = 12501862097754.246\n",
      "Iteration 4467: Loss = 12501795178969.02\n",
      "Iteration 4468: Loss = 12501728279129.287\n",
      "Iteration 4469: Loss = 12501661398229.56\n",
      "Iteration 4470: Loss = 12501594536264.352\n",
      "Iteration 4471: Loss = 12501527693228.182\n",
      "Iteration 4472: Loss = 12501460869115.568\n",
      "Iteration 4473: Loss = 12501394063921.031\n",
      "Iteration 4474: Loss = 12501327277639.09\n",
      "Iteration 4475: Loss = 12501260510264.27\n",
      "Iteration 4476: Loss = 12501193761791.094\n",
      "Iteration 4477: Loss = 12501127032214.088\n",
      "Iteration 4478: Loss = 12501060321527.78\n",
      "Iteration 4479: Loss = 12500993629726.705\n",
      "Iteration 4480: Loss = 12500926956805.387\n",
      "Iteration 4481: Loss = 12500860302758.363\n",
      "Iteration 4482: Loss = 12500793667580.166\n",
      "Iteration 4483: Loss = 12500727051265.332\n",
      "Iteration 4484: Loss = 12500660453808.4\n",
      "Iteration 4485: Loss = 12500593875203.902\n",
      "Iteration 4486: Loss = 12500527315446.389\n",
      "Iteration 4487: Loss = 12500460774530.396\n",
      "Iteration 4488: Loss = 12500394252450.47\n",
      "Iteration 4489: Loss = 12500327749201.16\n",
      "Iteration 4490: Loss = 12500261264777.006\n",
      "Iteration 4491: Loss = 12500194799172.56\n",
      "Iteration 4492: Loss = 12500128352382.375\n",
      "Iteration 4493: Loss = 12500061924400.998\n",
      "Iteration 4494: Loss = 12499995515222.986\n",
      "Iteration 4495: Loss = 12499929124842.895\n",
      "Iteration 4496: Loss = 12499862753255.277\n",
      "Iteration 4497: Loss = 12499796400454.695\n",
      "Iteration 4498: Loss = 12499730066435.71\n",
      "Iteration 4499: Loss = 12499663751192.879\n",
      "Iteration 4500: Loss = 12499597454720.764\n",
      "Iteration 4501: Loss = 12499531177013.936\n",
      "Iteration 4502: Loss = 12499464918066.955\n",
      "Iteration 4503: Loss = 12499398677874.395\n",
      "Iteration 4504: Loss = 12499332456430.82\n",
      "Iteration 4505: Loss = 12499266253730.807\n",
      "Iteration 4506: Loss = 12499200069768.924\n",
      "Iteration 4507: Loss = 12499133904539.746\n",
      "Iteration 4508: Loss = 12499067758037.854\n",
      "Iteration 4509: Loss = 12499001630257.812\n",
      "Iteration 4510: Loss = 12498935521194.215\n",
      "Iteration 4511: Loss = 12498869430841.637\n",
      "Iteration 4512: Loss = 12498803359194.656\n",
      "Iteration 4513: Loss = 12498737306247.863\n",
      "Iteration 4514: Loss = 12498671271995.838\n",
      "Iteration 4515: Loss = 12498605256433.174\n",
      "Iteration 4516: Loss = 12498539259554.455\n",
      "Iteration 4517: Loss = 12498473281354.271\n",
      "Iteration 4518: Loss = 12498407321827.215\n",
      "Iteration 4519: Loss = 12498341380967.883\n",
      "Iteration 4520: Loss = 12498275458770.863\n",
      "Iteration 4521: Loss = 12498209555230.758\n",
      "Iteration 4522: Loss = 12498143670342.166\n",
      "Iteration 4523: Loss = 12498077804099.682\n",
      "Iteration 4524: Loss = 12498011956497.912\n",
      "Iteration 4525: Loss = 12497946127531.455\n",
      "Iteration 4526: Loss = 12497880317194.92\n",
      "Iteration 4527: Loss = 12497814525482.908\n",
      "Iteration 4528: Loss = 12497748752390.031\n",
      "Iteration 4529: Loss = 12497682997910.895\n",
      "Iteration 4530: Loss = 12497617262040.111\n",
      "Iteration 4531: Loss = 12497551544772.297\n",
      "Iteration 4532: Loss = 12497485846102.062\n",
      "Iteration 4533: Loss = 12497420166024.018\n",
      "Iteration 4534: Loss = 12497354504532.791\n",
      "Iteration 4535: Loss = 12497288861622.99\n",
      "Iteration 4536: Loss = 12497223237289.244\n",
      "Iteration 4537: Loss = 12497157631526.168\n",
      "Iteration 4538: Loss = 12497092044328.393\n",
      "Iteration 4539: Loss = 12497026475690.537\n",
      "Iteration 4540: Loss = 12496960925607.229\n",
      "Iteration 4541: Loss = 12496895394073.098\n",
      "Iteration 4542: Loss = 12496829881082.773\n",
      "Iteration 4543: Loss = 12496764386630.887\n",
      "Iteration 4544: Loss = 12496698910712.066\n",
      "Iteration 4545: Loss = 12496633453320.955\n",
      "Iteration 4546: Loss = 12496568014452.184\n",
      "Iteration 4547: Loss = 12496502594100.393\n",
      "Iteration 4548: Loss = 12496437192260.213\n",
      "Iteration 4549: Loss = 12496371808926.297\n",
      "Iteration 4550: Loss = 12496306444093.283\n",
      "Iteration 4551: Loss = 12496241097755.812\n",
      "Iteration 4552: Loss = 12496175769908.53\n",
      "Iteration 4553: Loss = 12496110460546.086\n",
      "Iteration 4554: Loss = 12496045169663.127\n",
      "Iteration 4555: Loss = 12495979897254.305\n",
      "Iteration 4556: Loss = 12495914643314.271\n",
      "Iteration 4557: Loss = 12495849407837.678\n",
      "Iteration 4558: Loss = 12495784190819.184\n",
      "Iteration 4559: Loss = 12495718992253.44\n",
      "Iteration 4560: Loss = 12495653812135.105\n",
      "Iteration 4561: Loss = 12495588650458.842\n",
      "Iteration 4562: Loss = 12495523507219.31\n",
      "Iteration 4563: Loss = 12495458382411.174\n",
      "Iteration 4564: Loss = 12495393276029.094\n",
      "Iteration 4565: Loss = 12495328188067.74\n",
      "Iteration 4566: Loss = 12495263118521.775\n",
      "Iteration 4567: Loss = 12495198067385.875\n",
      "Iteration 4568: Loss = 12495133034654.707\n",
      "Iteration 4569: Loss = 12495068020322.94\n",
      "Iteration 4570: Loss = 12495003024385.248\n",
      "Iteration 4571: Loss = 12494938046836.312\n",
      "Iteration 4572: Loss = 12494873087670.803\n",
      "Iteration 4573: Loss = 12494808146883.404\n",
      "Iteration 4574: Loss = 12494743224468.793\n",
      "Iteration 4575: Loss = 12494678320421.65\n",
      "Iteration 4576: Loss = 12494613434736.656\n",
      "Iteration 4577: Loss = 12494548567408.502\n",
      "Iteration 4578: Loss = 12494483718431.871\n",
      "Iteration 4579: Loss = 12494418887801.453\n",
      "Iteration 4580: Loss = 12494354075511.932\n",
      "Iteration 4581: Loss = 12494289281558.004\n",
      "Iteration 4582: Loss = 12494224505934.357\n",
      "Iteration 4583: Loss = 12494159748635.69\n",
      "Iteration 4584: Loss = 12494095009656.693\n",
      "Iteration 4585: Loss = 12494030288992.068\n",
      "Iteration 4586: Loss = 12493965586636.51\n",
      "Iteration 4587: Loss = 12493900902584.723\n",
      "Iteration 4588: Loss = 12493836236831.406\n",
      "Iteration 4589: Loss = 12493771589371.262\n",
      "Iteration 4590: Loss = 12493706960198.996\n",
      "Iteration 4591: Loss = 12493642349309.318\n",
      "Iteration 4592: Loss = 12493577756696.932\n",
      "Iteration 4593: Loss = 12493513182356.549\n",
      "Iteration 4594: Loss = 12493448626282.879\n",
      "Iteration 4595: Loss = 12493384088470.635\n",
      "Iteration 4596: Loss = 12493319568914.533\n",
      "Iteration 4597: Loss = 12493255067609.287\n",
      "Iteration 4598: Loss = 12493190584549.615\n",
      "Iteration 4599: Loss = 12493126119730.232\n",
      "Iteration 4600: Loss = 12493061673145.863\n",
      "Iteration 4601: Loss = 12492997244791.23\n",
      "Iteration 4602: Loss = 12492932834661.057\n",
      "Iteration 4603: Loss = 12492868442750.068\n",
      "Iteration 4604: Loss = 12492804069052.986\n",
      "Iteration 4605: Loss = 12492739713564.541\n",
      "Iteration 4606: Loss = 12492675376279.465\n",
      "Iteration 4607: Loss = 12492611057192.49\n",
      "Iteration 4608: Loss = 12492546756298.346\n",
      "Iteration 4609: Loss = 12492482473591.764\n",
      "Iteration 4610: Loss = 12492418209067.486\n",
      "Iteration 4611: Loss = 12492353962720.246\n",
      "Iteration 4612: Loss = 12492289734544.785\n",
      "Iteration 4613: Loss = 12492225524535.842\n",
      "Iteration 4614: Loss = 12492161332688.166\n",
      "Iteration 4615: Loss = 12492097158996.484\n",
      "Iteration 4616: Loss = 12492033003455.56\n",
      "Iteration 4617: Loss = 12491968866060.125\n",
      "Iteration 4618: Loss = 12491904746804.936\n",
      "Iteration 4619: Loss = 12491840645684.742\n",
      "Iteration 4620: Loss = 12491776562694.293\n",
      "Iteration 4621: Loss = 12491712497828.344\n",
      "Iteration 4622: Loss = 12491648451081.645\n",
      "Iteration 4623: Loss = 12491584422448.955\n",
      "Iteration 4624: Loss = 12491520411925.03\n",
      "Iteration 4625: Loss = 12491456419504.627\n",
      "Iteration 4626: Loss = 12491392445182.514\n",
      "Iteration 4627: Loss = 12491328488953.445\n",
      "Iteration 4628: Loss = 12491264550812.188\n",
      "Iteration 4629: Loss = 12491200630753.504\n",
      "Iteration 4630: Loss = 12491136728772.166\n",
      "Iteration 4631: Loss = 12491072844862.94\n",
      "Iteration 4632: Loss = 12491008979020.59\n",
      "Iteration 4633: Loss = 12490945131239.895\n",
      "Iteration 4634: Loss = 12490881301515.62\n",
      "Iteration 4635: Loss = 12490817489842.547\n",
      "Iteration 4636: Loss = 12490753696215.447\n",
      "Iteration 4637: Loss = 12490689920629.104\n",
      "Iteration 4638: Loss = 12490626163078.287\n",
      "Iteration 4639: Loss = 12490562423557.781\n",
      "Iteration 4640: Loss = 12490498702062.37\n",
      "Iteration 4641: Loss = 12490434998586.836\n",
      "Iteration 4642: Loss = 12490371313125.965\n",
      "Iteration 4643: Loss = 12490307645674.54\n",
      "Iteration 4644: Loss = 12490243996227.354\n",
      "Iteration 4645: Loss = 12490180364779.193\n",
      "Iteration 4646: Loss = 12490116751324.85\n",
      "Iteration 4647: Loss = 12490053155859.113\n",
      "Iteration 4648: Loss = 12489989578376.785\n",
      "Iteration 4649: Loss = 12489926018872.656\n",
      "Iteration 4650: Loss = 12489862477341.527\n",
      "Iteration 4651: Loss = 12489798953778.191\n",
      "Iteration 4652: Loss = 12489735448177.45\n",
      "Iteration 4653: Loss = 12489671960534.107\n",
      "Iteration 4654: Loss = 12489608490842.97\n",
      "Iteration 4655: Loss = 12489545039098.836\n",
      "Iteration 4656: Loss = 12489481605296.516\n",
      "Iteration 4657: Loss = 12489418189430.82\n",
      "Iteration 4658: Loss = 12489354791496.55\n",
      "Iteration 4659: Loss = 12489291411488.521\n",
      "Iteration 4660: Loss = 12489228049401.553\n",
      "Iteration 4661: Loss = 12489164705230.443\n",
      "Iteration 4662: Loss = 12489101378970.023\n",
      "Iteration 4663: Loss = 12489038070615.104\n",
      "Iteration 4664: Loss = 12488974780160.5\n",
      "Iteration 4665: Loss = 12488911507601.04\n",
      "Iteration 4666: Loss = 12488848252931.537\n",
      "Iteration 4667: Loss = 12488785016146.82\n",
      "Iteration 4668: Loss = 12488721797241.71\n",
      "Iteration 4669: Loss = 12488658596211.037\n",
      "Iteration 4670: Loss = 12488595413049.625\n",
      "Iteration 4671: Loss = 12488532247752.309\n",
      "Iteration 4672: Loss = 12488469100313.91\n",
      "Iteration 4673: Loss = 12488405970729.266\n",
      "Iteration 4674: Loss = 12488342858993.215\n",
      "Iteration 4675: Loss = 12488279765100.586\n",
      "Iteration 4676: Loss = 12488216689046.219\n",
      "Iteration 4677: Loss = 12488153630824.947\n",
      "Iteration 4678: Loss = 12488090590431.62\n",
      "Iteration 4679: Loss = 12488027567861.066\n",
      "Iteration 4680: Loss = 12487964563108.14\n",
      "Iteration 4681: Loss = 12487901576167.682\n",
      "Iteration 4682: Loss = 12487838607034.535\n",
      "Iteration 4683: Loss = 12487775655703.55\n",
      "Iteration 4684: Loss = 12487712722169.574\n",
      "Iteration 4685: Loss = 12487649806427.46\n",
      "Iteration 4686: Loss = 12487586908472.057\n",
      "Iteration 4687: Loss = 12487524028298.22\n",
      "Iteration 4688: Loss = 12487461165900.803\n",
      "Iteration 4689: Loss = 12487398321274.66\n",
      "Iteration 4690: Loss = 12487335494414.656\n",
      "Iteration 4691: Loss = 12487272685315.646\n",
      "Iteration 4692: Loss = 12487209893972.492\n",
      "Iteration 4693: Loss = 12487147120380.05\n",
      "Iteration 4694: Loss = 12487084364533.197\n",
      "Iteration 4695: Loss = 12487021626426.79\n",
      "Iteration 4696: Loss = 12486958906055.693\n",
      "Iteration 4697: Loss = 12486896203414.783\n",
      "Iteration 4698: Loss = 12486833518498.924\n",
      "Iteration 4699: Loss = 12486770851302.99\n",
      "Iteration 4700: Loss = 12486708201821.855\n",
      "Iteration 4701: Loss = 12486645570050.389\n",
      "Iteration 4702: Loss = 12486582955983.47\n",
      "Iteration 4703: Loss = 12486520359615.984\n",
      "Iteration 4704: Loss = 12486457780942.799\n",
      "Iteration 4705: Loss = 12486395219958.799\n",
      "Iteration 4706: Loss = 12486332676658.863\n",
      "Iteration 4707: Loss = 12486270151037.883\n",
      "Iteration 4708: Loss = 12486207643090.734\n",
      "Iteration 4709: Loss = 12486145152812.309\n",
      "Iteration 4710: Loss = 12486082680197.496\n",
      "Iteration 4711: Loss = 12486020225241.182\n",
      "Iteration 4712: Loss = 12485957787938.26\n",
      "Iteration 4713: Loss = 12485895368283.62\n",
      "Iteration 4714: Loss = 12485832966272.158\n",
      "Iteration 4715: Loss = 12485770581898.771\n",
      "Iteration 4716: Loss = 12485708215158.352\n",
      "Iteration 4717: Loss = 12485645866045.803\n",
      "Iteration 4718: Loss = 12485583534556.023\n",
      "Iteration 4719: Loss = 12485521220683.91\n",
      "Iteration 4720: Loss = 12485458924424.375\n",
      "Iteration 4721: Loss = 12485396645772.316\n",
      "Iteration 4722: Loss = 12485334384722.643\n",
      "Iteration 4723: Loss = 12485272141270.258\n",
      "Iteration 4724: Loss = 12485209915410.076\n",
      "Iteration 4725: Loss = 12485147707137.002\n",
      "Iteration 4726: Loss = 12485085516445.953\n",
      "Iteration 4727: Loss = 12485023343331.838\n",
      "Iteration 4728: Loss = 12484961187789.578\n",
      "Iteration 4729: Loss = 12484899049814.084\n",
      "Iteration 4730: Loss = 12484836929400.28\n",
      "Iteration 4731: Loss = 12484774826543.074\n",
      "Iteration 4732: Loss = 12484712741237.396\n",
      "Iteration 4733: Loss = 12484650673478.168\n",
      "Iteration 4734: Loss = 12484588623260.314\n",
      "Iteration 4735: Loss = 12484526590578.758\n",
      "Iteration 4736: Loss = 12484464575428.424\n",
      "Iteration 4737: Loss = 12484402577804.244\n",
      "Iteration 4738: Loss = 12484340597701.146\n",
      "Iteration 4739: Loss = 12484278635114.064\n",
      "Iteration 4740: Loss = 12484216690037.93\n",
      "Iteration 4741: Loss = 12484154762467.678\n",
      "Iteration 4742: Loss = 12484092852398.24\n",
      "Iteration 4743: Loss = 12484030959824.56\n",
      "Iteration 4744: Loss = 12483969084741.572\n",
      "Iteration 4745: Loss = 12483907227144.219\n",
      "Iteration 4746: Loss = 12483845387027.44\n",
      "Iteration 4747: Loss = 12483783564386.18\n",
      "Iteration 4748: Loss = 12483721759215.383\n",
      "Iteration 4749: Loss = 12483659971509.996\n",
      "Iteration 4750: Loss = 12483598201264.967\n",
      "Iteration 4751: Loss = 12483536448475.246\n",
      "Iteration 4752: Loss = 12483474713135.777\n",
      "Iteration 4753: Loss = 12483412995241.523\n",
      "Iteration 4754: Loss = 12483351294787.432\n",
      "Iteration 4755: Loss = 12483289611768.453\n",
      "Iteration 4756: Loss = 12483227946179.553\n",
      "Iteration 4757: Loss = 12483166298015.684\n",
      "Iteration 4758: Loss = 12483104667271.807\n",
      "Iteration 4759: Loss = 12483043053942.885\n",
      "Iteration 4760: Loss = 12482981458023.877\n",
      "Iteration 4761: Loss = 12482919879509.748\n",
      "Iteration 4762: Loss = 12482858318395.465\n",
      "Iteration 4763: Loss = 12482796774675.992\n",
      "Iteration 4764: Loss = 12482735248346.303\n",
      "Iteration 4765: Loss = 12482673739401.361\n",
      "Iteration 4766: Loss = 12482612247836.143\n",
      "Iteration 4767: Loss = 12482550773645.617\n",
      "Iteration 4768: Loss = 12482489316824.76\n",
      "Iteration 4769: Loss = 12482427877368.547\n",
      "Iteration 4770: Loss = 12482366455271.957\n",
      "Iteration 4771: Loss = 12482305050529.969\n",
      "Iteration 4772: Loss = 12482243663137.56\n",
      "Iteration 4773: Loss = 12482182293089.71\n",
      "Iteration 4774: Loss = 12482120940381.406\n",
      "Iteration 4775: Loss = 12482059605007.633\n",
      "Iteration 4776: Loss = 12481998286963.379\n",
      "Iteration 4777: Loss = 12481936986243.623\n",
      "Iteration 4778: Loss = 12481875702843.363\n",
      "Iteration 4779: Loss = 12481814436757.586\n",
      "Iteration 4780: Loss = 12481753187981.285\n",
      "Iteration 4781: Loss = 12481691956509.445\n",
      "Iteration 4782: Loss = 12481630742337.076\n",
      "Iteration 4783: Loss = 12481569545459.164\n",
      "Iteration 4784: Loss = 12481508365870.71\n",
      "Iteration 4785: Loss = 12481447203566.713\n",
      "Iteration 4786: Loss = 12481386058542.17\n",
      "Iteration 4787: Loss = 12481324930792.09\n",
      "Iteration 4788: Loss = 12481263820311.47\n",
      "Iteration 4789: Loss = 12481202727095.32\n",
      "Iteration 4790: Loss = 12481141651138.65\n",
      "Iteration 4791: Loss = 12481080592436.457\n",
      "Iteration 4792: Loss = 12481019550983.758\n",
      "Iteration 4793: Loss = 12480958526775.56\n",
      "Iteration 4794: Loss = 12480897519806.885\n",
      "Iteration 4795: Loss = 12480836530072.738\n",
      "Iteration 4796: Loss = 12480775557568.13\n",
      "Iteration 4797: Loss = 12480714602288.092\n",
      "Iteration 4798: Loss = 12480653664227.63\n",
      "Iteration 4799: Loss = 12480592743381.771\n",
      "Iteration 4800: Loss = 12480531839745.531\n",
      "Iteration 4801: Loss = 12480470953313.941\n",
      "Iteration 4802: Loss = 12480410084082.014\n",
      "Iteration 4803: Loss = 12480349232044.783\n",
      "Iteration 4804: Loss = 12480288397197.273\n",
      "Iteration 4805: Loss = 12480227579534.512\n",
      "Iteration 4806: Loss = 12480166779051.533\n",
      "Iteration 4807: Loss = 12480105995743.361\n",
      "Iteration 4808: Loss = 12480045229605.035\n",
      "Iteration 4809: Loss = 12479984480631.592\n",
      "Iteration 4810: Loss = 12479923748818.057\n",
      "Iteration 4811: Loss = 12479863034159.475\n",
      "Iteration 4812: Loss = 12479802336650.887\n",
      "Iteration 4813: Loss = 12479741656287.324\n",
      "Iteration 4814: Loss = 12479680993063.834\n",
      "Iteration 4815: Loss = 12479620346975.465\n",
      "Iteration 4816: Loss = 12479559718017.25\n",
      "Iteration 4817: Loss = 12479499106184.242\n",
      "Iteration 4818: Loss = 12479438511471.486\n",
      "Iteration 4819: Loss = 12479377933874.035\n",
      "Iteration 4820: Loss = 12479317373386.936\n",
      "Iteration 4821: Loss = 12479256830005.236\n",
      "Iteration 4822: Loss = 12479196303724.0\n",
      "Iteration 4823: Loss = 12479135794538.273\n",
      "Iteration 4824: Loss = 12479075302443.115\n",
      "Iteration 4825: Loss = 12479014827433.586\n",
      "Iteration 4826: Loss = 12478954369504.738\n",
      "Iteration 4827: Loss = 12478893928651.637\n",
      "Iteration 4828: Loss = 12478833504869.344\n",
      "Iteration 4829: Loss = 12478773098152.92\n",
      "Iteration 4830: Loss = 12478712708497.436\n",
      "Iteration 4831: Loss = 12478652335897.947\n",
      "Iteration 4832: Loss = 12478591980349.533\n",
      "Iteration 4833: Loss = 12478531641847.256\n",
      "Iteration 4834: Loss = 12478471320386.188\n",
      "Iteration 4835: Loss = 12478411015961.402\n",
      "Iteration 4836: Loss = 12478350728567.973\n",
      "Iteration 4837: Loss = 12478290458200.975\n",
      "Iteration 4838: Loss = 12478230204855.48\n",
      "Iteration 4839: Loss = 12478169968526.572\n",
      "Iteration 4840: Loss = 12478109749209.326\n",
      "Iteration 4841: Loss = 12478049546898.83\n",
      "Iteration 4842: Loss = 12477989361590.154\n",
      "Iteration 4843: Loss = 12477929193278.393\n",
      "Iteration 4844: Loss = 12477869041958.623\n",
      "Iteration 4845: Loss = 12477808907625.938\n",
      "Iteration 4846: Loss = 12477748790275.422\n",
      "Iteration 4847: Loss = 12477688689902.166\n",
      "Iteration 4848: Loss = 12477628606501.256\n",
      "Iteration 4849: Loss = 12477568540067.793\n",
      "Iteration 4850: Loss = 12477508490596.863\n",
      "Iteration 4851: Loss = 12477448458083.566\n",
      "Iteration 4852: Loss = 12477388442522.998\n",
      "Iteration 4853: Loss = 12477328443910.258\n",
      "Iteration 4854: Loss = 12477268462240.44\n",
      "Iteration 4855: Loss = 12477208497508.646\n",
      "Iteration 4856: Loss = 12477148549709.984\n",
      "Iteration 4857: Loss = 12477088618839.555\n",
      "Iteration 4858: Loss = 12477028704892.459\n",
      "Iteration 4859: Loss = 12476968807863.81\n",
      "Iteration 4860: Loss = 12476908927748.713\n",
      "Iteration 4861: Loss = 12476849064542.28\n",
      "Iteration 4862: Loss = 12476789218239.617\n",
      "Iteration 4863: Loss = 12476729388835.84\n",
      "Iteration 4864: Loss = 12476669576326.06\n",
      "Iteration 4865: Loss = 12476609780705.398\n",
      "Iteration 4866: Loss = 12476550001968.963\n",
      "Iteration 4867: Loss = 12476490240111.879\n",
      "Iteration 4868: Loss = 12476430495129.262\n",
      "Iteration 4869: Loss = 12476370767016.238\n",
      "Iteration 4870: Loss = 12476311055767.924\n",
      "Iteration 4871: Loss = 12476251361379.443\n",
      "Iteration 4872: Loss = 12476191683845.926\n",
      "Iteration 4873: Loss = 12476132023162.498\n",
      "Iteration 4874: Loss = 12476072379324.283\n",
      "Iteration 4875: Loss = 12476012752326.412\n",
      "Iteration 4876: Loss = 12475953142164.021\n",
      "Iteration 4877: Loss = 12475893548832.236\n",
      "Iteration 4878: Loss = 12475833972326.195\n",
      "Iteration 4879: Loss = 12475774412641.031\n",
      "Iteration 4880: Loss = 12475714869771.883\n",
      "Iteration 4881: Loss = 12475655343713.887\n",
      "Iteration 4882: Loss = 12475595834462.184\n",
      "Iteration 4883: Loss = 12475536342011.912\n",
      "Iteration 4884: Loss = 12475476866358.215\n",
      "Iteration 4885: Loss = 12475417407496.238\n",
      "Iteration 4886: Loss = 12475357965421.129\n",
      "Iteration 4887: Loss = 12475298540128.025\n",
      "Iteration 4888: Loss = 12475239131612.084\n",
      "Iteration 4889: Loss = 12475179739868.451\n",
      "Iteration 4890: Loss = 12475120364892.28\n",
      "Iteration 4891: Loss = 12475061006678.717\n",
      "Iteration 4892: Loss = 12475001665222.924\n",
      "Iteration 4893: Loss = 12474942340520.049\n",
      "Iteration 4894: Loss = 12474883032565.256\n",
      "Iteration 4895: Loss = 12474823741353.691\n",
      "Iteration 4896: Loss = 12474764466880.53\n",
      "Iteration 4897: Loss = 12474705209140.918\n",
      "Iteration 4898: Loss = 12474645968130.025\n",
      "Iteration 4899: Loss = 12474586743843.016\n",
      "Iteration 4900: Loss = 12474527536275.057\n",
      "Iteration 4901: Loss = 12474468345421.307\n",
      "Iteration 4902: Loss = 12474409171276.94\n",
      "Iteration 4903: Loss = 12474350013837.125\n",
      "Iteration 4904: Loss = 12474290873097.03\n",
      "Iteration 4905: Loss = 12474231749051.828\n",
      "Iteration 4906: Loss = 12474172641696.693\n",
      "Iteration 4907: Loss = 12474113551026.805\n",
      "Iteration 4908: Loss = 12474054477037.336\n",
      "Iteration 4909: Loss = 12473995419723.46\n",
      "Iteration 4910: Loss = 12473936379080.363\n",
      "Iteration 4911: Loss = 12473877355103.225\n",
      "Iteration 4912: Loss = 12473818347787.223\n",
      "Iteration 4913: Loss = 12473759357127.541\n",
      "Iteration 4914: Loss = 12473700383119.37\n",
      "Iteration 4915: Loss = 12473641425757.893\n",
      "Iteration 4916: Loss = 12473582485038.293\n",
      "Iteration 4917: Loss = 12473523560955.77\n",
      "Iteration 4918: Loss = 12473464653505.5\n",
      "Iteration 4919: Loss = 12473405762682.691\n",
      "Iteration 4920: Loss = 12473346888482.53\n",
      "Iteration 4921: Loss = 12473288030900.203\n",
      "Iteration 4922: Loss = 12473229189930.918\n",
      "Iteration 4923: Loss = 12473170365569.871\n",
      "Iteration 4924: Loss = 12473111557812.254\n",
      "Iteration 4925: Loss = 12473052766653.271\n",
      "Iteration 4926: Loss = 12472993992088.13\n",
      "Iteration 4927: Loss = 12472935234112.023\n",
      "Iteration 4928: Loss = 12472876492720.166\n",
      "Iteration 4929: Loss = 12472817767907.756\n",
      "Iteration 4930: Loss = 12472759059670.006\n",
      "Iteration 4931: Loss = 12472700368002.121\n",
      "Iteration 4932: Loss = 12472641692899.314\n",
      "Iteration 4933: Loss = 12472583034356.797\n",
      "Iteration 4934: Loss = 12472524392369.78\n",
      "Iteration 4935: Loss = 12472465766933.479\n",
      "Iteration 4936: Loss = 12472407158043.11\n",
      "Iteration 4937: Loss = 12472348565693.895\n",
      "Iteration 4938: Loss = 12472289989881.045\n",
      "Iteration 4939: Loss = 12472231430599.783\n",
      "Iteration 4940: Loss = 12472172887845.332\n",
      "Iteration 4941: Loss = 12472114361612.916\n",
      "Iteration 4942: Loss = 12472055851897.752\n",
      "Iteration 4943: Loss = 12471997358695.076\n",
      "Iteration 4944: Loss = 12471938882000.105\n",
      "Iteration 4945: Loss = 12471880421808.076\n",
      "Iteration 4946: Loss = 12471821978114.217\n",
      "Iteration 4947: Loss = 12471763550913.754\n",
      "Iteration 4948: Loss = 12471705140201.924\n",
      "Iteration 4949: Loss = 12471646745973.959\n",
      "Iteration 4950: Loss = 12471588368225.098\n",
      "Iteration 4951: Loss = 12471530006950.576\n",
      "Iteration 4952: Loss = 12471471662145.629\n",
      "Iteration 4953: Loss = 12471413333805.502\n",
      "Iteration 4954: Loss = 12471355021925.432\n",
      "Iteration 4955: Loss = 12471296726500.66\n",
      "Iteration 4956: Loss = 12471238447526.436\n",
      "Iteration 4957: Loss = 12471180184997.996\n",
      "Iteration 4958: Loss = 12471121938910.594\n",
      "Iteration 4959: Loss = 12471063709259.477\n",
      "Iteration 4960: Loss = 12471005496039.895\n",
      "Iteration 4961: Loss = 12470947299247.096\n",
      "Iteration 4962: Loss = 12470889118876.332\n",
      "Iteration 4963: Loss = 12470830954922.861\n",
      "Iteration 4964: Loss = 12470772807381.932\n",
      "Iteration 4965: Loss = 12470714676248.807\n",
      "Iteration 4966: Loss = 12470656561518.74\n",
      "Iteration 4967: Loss = 12470598463186.99\n",
      "Iteration 4968: Loss = 12470540381248.822\n",
      "Iteration 4969: Loss = 12470482315699.494\n",
      "Iteration 4970: Loss = 12470424266534.271\n",
      "Iteration 4971: Loss = 12470366233748.418\n",
      "Iteration 4972: Loss = 12470308217337.2\n",
      "Iteration 4973: Loss = 12470250217295.883\n",
      "Iteration 4974: Loss = 12470192233619.738\n",
      "Iteration 4975: Loss = 12470134266304.033\n",
      "Iteration 4976: Loss = 12470076315344.045\n",
      "Iteration 4977: Loss = 12470018380735.043\n",
      "Iteration 4978: Loss = 12469960462472.3\n",
      "Iteration 4979: Loss = 12469902560551.096\n",
      "Iteration 4980: Loss = 12469844674966.707\n",
      "Iteration 4981: Loss = 12469786805714.408\n",
      "Iteration 4982: Loss = 12469728952789.482\n",
      "Iteration 4983: Loss = 12469671116187.213\n",
      "Iteration 4984: Loss = 12469613295902.879\n",
      "Iteration 4985: Loss = 12469555491931.766\n",
      "Iteration 4986: Loss = 12469497704269.16\n",
      "Iteration 4987: Loss = 12469439932910.346\n",
      "Iteration 4988: Loss = 12469382177850.611\n",
      "Iteration 4989: Loss = 12469324439085.252\n",
      "Iteration 4990: Loss = 12469266716609.555\n",
      "Iteration 4991: Loss = 12469209010418.807\n",
      "Iteration 4992: Loss = 12469151320508.314\n",
      "Iteration 4993: Loss = 12469093646873.361\n",
      "Iteration 4994: Loss = 12469035989509.252\n",
      "Iteration 4995: Loss = 12468978348411.277\n",
      "Iteration 4996: Loss = 12468920723574.742\n",
      "Iteration 4997: Loss = 12468863114994.943\n",
      "Iteration 4998: Loss = 12468805522667.19\n",
      "Iteration 4999: Loss = 12468747946586.773\n",
      "Iteration 5000: Loss = 12468690386749.01\n",
      "Оптимальні параметри моделі:\n",
      "Зсув (базова вартість будинку) [370080.91231773]\n",
      "коеф. w1:  [343338.43966686]\n",
      "коеф. w2\":  [226008.61849175]\n"
     ]
    }
   ],
   "source": [
    "# Знайдіть найкращі параметри w для датасету використовуючи написані вами функції, \n",
    "# прогнозуючу ціну на будинок залежно від площі, кількості ванних кімнат та кількості спалень;\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Завантаження даних\n",
    "data = pd.read_csv(r'C:\\Users\\Igor\\Housing.csv')\n",
    "\n",
    "# Розділення даних на ознаки та цільову змінну\n",
    "X = data[['area', 'bathrooms', 'bedrooms']].values\n",
    "y = data['price'].values\n",
    "\n",
    "# Нормалізація ознак\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "# Ініціалізація параметрів моделі\n",
    "theta = np.random.randn(X.shape[1], 1)\n",
    "    \n",
    "# Функція втрат (MSE) - середньоквадратична помилка між передбаченими значеннями та істинними значеннями цільової змінної\n",
    "def loss_function(X, y, theta):\n",
    "    m = len(y)  #  кількість спостережень\n",
    "    predictions = np.dot(X, theta)  # передбачені значення цільової змінної для кожного спостереження\n",
    "    loss = np.sum((predictions - y) ** 2) / (2 * m)  # середньоквадратична помилка між передбаченими значеннями та істинними значеннями цільової змінної\n",
    "    return loss\n",
    "\n",
    "# Функція гіпотези лінійної регресії\n",
    "def hypothesis(X, theta):\n",
    "    return np.dot(X, theta)   # передбачені значення для кожного спостереження в X\n",
    "\n",
    "# Градієнтний спуск\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    losses = []   # матриця втрат - значення втрат для кожної ітерації (loss)\n",
    "    for i in range(iterations):\n",
    "        predictions = hypothesis(X, theta)\n",
    "        gradient = np.dot(X.T, (predictions - y)) / m\n",
    "        theta -= learning_rate * gradient\n",
    "        loss = loss_function(X, y, theta)\n",
    "        losses.append(loss)\n",
    "        print(f\"Iteration {i+1}: Loss = {loss}\")\n",
    "    return theta, losses\n",
    "\n",
    "# Параметри градієнтного спуску\n",
    "learning_rate = 0.0001\n",
    "iterations = 5000\n",
    "\n",
    "# Виконання градієнтного спуску\n",
    "optimal_theta, losses = gradient_descent(X, y.reshape(-1, 1), theta, learning_rate, iterations)\n",
    "\n",
    "print(\"Оптимальні параметри моделі:\")\n",
    "print(\"Зсув (базова вартість будинку)\", optimal_theta[0])\n",
    "print('коеф. w1: ', optimal_theta[1])\n",
    "print('коеф. w2\": ', optimal_theta[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a89aa791-0fdb-449d-943b-7a6593d0b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Зсув (коеф. w0) - початкова вартість будинку, коли всі інші ознаки (площа, кількість ванних і спалень) рівні 0:  4766729.247706424\n",
      "коеф. w1 - коефіцієнт перед ознакою \"площа:  821214.1434951882\n",
      "коеф. w2 - коефіцієнт перед ознакою \"кількість ванних кімнат\":  695808.5227253665\n",
      "коеф. w3 - коефіцієнт перед ознакою \"кількість спалень\":  299983.57107963355\n"
     ]
    }
   ],
   "source": [
    "# знайдіть найкращі параметри w за допомогою аналітичного рішення\n",
    " \n",
    "\n",
    "# Додавання стовпця одиниць для зсуву до матриці ознак\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Обчислення параметрів за допомогою аналітичного рішення\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "print(\"Зсув (коеф. w0) - початкова вартість будинку, коли всі інші ознаки (площа, кількість ванних і спалень) рівні 0: \", theta_best[0])\n",
    "print('коеф. w1 - коефіцієнт перед ознакою \"площа: ', theta_best[1])\n",
    "print('коеф. w2 - коефіцієнт перед ознакою \"кількість ванних кімнат\": ', theta_best[2])\n",
    "print('коеф. w3 - коефіцієнт перед ознакою \"кількість спалень\": ', theta_best[3])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a83f3b07-514b-4375-9fbb-0669f6c84453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коефіцієнт w0:\n",
      "Значення: 4766729.2477064235\n",
      "Стандартна помилка: 57539.989308687196\n",
      "t-статистика: 82.84202525888823\n",
      "p-значення: 0.0\n",
      "Значущість: Значущий\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Коефіцієнт w1:\n",
      "Значення: 821214.1434951869\n",
      "Стандартна помилка: 58876.713766390836\n",
      "t-статистика: 13.948029551268341\n",
      "p-значення: 0.0\n",
      "Значущість: Значущий\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Коефіцієнт w2:\n",
      "Значення: 695808.5227253669\n",
      "Стандартна помилка: 62745.64380204515\n",
      "t-статистика: 11.08935187469839\n",
      "p-значення: 0.0\n",
      "Значущість: Значущий\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Коефіцієнт w3:\n",
      "Значення: 299983.5710796333\n",
      "Стандартна помилка: 62278.09679052408\n",
      "t-статистика: 4.816839090132203\n",
      "p-значення: 1.8936954899739789e-06\n",
      "Значущість: Значущий\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    " import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# Параметри моделі\n",
    "X = sm.add_constant(X)  # Додаємо зсув\n",
    "\n",
    "# Навчання моделі\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Отримання коефіцієнтів та стандартних помилок\n",
    "theta_best = model.params   #  вектор параметрів моделі, які знайдені за допомогою аналітичного рішення\n",
    "std_err = model.bse\n",
    "\n",
    "# Виконання т-тесту для кожного коефіцієнта\n",
    "for i in range(len(theta_best)):\n",
    "    coef = theta_best[i]\n",
    "    std_err = model.bse[i]  # Стандартна помилка коефіцієнта\n",
    "    t_stat = coef / std_err  # t-статистика\n",
    "    p_value = (1 - stats.t.cdf(abs(t_stat), len(y) - 1)) * 2  # p-значення\n",
    "\n",
    "    print(f\"Коефіцієнт w{i}:\")\n",
    "    print(f\"Значення: {coef}\")\n",
    "    print(f\"Стандартна помилка: {std_err}\")\n",
    "    print(f\"t-статистика: {t_stat}\")\n",
    "    print(f\"p-значення: {p_value}\")\n",
    "    print(\"Значущість: Значущий\" if p_value < 0.05 else \"Значення не є значущим\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5ca70a0-6825-468e-a778-6c42bd183ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для LinearRegression: 2750040479309.0522\n",
      "MSE для аналітичного рішення: 2682269809484.3843\n"
     ]
    }
   ],
   "source": [
    "#для перевірки спрогнозованих значень, використайте LinearRegression з бібліотеки scikit-learn та порівняйте результати\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Розділення даних на тренувальний та тестувальний набори\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Параметри: # навчальний набір (X_train, y_train)\n",
    "                # тестовий набір   (X_test, y_test)\n",
    "                # розмір тестового набору відносно загального обсягу даних\n",
    "                # test_size=0.2 означає, що 20% даних будуть використані для тестування, а 80% - для навчання моделі\n",
    "                # random_state - визначення початкового стану генератора псевдовипадкових чисел\n",
    "\n",
    "# Навчання моделі лінійної регресії\n",
    "lr = LinearRegression()   # LinearRegression() створює об'єкт моделі лінійної регресії.                      \n",
    "lr.fit(X_train, y_train)  # fit(X_train, y_train) навчає модель на навчальних ознаках X_train та відповідях y_train. \n",
    "\n",
    "# Прогнозування на тестовому наборі\n",
    "y_pred_lr = lr.predict(X_test)  # прогноз на тестових даних X_test\n",
    "\n",
    "# Додавання стовпця одиниць до X_test\n",
    "X_test_with_intercept = sm.add_constant(X_test)   # це тестові дані, до яких доданий зсув (intercept)\n",
    "\n",
    "# Прогнозування за допомогою аналітичного рішення - вектор прогнозованих значень цін на будинки для тестових даних\n",
    "y_pred_analytical = np.dot(X_test_with_intercept, theta_best)\n",
    "\n",
    "# Порівняння результатів за допомогою MSE\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mse_analytical = mean_squared_error(y_test, y_pred_analytical)\n",
    "\n",
    "print(\"MSE для LinearRegression:\", mse_lr)\n",
    "print(\"MSE для аналітичного рішення:\", mse_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc63919-e31d-43e7-9400-472a3c00d83d",
   "metadata": {},
   "source": [
    "Для аналітичного рішення MSE є трохи меншим, що вказує на те, що модель, \n",
    "побудована за допомогою аналітичного рішення, має меншу середньо-квадратичну помилку \n",
    "і краще підходить для прогнозування цін на будинки на основі вказаних ознак."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
